{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📦 **Step 1: Install Required Packages + Configure API Keys**\n",
    "\n",
    "Before you start using this notebook, ensure all required Python packages are installed **and** that your API keys are properly set up in a `.env` file. This prepares your environment for accessing services like OpenAI or Google Gemini.\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 How to Install Required Packages\n",
    "\n",
    "Open your terminal or command prompt, navigate to the project directory (where `requirements.txt` is located), and run:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "> ✅ **Tip:** Use a virtual environment (e.g., `venv`, `conda`) to keep your dependencies clean and isolated.\n",
    "\n",
    "If you're running this from a Jupyter notebook:\n",
    "\n",
    "```python\n",
    "!pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔐 Set Up API Keys in `.env` File\n",
    "\n",
    "To keep your API keys safe and reusable, store them in a `.env` file at the root of your project.\n",
    "\n",
    "### 📄 Example `.env` File\n",
    "\n",
    "```env\n",
    "GEMINI_API_KEY=your_gemini_api_key_here\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "```\n",
    "\n",
    "> 🔑 **Get your API keys from:**\n",
    ">\n",
    "> * **Gemini (Google AI Studio):** [aistudio](https://aistudio.google.com/apikey)\n",
    "> * **OpenAI (ChatGPT/GPT-4):** [openai.com](https://platform.openai.com/settings/organization/api-keys)\n",
    "\n",
    "This `.env` file is read by the Python `dotenv` library, so make sure it's included in your `requirements.txt`.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Troubleshooting\n",
    "\n",
    "* If you get a \"permission denied\" error:\n",
    "\n",
    "  ```bash\n",
    "  pip install --user -r requirements.txt\n",
    "  ```\n",
    "\n",
    "* Ensure `.env` is not shared or committed to version control (e.g., use `.gitignore`):\n",
    "\n",
    "  ```\n",
    "  .env\n",
    "  ```\n"
   ],
   "id": "388df311542bae67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🔧 Second Step: Create and Register Your Project Folder\n",
    "\n",
    "In this step, you'll register the **main project folder**, so the system knows where to store and retrieve all files related to your project.\n",
    "\n",
    "This setup automatically creates a structured folder system under your specified `main_folder`, and stores the configuration in a central JSON file. This ensures your projects remain organized, especially when working with multiple studies or datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Project Folder Structure\n",
    "\n",
    "On the first run, the following folder structure will be automatically created under your specified `main_folder`:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│   └── <project_name>_database.xlsx  ← auto-generated path (not file creation)\n",
    "├── pdf/\n",
    "└── xml/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Example\n",
    "\n",
    "Suppose your project name is `corona_discharge`, and you want to store all project files under:\n",
    "\n",
    "```\n",
    "G:\\My Drive\\research_related\\ear_eog\n",
    "```\n",
    "\n",
    "You can register this setup by running:\n",
    "\n",
    "```python\n",
    "project_folder(\n",
    "    project_review='corona_discharge',\n",
    "    main_folder=r'G:\\My Drive\\research_related\\ear_eog'\n",
    ")\n",
    "```\n",
    "\n",
    "✅ This will:\n",
    "\n",
    "* Save the project path in `setting/project_folders.json`\n",
    "* Create the full folder structure: `database/scopus`, `pdf`, and `xml`\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Loading the Project Later\n",
    "\n",
    "Once registered, you can access the project folders in future sessions by providing just the `project_review` name:\n",
    "\n",
    "```python\n",
    "paths = project_folder(project_review='corona_discharge')\n",
    "\n",
    "print(paths['main_folder'])  # Main project folder\n",
    "print(paths['csv_path'])     # Path to the Excel database file\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ What Happens in the Background?\n",
    "\n",
    "* A file named `project_folders.json` is stored in the `setting/` directory within your project.\n",
    "* It maps each `project_review` to its corresponding `main_folder`.\n",
    "* Folder structure is created automatically on the first run.\n",
    "* On subsequent runs, the system reads the JSON to locate your project — no need to re-enter paths.\n"
   ],
   "id": "8f032c61e06fff01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from setting.project_path import project_folder\n",
    "project_name='corona_discharge'\n",
    "main_folder=r\"D:\\my_project\"\n",
    "\n",
    "project_folder(project_review=project_name, main_folder=main_folder)"
   ],
   "id": "95782a760f85b5b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📥 Third Step: Download the Scopus BibTeX File\n",
    "\n",
    "In this step, you'll use the **Scopus database** to find and download relevant papers for your project. Scopus is a comprehensive and widely-used repository of peer-reviewed academic literature.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Using Scopus Advanced Search\n",
    "\n",
    "To retrieve high-quality and relevant papers, we recommend using **Scopus' Advanced Search** feature. This powerful tool lets you refine your search based on:\n",
    "\n",
    "* Keywords\n",
    "* Authors\n",
    "* Publication dates\n",
    "* Document types\n",
    "* And more...\n",
    "\n",
    "This ensures that your literature collection is both targeted and comprehensive.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Get Keyword Ideas with a Prompt\n",
    "\n",
    "To help you formulate effective search queries, you can use the following **prompt-based suggestion tool**:\n",
    "\n",
    "👉 [Keyword Search Prompt](https://gist.github.com/balandongiv/886437963d38252e61634ddc00b9d983)\n",
    "\n",
    "You may need to modify the prompt to better suit your research domain. Here are some example domains:\n",
    "\n",
    "* `\"corona discharge\"`\n",
    "* `\"fatigue driving EEG\"`\n",
    "* `\"wafer classification\"`\n",
    "\n",
    "Feel free to add, remove, or tweak keywords as needed to refine your search results.\n",
    "\n",
    "---\n",
    "\n",
    "## 💾 Save and Organize Your Results\n",
    "\n",
    "Once you've finalized your search:\n",
    "\n",
    "1. **Select all available attributes** when exporting results from Scopus.\n",
    "2. # Access Scopus\n",
    "![Scopus CSV Export](image/scopus_csv_export.png)\n",
    "\n",
    "\n",
    "2. Choose the **BibTeX** format when saving the export file.\n",
    "3. Save the file inside the `database/scopus/` folder of your project.\n",
    "\n",
    "The resulting folder structure might look like this:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── scopus(1).bib\n",
    "│       ├── scopus(2).bib\n",
    "│       ├── scopus(3).bib\n",
    "```\n",
    "\n",
    "Make sure the BibTeX files are correctly named and stored to ensure smooth integration in later steps."
   ],
   "id": "72c1fb6183a242ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📊 Fourth Step: Combine Scopus BibTeX Files into Excel\n",
    "\n",
    "Once you've downloaded multiple `.bib` files from Scopus, the next step is to **combine and convert** them into a structured Excel file. This makes it easier to filter, sort, and review the metadata of all collected papers.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 What This Step Does\n",
    "\n",
    "* Loads all `.bib` files from your project's `database/scopus/` folder\n",
    "* Parses the relevant metadata (e.g., title, authors, year, source, DOI)\n",
    "* Combines the results into a single Excel spreadsheet\n",
    "* Saves the spreadsheet in the `database/` folder as `combined_filtered.xlsx`\n",
    "\n",
    "\n",
    "\n",
    "## 📁 Folder Structure Example\n",
    "\n",
    "After running the script, your folder might look like this:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   ├── scopus/\n",
    "│   │   ├── scopus(1).bib\n",
    "│   │   ├── scopus(2).bib\n",
    "│   │   ├── scopus(3).bib\n",
    "│   └── combined_filtered.xlsx\n",
    "```\n",
    "\n",
    "This Excel file will serve as your primary reference for filtering papers before downloading PDFs.\n"
   ],
   "id": "b5539ae56b1485ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from download_pdf.database_preparation import combine_scopus_bib_to_excel\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "# project_review='corona_discharge'\n",
    "project_review='wafer'\n",
    "path_dic=project_folder(project_review=project_review)\n",
    "folder_path=path_dic['scopus_path']\n",
    "output_excel =  path_dic['database_path']\n",
    "combine_scopus_bib_to_excel(folder_path, output_excel)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 5: Automatically Filter Relevant References Using LLM**\n",
    "\n",
    "After retrieving thousands of BibTeX references from Scopus (**Step 3**) and combining them into an Excel file (`combined_filtered.xlsx`) in **Step 4**, you'll likely find many entries irrelevant to your specific research focus.\n",
    "\n",
    "In this step, we use a **Large Language Model (LLM)** to classify abstracts based on a defined research topic and context, eliminating the need for tedious manual filtering.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ **What This Step Does**\n",
    "\n",
    "* Loads abstracts from `combined_filtered.xlsx`\n",
    "* Applies a **custom LLM prompt** to assess relevance\n",
    "* Adds a new column to the Excel file with `True`/`False` labels:\n",
    "\n",
    "  * ✅ `True` → Relevant\n",
    "  * ❌ `False` → Not relevant\n",
    "\n",
    "This process is based on a topic-specific prompt defined by you.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 **LLM Prompt Customization**\n",
    "\n",
    "For the automated relevance filtering to work effectively, you **must provide two key inputs** to guide the LLM:\n",
    "\n",
    "* `topic`: A concise statement of your **specific research goal or area of interest**.\n",
    "  *Example:* `\"wafer defect classification\"`\n",
    "\n",
    "* `topic_context`: A description of the **broader scientific or industrial context** where your topic belongs.\n",
    "  *Example:* `\"semiconductor manufacturing and inspection\"`\n",
    "\n",
    "These inputs help the LLM understand what kinds of abstracts are considered relevant and which ones should be filtered out.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 *Not sure how to define your `topic` and `topic_context`?*\n",
    "\n",
    "You can get help from an LLM to generate these values. Here’s how:\n",
    "\n",
    "1. **Open the filtering prompt** template defined in the YAML file:\n",
    "\n",
    "   ```\n",
    "   research_filter/agent/abstract_check.yaml\n",
    "   ```\n",
    "\n",
    "2. **Copy the prompt structure** (including the placeholders for `topic` and `topic_context`).\n",
    "\n",
    "3. **Ask any LLM to assist**, by providing it the YAML prompt and a description of your research.\n",
    "   For example, you could say:\n",
    "\n",
    "   > 🧠 *\"Given the following prompt structure, and knowing that my research is about identifying AI-generated academic papers, can you help me fill in the `topic` and `topic_context` placeholders?\"*\n",
    "\n",
    "4. The LLM might then suggest:\n",
    "\n",
    "   ```yaml\n",
    "   topic: \"AI-generated academic paper detection\"\n",
    "   topic_context: \"scientific publishing and machine learning ethics\"\n",
    "   ```\n",
    "\n",
    "This approach ensures your filtering prompt is both precise and contextually grounded, improving the accuracy of the classification.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ **Configuration via `agentic_setting`**\n",
    "\n",
    "The entire filtering behavior is controlled by a configuration dictionary called `agentic_setting`:\n",
    "\n",
    "```python\n",
    "agentic_setting = {\n",
    "    \"agent_name\": \"abstract_filter\",     # The identifier for the agent logic (must match YAML)\n",
    "    \"column_name\": \"abstract_filter\",    # Name of the column added to the Excel file\n",
    "    \"yaml_path\": yaml_path,              # Path to the YAML file defining agent behavior\n",
    "    \"model_name\": model_name             # Name of the LLM model to use\n",
    "}\n",
    "```\n",
    "\n",
    "### 🔍 Parameter Breakdown:\n",
    "\n",
    "| Key           | Description                                                             |\n",
    "| ------------- | ----------------------------------------------------------------------- |\n",
    "| `agent_name`  | Matches the name of the agent defined in the YAML configuration file.   |\n",
    "| `column_name` | The name of the new column in the Excel file where results are saved.   |\n",
    "| `yaml_path`   | Path to the YAML file containing the agent's logic and prompt template. |\n",
    "| `model_name`  | The specific LLM model used (e.g., `\"gpt-4\"` or `\"claude-3-opus\"`).     |\n",
    "\n",
    "---\n",
    "\n",
    "## 📂 **File and Folder Structure**\n",
    "\n",
    "To avoid redundant LLM calls (which can be costly), results are cached as JSON files:\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       └── combined_filtered.xlsx           ← Updated with filtering results\n",
    "├── abstract_filter/\n",
    "│   └── json_output/\n",
    "│       ├── kim_2019.json\n",
    "│       ├── smith_2020.json\n",
    "│       └── ...\n",
    "```\n",
    "\n",
    "* Each abstract’s filtering result is saved individually.\n",
    "* If the run encounters an error, it can resume without reprocessing previous abstracts.\n",
    "* These files are later reused for **cross-checking** and **final review**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📤 **Excel Output Behavior**\n",
    "\n",
    "Depending on the `overwrite_csv` setting:\n",
    "\n",
    "* `True` → Updates the original `combined_filtered.xlsx`\n",
    "* `False` → Creates a new file (e.g., `combined_filtered_updated.xlsx`)\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ **Caution: Manual Review is Still Required**\n",
    "\n",
    "> ❗ **LLMs are powerful but not perfect**. They may misclassify edge cases or ambiguous abstracts.\n",
    ">\n",
    "> 🔍 Always **manually inspect** the final results before using them for publication or decision-making."
   ],
   "id": "712bb934a2db9134"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from research_filter.auto_llm import run_pipeline\n",
    "from setting.project_path import project_folder\n",
    "import os\n",
    "\n",
    "# Define your project\n",
    "project_review = 'wafer'\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Choose your LLM. or you can experiment with other models that is cheap or expensive and check the performance\n",
    "model_name = \"gpt-4o-mini\"\n",
    "base_dir = os.getcwd()\n",
    "yaml_path = os.path.join(base_dir, \"research_filter\", \"agent\", \"abstract_check.yaml\")\n",
    "# Agent configuration\n",
    "agentic_setting = {\n",
    "    \"agent_name\": \"abstract_filter\",# This is agent name as what available in yaml\n",
    "    \"column_name\": \"abstract_filter\", # This is the column name in the excel, we will use this name to save the result and the result is either True or False. While you have the flexibility to choose the column name, it is recommended to use the same name as written here as there some task that use this naming convention to clean up the llm output that being stored under the df in the column column_name. This is specifically true only for this step.\n",
    "    \"yaml_path\": yaml_path, # This is the path to the yaml file or the agent\n",
    "    \"model_name\": model_name # This is the model name, you can choose from the available models\n",
    "}\n",
    "\n",
    "# Paths and folders\n",
    "csv_path = path_dic['database_path']\n",
    "methodology_json_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'json_output')\n",
    "multiple_runs_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'multiple_runs_folder')\n",
    "final_cross_check_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'final_cross_check_folder')\n",
    "placeholders = {\n",
    "    \"topic\": \"wafer defect classification\",\n",
    "    \"topic_context\": \"semiconductor manufacturing and inspection\"\n",
    "}\n",
    "# LLM runtime settings. If unsure, use the default settings\n",
    "process_setup = {\n",
    "    'batch_process': False,\n",
    "    'manual_paste_llm': False,\n",
    "    'iterative_confirmation': False,\n",
    "    'overwrite_csv': True,\n",
    "    'cross_check_enabled': False,\n",
    "    'cross_check_runs': 3,\n",
    "    'cross_check_agent_name': 'agent_cross_check',\n",
    "    'cleanup_json': False\n",
    "}\n",
    "\n",
    "# Run the LLM-based filtering\n",
    "run_pipeline(\n",
    "    agentic_setting,\n",
    "    process_setup,\n",
    "    placeholders=placeholders,\n",
    "    csv_path=csv_path,\n",
    "    main_folder=main_folder,\n",
    "    methodology_json_folder=methodology_json_folder,\n",
    "    multiple_runs_folder=multiple_runs_folder,\n",
    "    final_cross_check_folder=final_cross_check_folder,\n",
    ")\n"
   ],
   "id": "7ea8ae2257b2a4d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Sixth Step: Download PDFs**\n",
    "\n",
    "Now that your reference list has been filtered to include only **relevant papers** (based on the abstract analysis in **Step 5**), you're ready to automatically download their corresponding PDFs.\n",
    "\n",
    "This step uses the filtered Excel file (updated in Step 5) to retrieve and save PDFs for each BibTeX entry. The script is powered by Selenium and supports fallback strategies for sources like IEEE, MDPI, and ScienceDirect.\n",
    "\n",
    "> 🛑 **Note:** This step launches a full browser window during execution. Some publishers may block headless downloads — using a visible browser avoids this issue.\n",
    "\n",
    "> 📝 **Important:** By default, only **Sci-Hub** is enabled. To use fallback sources like IEEE, MDPI, or ScienceDirect, you must **manually uncomment the relevant function calls** in the script. This allows you to selectively control which sources to attempt.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Loads metadata from the filtered Excel file (`combined_filtered.xlsx` or the updated version from Step 5).\n",
    "* Attempts to download each paper from **Sci-Hub** first.\n",
    "* If Sci-Hub fails for a paper, you can optionally enable **fallback downloads** from:\n",
    "\n",
    "  * IEEE\n",
    "  * IEEE Search\n",
    "  * MDPI\n",
    "  * ScienceDirect (note: may fail due to access restrictions)\n",
    "* Saves each PDF as `{bibtex_key}.pdf` in the `pdf/` directory for easy tracking and consistent file naming.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Code Snippet\n",
    "\n",
    "```python\n",
    "from download_pdf.download_pdf import (\n",
    "    run_pipeline,\n",
    "    process_scihub_downloads,\n",
    "    process_fallback_ieee,\n",
    "    # process_fallback_ieee_search,\n",
    "    # process_fallback_mdpi,\n",
    "    # process_fallback_sciencedirect,\n",
    ")\n",
    "from setting.project_path import project_folder\n",
    "import os\n",
    "\n",
    "# Project setup\n",
    "project_review = 'wafer_defect'\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Use the filtered Excel from Step 5\n",
    "file_path = path_dic['csv_path']\n",
    "output_folder = os.path.join(main_folder, 'pdf')\n",
    "\n",
    "# Load and categorize data\n",
    "categories, data_filtered = run_pipeline(file_path)\n",
    "\n",
    "# Step 1: Attempt to download from Sci-Hub\n",
    "process_scihub_downloads(categories, output_folder, data_filtered)\n",
    "\n",
    "# Optional fallback sources — uncomment as needed:\n",
    "# Step 2: Try IEEE fallback if Sci-Hub fails\n",
    "# process_fallback_ieee(categories, data_filtered, output_folder)\n",
    "\n",
    "# Step 3: Use IEEE Search-based fallback\n",
    "# process_fallback_ieee_search(categories, data_filtered, output_folder)\n",
    "\n",
    "# Step 4: Try MDPI fallback\n",
    "# process_fallback_mdpi(categories, data_filtered, output_folder)\n",
    "\n",
    "# Step 5: Try ScienceDirect fallback (limited due to restrictions)\n",
    "# process_fallback_sciencedirect(categories, data_filtered, output_folder)\n",
    "\n",
    "# Optional: Save updated Excel with download statuses\n",
    "# save_data(data_filtered, file_path)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Example Folder Structure\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       └── combined_filtered.xlsx     ← Filtered metadata with BibTeX keys (updated in Step 5)\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf                 ← Saved using bibtex_key\n",
    "│   ├── kim_2019.pdf\n",
    "│   └── ...\n",
    "```\n",
    "\n",
    "---"
   ],
   "id": "99f29be5d0a2412b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from download_pdf.download_pdf import run_pipeline, process_scihub_downloads, process_fallback_ieee\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "# Define your project\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "file_path = path_dic['database_path']\n",
    "output_folder = os.path.join(main_folder, 'pdf')\n",
    "\n",
    "# Run the main pipeline to load and categorize the data\n",
    "categories, data_filtered = run_pipeline(file_path)\n",
    "\n",
    "# First step, we will always use Sci-Hub to attempt PDF downloads\n",
    "process_scihub_downloads(categories, output_folder, data_filtered)\n",
    "\n",
    "# Fallback options for entries not available via Sci-Hub:\n",
    "# Uncomment the following lines one by one if you want to try downloading from specific sources\n",
    "\n",
    "# Uncomment to attempt fallback download from IEEE\n",
    "# process_fallback_ieee(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download using IEEE Search\n",
    "# process_fallback_ieee_search(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download from MDPI\n",
    "# process_fallback_mdpi(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download from ScienceDirect\n",
    "# Note: ScienceDirect URLs can be extracted but PDFs may not be downloadable due to security restrictions\n",
    "# process_fallback_sciencedirect(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to save the updated data to Excel after processing\n",
    "# save_data(data_filtered, file_path)"
   ],
   "id": "6a37458687d840b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Seventh Step: Convert PDFs to XML using GROBID (OPTIONAL)**\n",
    "\n",
    "After downloading the PDFs, the next step is to convert them into structured **TEI XML** format using [**GROBID**](https://grobid.readthedocs.io). This step enables downstream tasks like metadata extraction, reference parsing, and full-text analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 What This Step Does\n",
    "\n",
    "* Processes all PDF files in your `pdf/` directory.\n",
    "* Uses GROBID's **batch processing API**.\n",
    "* Saves the resulting XML files into the `xml/` folder (one `.xml` per `.pdf`).\n",
    "* Leverages Docker for fast, isolated execution.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Setup Requirements\n",
    "\n",
    "> 🛠️ **GROBID requires WSL + Docker on Windows**\n",
    "\n",
    "* You must have **WSL** installed (tested on **WSL2 with Ubuntu 22.04**).\n",
    "* You must have **Docker** installed and **running** before launching GROBID.\n",
    "\n",
    "---\n",
    "\n",
    "## 🐳 How to Install & Run GROBID\n",
    "\n",
    "1. **Pull the Docker image from Docker Hub**\n",
    "   Check for the [latest version](https://hub.docker.com/r/grobid/grobid/tags), or use the stable one:\n",
    "\n",
    "   ```bash\n",
    "   docker pull grobid/grobid:0.8.1\n",
    "   ```\n",
    "\n",
    "2. **Start the GROBID container in Ubuntu (WSL)**\n",
    "\n",
    "   Open your Ubuntu terminal and run:\n",
    "\n",
    "   ```bash\n",
    "   docker run --rm --gpus all --init --ulimit core=0 -p 8070:8070 grobid/grobid:0.8.1\n",
    "   ```\n",
    "\n",
    "   > ✅ This exposes GROBID's REST API on `http://localhost:8070/`\n",
    "\n",
    "3. **Test it in your browser**\n",
    "\n",
    "   Open a browser (e.g., Firefox or Chrome) and navigate to:\n",
    "\n",
    "   ```\n",
    "   http://localhost:8070/\n",
    "   ```\n",
    "\n",
    "   You should see the GROBID interface.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Batch Conversion Command\n",
    "\n",
    "Once the GROBID service is running, you can convert all PDFs in your `pdf/` folder to XML using:\n",
    "\n",
    "```bash\n",
    "# From your project root (in WSL/Ubuntu)\n",
    "cd path/to/your/project\n",
    "\n",
    "# Create output folder if not exists\n",
    "mkdir -p xml\n",
    "\n",
    "# Run batch processing using curl\n",
    "curl -v --form \"input=@pdf/\" localhost:8070/api/processFulltextDocument -o xml/\n",
    "```\n",
    "\n",
    "Or, use a Python wrapper or script to iterate over PDFs and call GROBID’s REST API for more control.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Folder Structure After Conversion\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf\n",
    "│   └── kim_2019.pdf\n",
    "├── xml/\n",
    "│   ├── smith_2020.xml\n",
    "│   └── kim_2019.xml\n",
    "```\n"
   ],
   "id": "c3026c463e76dec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Eighth Step (Optional): Convert XML to JSON**\n",
    "\n",
    "This step converts GROBID-generated TEI XML files into structured JSON format. While optional, it can be helpful for reviewing document content, integrating into other tools, or preparing data to feed into an LLM.\n",
    "\n",
    "> 📝 **Note:** This step is **optional** — the main pipeline (`run_llm`) reads directly from XML. Use this conversion if you want to inspect or process JSON files instead.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Reads all `*.xml` files from the `xml/` directory.\n",
    "* Converts each into a corresponding `*.json` file (preserving the **BibTeX key as filename** for consistency).\n",
    "* Stores all JSON outputs in `xml/json/`.\n",
    "\n",
    "In addition, it handles and organizes special cases:\n",
    "\n",
    "* 📁 **`xml/json/no_intro_conclusion/`**: XML files where GROBID could not detect an *introduction* or *conclusion* section.\n",
    "* 📁 **`xml/json/untitled_section/`**: XML files where GROBID could not detect any section titles at all — these require manual checking.\n",
    "* 📄 Other successfully processed files are stored directly in `xml/json/`.\n",
    "\n",
    "---\n",
    "\n",
    "## ▶️ Example Code\n",
    "\n",
    "```python\n",
    "from setting.project_path import project_folder\n",
    "from grobid_tei_xml.xml_json import run_pipeline\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "\n",
    "# Convert all XML files in the specified folder to JSON\n",
    "run_pipeline(path_dic['xml_path'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Example Folder Structure After Conversion\n",
    "\n",
    "```\n",
    "corona_discharge/\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf\n",
    "│   └── kim_2019.pdf\n",
    "├── xml/\n",
    "│   ├── smith_2020.xml\n",
    "│   ├── kim_2019.xml\n",
    "│   └── json/\n",
    "│       ├── smith_2020.json\n",
    "│       ├── kim_2019.json\n",
    "│       ├── no_intro_conclusion/\n",
    "│       │   └── failed_paper1.json\n",
    "│       └── untitled_section/\n",
    "│           └── failed_paper2.json\n",
    "```"
   ],
   "id": "24dba46998c443c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from setting.project_path import project_folder\n",
    "from grobid_tei_xml.xml_json import run_pipeline\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "run_pipeline(path_dic['xml_path'])"
   ],
   "id": "17f7f677505cff0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Ninth Step: Extract Methodology Details Using LLM**\n",
    "\n",
    "At this stage, your reference list is filtered and the corresponding PDFs (or abstracts) are available. Now, the focus shifts to **extracting key methodological insights** from each paper, such as:\n",
    "\n",
    "* 🧠 Classification algorithms\n",
    "* 🛠️ Feature engineering approaches\n",
    "* 📏 Evaluation metrics\n",
    "\n",
    "This is achieved using a specialized **LLM agent** with a targeted prompt for methodology extraction.\n",
    "\n",
    "> 📌 This step works with **full manuscripts (PDF)** when available, and **falls back to abstracts** if no PDF exists. This flexibility ensures comprehensive analysis even with incomplete data.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Loads your filtered Excel or CSV file from Step 5 or 6.\n",
    "* For each relevant paper:\n",
    "\n",
    "  * If the PDF is available: extracts methodology from the full text.\n",
    "  * If the PDF is **not** available: extracts from the **abstract** instead.\n",
    "* Uses a domain-specific LLM prompt to analyze methodological content.\n",
    "* Appends the results to the existing Excel or CSV file.\n",
    "* Saves per-paper structured JSON files for advanced or customized usage.\n",
    "* Handles backups automatically if overwriting the metadata file.\n",
    "\n",
    "> ⛑️ **Safety Note**: If `'overwrite_csv': True`, a **timestamped backup** of the original `.csv` or `.xlsx` file is automatically created **in the same folder** before any updates are made. This prevents accidental corruption and allows for recovery or version tracking.\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Prompt Purpose\n",
    "\n",
    "This step uses a **domain-aware analytical agent** designed to:\n",
    "\n",
    "> “Extract methodological details (e.g., classification algorithms, feature engineering, evaluation metrics) from filtered papers relevant to a specific machine learning task.”\n",
    "\n",
    "The prompt is defined in a YAML config file (`agent_ml.yaml`) and is tailored by the agent name you provide.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Folder and Output Structure\n",
    "\n",
    "Your project directory may look like this after completing this step:\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── combined_filtered.xlsx                  ← Updated metadata with extraction results\n",
    "│       ├── combined_filtered_backup_20250508_1530.xlsx  ← Auto-generated backup (if overwrite_csv=True)\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf                              ← Saved using bibtex_key\n",
    "│   ├── kim_2019.pdf\n",
    "│   └── ...\n",
    "├── methodology_gap_extractor/\n",
    "│   ├── json_output/\n",
    "│   │   └── smith_2020.json\n",
    "│   ├── multiple_runs_folder/\n",
    "│   └── final_cross_check_folder/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Supported Models\n",
    "\n",
    "Choose from the following supported LLMs:\n",
    "\n",
    "* `\"gpt-4o\"`\n",
    "* `\"gpt-4o-mini\"`\n",
    "* `\"gpt-o3-mini\"`\n",
    "* `'gemini-1.5-pro'`\n",
    "* `'gemini-exp-1206'`\n",
    "* `'gemini-2.0-flash-thinking-exp-01-21'`\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Important Reminders\n",
    "\n",
    "* ✅ Set `used_abstract = True` if some papers lack full PDFs.\n",
    "* 🛑 **Always verify** extracted methodologies manually before using them in analysis, models, or publication. LLMs can hallucinate or misinterpret technical details.\n",
    "\n",
    "---"
   ],
   "id": "9f09bf489150aabb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from research_filter.auto_llm import run_pipeline\n",
    "from setting.project_path import project_folder\n",
    "import os\n",
    "\n",
    "# Define the project\n",
    "project_review = 'wafer'\n",
    "# Select your LLM model. For this step, it is recommended to use a more powerful model like `gpt-4o` or `gemini-2.0-flash-thinking-exp-01-21` for better performance. Use model that have reasoning capability for better performance.\n",
    "\n",
    "\n",
    "model_name = 'gpt-4o-mini'\n",
    "agent_name = \"methodology_gap_extractor\"\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "# Construct the relative path\n",
    "yaml_path = os.path.join(base_dir, \"research_filter\", \"agent\", \"agent_ml.yaml\")\n",
    "\n",
    "# Agent and config setup\n",
    "agentic_setting = {\n",
    "    \"agent_name\": agent_name,\n",
    "    \"column_name\": \"methodology_gap\",\n",
    "    \"yaml_path\": yaml_path,\n",
    "    \"model_name\": model_name\n",
    "}\n",
    "\n",
    "# Output directories\n",
    "methodology_json_folder = os.path.join(main_folder, agent_name, 'json_output',model_name)\n",
    "multiple_runs_folder = os.path.join(main_folder, agent_name, 'multiple_runs_folder')\n",
    "final_cross_check_folder = os.path.join(main_folder, agent_name, 'final_cross_check_folder')\n",
    "\n",
    "# Excel input\n",
    "csv_path = path_dic['database_path']\n",
    "\n",
    "# Topic placeholders (adjust per project)\n",
    "placeholders = {\n",
    "    \"topic\": \"wafer defect classification\",\n",
    "    \"topic_context\": \"semiconductor manufacturing and inspection\"\n",
    "}\n",
    "\n",
    "# LLM runtime configuration\n",
    "process_setup = {\n",
    "    'batch_process': False,\n",
    "    'manual_paste_llm': False,\n",
    "    'iterative_confirmation': False,\n",
    "    'overwrite_csv': True,\n",
    "    'cross_check_enabled': False,\n",
    "    'cross_check_runs': 3,\n",
    "    'cross_check_agent_name': 'agent_cross_check',\n",
    "    'cleanup_json': False,\n",
    "    'used_abstract': True  # Always True to enable fallback to abstract if PDF is missing\n",
    "}\n",
    "\n",
    "# Run the pipeline\n",
    "run_pipeline(\n",
    "    agentic_setting,\n",
    "    process_setup,\n",
    "    placeholders=placeholders,\n",
    "    csv_path=csv_path,\n",
    "    main_folder=main_folder,\n",
    "    methodology_json_folder=methodology_json_folder,\n",
    "    multiple_runs_folder=multiple_runs_folder,\n",
    "    final_cross_check_folder=final_cross_check_folder,\n",
    ")"
   ],
   "id": "d4cc866c39eb8c2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Tenth Step: Draft Literature Review (Chapter 2) Using Combined JSON**\n",
    "\n",
    "Once you've extracted methodological insights in Step 9, the next logical move is to **start organizing and drafting your literature review** (typically Chapter 2 of a thesis or paper). This step outlines how to **combine extracted results** into a single, structured file — and how to use that file to produce high-quality summaries, tables, and narrative drafts using an LLM.\n",
    "\n",
    "> 📌 This is a branching step — not everyone will follow this exact path — but it's a powerful way to move from **structured data → written draft** efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Collects individual methodology JSON files.\n",
    "* Merges them into a single, unified JSON (`combined_output.json`).\n",
    "* Prepares this JSON as input for:\n",
    "\n",
    "  * Google AI Studio (e.g., Gemini Pro)\n",
    "  * GPT-based agents\n",
    "  * Jupyter notebooks or drafting pipelines\n",
    "* Enables:\n",
    "\n",
    "  * Narrative generation for Chapter 2\n",
    "  * Thematic clustering of methods\n",
    "  * Structured tables summarizing key findings\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Code Example: Combine JSONs\n",
    "\n",
    "```python\n",
    "import os\n",
    "from setting.project_path import project_folder\n",
    "from helper.combine_json import combine_json_files\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "path_dict = project_folder(project_review=project_review)\n",
    "\n",
    "input_dir = os.path.join(\n",
    "    path_dict['main_folder'],\n",
    "    r\"methodology_gap_extractor_partial_discharge\\json_output\\gemini-2.0-flash-thinking-exp-01-21_updated\"\n",
    ")\n",
    "output_file = os.path.join(path_dict['main_folder'], 'combined_output.json')\n",
    "\n",
    "if not os.path.exists(input_dir):\n",
    "    raise FileNotFoundError(f\"Input directory not found: {input_dir}\")\n",
    "\n",
    "combine_json_files(\n",
    "    input_directory=input_dir,\n",
    "    output_file=output_file\n",
    ")\n",
    "\n",
    "print(f\"Combined JSON saved to: {output_file}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Output Structure Example\n",
    "\n",
    "```\n",
    "corona_discharge/\n",
    "├── combined_output.json      ← ✅ Master file for summarization and drafting\n",
    "├── methodology_gap_extractor_partial_discharge/\n",
    "│   └── json_output/\n",
    "│       └── *.json            ← Individual extracted documents\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Using LLM to Generate Summary Tables\n",
    "\n",
    "With the `combined_output.json`, you can prompt an LLM to create structured tables summarizing key findings. For example:\n",
    "\n",
    "> “Using the combined JSON, generate a table with the following columns:\n",
    "> **Author**, **Machine Learning Algorithm**, **Feature Types**, **Dataset Used**, and **Performance Metrics**.”\n",
    "\n",
    "This gives you a **quick-glance overview** of the landscape, helpful both for understanding trends and citing clearly in your literature review.\n",
    "\n",
    "### ✅ Sample Columns for Summary Table:\n",
    "\n",
    "* **Author / Year**\n",
    "* **ML Algorithm(s) Used**\n",
    "* **Features**\n",
    "* **Dataset / Source**\n",
    "* **Performance (Accuracy, F1, etc.)**\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ Drafting Strategy Tips\n",
    "\n",
    "You can also use LLM prompts like:\n",
    "\n",
    "> “Based on the combined JSON, write a summary paragraph comparing the top 3 machine learning techniques used for partial discharge classification.”\n",
    "\n",
    "Or:\n",
    "\n",
    "> “Generate an introduction section discussing the evolution of feature engineering techniques in this domain.”\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Important Reminders\n",
    "\n",
    "* ✅ Make sure all JSON structures are consistent before combining.\n",
    "* 📉 Segment or cluster the JSON by subdomain if the file becomes too large.\n",
    "* 🔍 Review all generated tables and text — LLMs are **tools**, not final authorities.\n",
    "\n",
    "---"
   ],
   "id": "411c0567bd535174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from setting.project_path import project_folder\n",
    "from helper.combine_json import combine_json_files\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "path_dict = project_folder(project_review=project_review)\n",
    "\n",
    "input_dir = os.path.join(\n",
    "    path_dict['main_folder'],\n",
    "    r\"methodology_gap_extractor_partial_discharge\\json_output\\gemini-2.0-flash-thinking-exp-01-21_updated\"\n",
    ")\n",
    "output_file = os.path.join(path_dict['main_folder'], 'combined_output.json')\n",
    "\n",
    "if not os.path.exists(input_dir):\n",
    "    raise FileNotFoundError(f\"Input directory not found: {input_dir}\")\n",
    "\n",
    "combine_json_files(\n",
    "    input_directory=input_dir,\n",
    "    output_file=output_file\n",
    ")\n",
    "\n",
    "print(f\"Combined JSON saved to: {output_file}\")\n"
   ],
   "id": "527d5be14321c3c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Eleventh Step: Export Filtered Excel to BibTeX**\n",
    "\n",
    "After reviewing and filtering your `combined_filtered.xlsx` file, you can convert the refined list of papers back into a **BibTeX file**. This can be helpful for citation management or integration with tools like LaTeX or reference managers.\n",
    "\n",
    "---\n",
    "\n",
    "# ✨ What This Step Does\n",
    "\n",
    "* Loads the filtered Excel file containing your selected papers\n",
    "* Converts the data back into BibTeX format\n",
    "* Saves the result to a `.bib` file for easy reuse or citation\n",
    "\n",
    "\n",
    "# 📁 File Structure Example\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── combined_filtered.xlsx                  ← Filtered Excel file with selected papers\n",
    "│       ├── combined_filtered_backup_20250508_1530.xlsx  ← Auto backup if overwrite is enabled\n",
    "│       └── filtered_output.bib                     ← Newly generated BibTeX file\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf                              ← Full-text PDFs saved by bibtex_key\n",
    "│   ├── kim_2019.pdf\n",
    "│   └── ...\n",
    "├── methodology_gap_extractor_wafer_defect/\n",
    "│   ├── json_output/\n",
    "│   │   └── smith_2020.json                         ← Extracted methodology as structured JSON\n",
    "│   ├── multiple_runs_folder/\n",
    "│   └── final_cross_check_folder/\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Notes\n",
    "\n",
    "* Ensure the Excel file has standard bibliographic columns like: `title`, `author`, `year`, `journal`, `doi`, etc.\n",
    "* The function `generate_bibtex()` maps these fields into valid BibTeX entries.\n",
    "* You can open the `.bib` file in any text editor or reference manager to confirm the results."
   ],
   "id": "3c8648d56cd6763f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from post_code_saviour.excel_to_bib import generate_bibtex\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "# Define your project\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Define input and output paths\n",
    "input_excel = os.path.join(main_folder, 'database', 'combined_filtered.xlsx')\n",
    "output_bib = os.path.join(main_folder, 'database', 'filtered_output.bib')\n",
    "\n",
    "# Load the filtered Excel file\n",
    "df = pd.read_excel(input_excel)\n",
    "\n",
    "# Generate BibTeX file\n",
    "generate_bibtex(df, output_file=output_bib)"
   ],
   "id": "35d9935f47aebf25",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
