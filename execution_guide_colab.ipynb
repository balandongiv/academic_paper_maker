{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notebook Summary: Academic Paper Maker Workflow\n",
    "\n",
    "This notebook guides you through a step-by-step process for setting up your environment, collecting academic papers, filtering them using Large Language Models (LLMs), extracting key information, and preparing files for literature review drafting and citation.\n",
    "\n",
    "---\n",
    "\n",
    "## How to Run This Notebook\n",
    "\n",
    "To run this notebook, follow these instructions:\n",
    "\n",
    "1.  **Read each step** below first to understand the process.\n",
    "2.  **Run the code cells** one by one, in the order they appear.\n",
    "3.  Click on a code cell and press `Shift + Enter`, or click the play button (`▶`) on the left side of the cell.\n",
    "4.  A `[*]` next to a cell indicates it's currently running. It will change to a number (e.g., `[1]`, `[2]`) when execution is complete.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Steps\n",
    "\n",
    "This notebook consists of **12 distinct steps** presented sequentially through Markdown headers, guiding you through the automated academic paper review process.\n",
    "\n",
    "Here is a summary of each step and its purpose:\n",
    "\n",
    "1.  **Step 1: Install Required Packages**\n",
    "    *   **Purpose:** To set up the necessary Python environment by cloning the `academic_paper_maker` repository and installing all required dependencies from `requirements.txt`.\n",
    "\n",
    "2.  **Step 2: Utility Functions for Colab Execution**\n",
    "    *   **Purpose:** To define helper functions specifically tailored for running the various steps within a Google Colab environment, simplifying common operations like API testing, filtering, JSON merging, and BibTeX generation. (Code cell implements these functions).\n",
    "\n",
    "3.  **Step 3: Obtain Your API Keys**\n",
    "    *   **Purpose:** To instruct the user on how to acquire necessary API keys (e.g., OpenAI, Gemini) from their respective platforms, which are required for using LLM-based features later in the workflow. Keys are saved externally at this stage.\n",
    "\n",
    "4.  **Step 4: Create and Register Your Project Folder**\n",
    "    *   **Purpose:** To define and initialize the main directory for your project, setting up a standard folder structure (`database/scopus`, `pdf`, `xml`) and registering the project's location for easy access in future sessions.\n",
    "\n",
    "5.  **Step 5: Create a `.env` File to Store API Keys**\n",
    "    *   **Purpose:** To create a `.env` file in the project directory. This file serves as a secure location to store your obtained API keys (manually added by the user after creation), preventing credentials from being hardcoded.\n",
    "\n",
    "6.  **Step 5a: Load API Key and Test OpenAI Connection**\n",
    "    *   **Purpose:** To verify that the OpenAI API key stored in the `.env` file is correctly loaded and can successfully connect and communicate with the OpenAI API by sending a simple test query.\n",
    "\n",
    "7.  **Step 6: Download the Scopus BibTeX File**\n",
    "    *   **Purpose:** To guide the user on how to use the Scopus database (or a similar source) to perform advanced searches for relevant academic papers and download the results in BibTeX format (`.bib`) into the designated `database/scopus` folder.\n",
    "\n",
    "8.  **Step 7: Combine Scopus BibTeX Files into Excel**\n",
    "    *   **Purpose:** To process the downloaded BibTeX (`.bib`) files from the `database/scopus` folder, extract key metadata from all files, and merge them into a single, structured Excel file (`combined_filtered.xlsx`) for subsequent filtering and review.\n",
    "\n",
    "9.  **Step 8: Automatically Filter Relevant References Using LLM**\n",
    "    *   **Purpose:** To employ a Large Language Model (LLM) to automatically read the abstracts from the `combined_filtered.xlsx` file and classify their relevance based on a user-defined `topic` and `topic_context`, adding a `True`/`False` filter column to the Excel.\n",
    "\n",
    "10. **Step 7: Extract Methodology Details Using LLM** (Note: Header numbers are inconsistent, but this is a distinct step logically)\n",
    "    *   **Purpose:** To use an LLM agent to extract detailed methodological information (e.g., algorithms, datasets, metrics) from the filtered papers, utilizing either the full PDF text (if available) or the abstract as a fallback, and saving the results in structured JSON files and updating the Excel.\n",
    "\n",
    "11. **Step 8: Draft Literature Review (Chapter 2) Using Combined JSON** (Note: Header numbers are inconsistent, but this is a distinct step logically)\n",
    "    *   **Purpose:** To combine the individual JSON files containing extracted methodology details into a single `combined_output.json`. This file then serves as structured input for LLMs or scripts to assist in drafting literature review sections, generating summary tables, or performing thematic analysis.\n",
    "\n",
    "12. **Step 9: Export Filtered Excel to BibTeX**\n",
    "    *   **Purpose:** To convert the final, manually reviewed and filtered Excel file (`combined_filtered.xlsx`) back into a BibTeX (`.bib`) file, facilitating easy import into citation managers or integration with LaTeX documents for generating bibliographies.\n",
    "\n",
    "---\n",
    "\n",
    "This sequence of steps automates significant portions of the literature review process, from collecting initial references to extracting detailed insights and preparing final outputs for writing."
   ],
   "id": "15d98b2ba4177581"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📦 **Step 1: Install Required Packages**\n",
    "\n",
    "Before running the notebook, you need to **set up your environment** by cloning the repository and installing all the required Python packages.\n",
    "\n",
    "---\n",
    "\n",
    "## 📥 What This Step Does\n",
    "\n",
    "* **Clones** the GitHub repository `academic_paper_maker` into your Colab environment\n",
    "* Changes into the project directory so all subsequent code runs in the right context\n",
    "* Installs dependencies listed in the project's `requirements.txt` file\n",
    "* Installs `openai` explicitly in case it’s not listed in `requirements.txt`\n",
    "\n",
    "> ✅ **Tip:** This ensures that all required libraries are available before continuing with API setup or running the app logic."
   ],
   "id": "be071df856ca8197"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!git clone https://github.com/balandongiv/academic_paper_maker.git\n",
    "%cd academic_paper_maker\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "print(\"✅ Packages installed successfully. You can now proceed to the next steps.\")"
   ],
   "id": "5a7bd8e31138e4e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# ⚙️ Step 2 **Utility Functions for Colab Execution**\n",
    "\n",
    "To streamline your workflow, we've wrapped common operations into **modular utility functions**. These abstract away repetitive code and let you focus on your analysis—not setup.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Why Use These Functions?\n",
    "\n",
    "Instead of writing long, error-prone blocks of boilerplate, you can execute key tasks using concise, high-level commands. These functions handle:\n",
    "\n",
    "* Environment setup\n",
    "* API connection testing\n",
    "* LLM-based filtering\n",
    "* JSON merging\n",
    "* BibTeX export\n",
    "\n",
    "> ✅ You only need to supply minimal inputs (like `cfg` or model name), and the functions will take care of the rest.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Available Functions\n",
    "\n",
    "| Function                                                           | Purpose                                                           |\n",
    "| ------------------------------------------------------------------ | ----------------------------------------------------------------- |\n",
    "| `create_env_file(env_path, ...)`                                   | Creates a `.env` file with placeholders or prefilled API keys     |\n",
    "| `test_openai_connection(model='gpt-4o')`                           | Tests your OpenAI key and prints a basic response                 |\n",
    "| `run_llm_filtering_pipeline_colab(model_name, cfg, placeholders)`  | Executes the LLM-based abstract filtering pipeline                |\n",
    "| `combine_methodology_json_colab(cfg, model_name_method_extractor)` | Merges multiple JSON outputs into one `combined_output.json`      |\n",
    "| `generate_bibtex_from_excel_colab(cfg)`                            | Converts a filtered Excel sheet into a `.bib` file for references |\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Reminder\n",
    "\n",
    "These functions are preloaded into the `academic_paper_maker.helper.google_colab` module to make your notebook cleaner, more modular, and easier to maintain.\n",
    "\n",
    "> ⚡ Use them as building blocks in your Colab workflow — no need to copy-paste long setup code every time.\n",
    "\n",
    "---\n"
   ],
   "id": "dcafd95301d4fa1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, AuthenticationError, RateLimitError, APIConnectionError\n",
    "\n",
    "from academic_paper_maker.setting.project_path import project_folder\n",
    "from academic_paper_maker.helper.combine_json import combine_json_files\n",
    "from academic_paper_maker.post_code_saviour.excel_to_bib import generate_bibtex\n",
    "\n",
    "\n",
    "\n",
    "# Path to the directory where \"research_filter\" lives\n",
    "project_root = \"/content/academic_paper_maker\"\n",
    "\n",
    "# Add it to Python's module search path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from research_filter.auto_llm import run_pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_bibtex_from_excel_colab(cfg: dict) -> None:\n",
    "    \"\"\"\n",
    "    Generate a BibTeX file from a filtered Excel database.\n",
    "\n",
    "    Parameters:\n",
    "        cfg (dict): Configuration dictionary containing at least:\n",
    "            - project_review (str)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load project paths\n",
    "    path_dic = project_folder(project_review=cfg['project_review'])\n",
    "    main_folder = path_dic['main_folder']\n",
    "\n",
    "    # Define input and output paths\n",
    "    input_excel = os.path.join(main_folder, 'database', 'combined_filtered.xlsx')\n",
    "    output_bib = os.path.join(main_folder, 'database', 'filtered_output.bib')\n",
    "\n",
    "    # Load the filtered Excel file\n",
    "    df = pd.read_excel(input_excel)\n",
    "\n",
    "    # Generate BibTeX file\n",
    "    generate_bibtex(df, output_file=output_bib)\n",
    "\n",
    "    print(f\"✅ BibTeX file saved to: {output_bib}\")\n",
    "\n",
    "\n",
    "def create_env_file(env_path, openai_api_key=None, gemini_api_key=None):\n",
    "    \"\"\"\n",
    "    Creates a .env file with placeholders or provided API keys.\n",
    "\n",
    "    Parameters:\n",
    "    - env_path (str): Full path to the .env file (e.g., \"/content/my_project/.env\").\n",
    "    - openai_api_key (str, optional): OpenAI API key (e.g., \"sk-...\").\n",
    "    - gemini_api_key (str, optional): Gemini (Google AI) API key.\n",
    "    \"\"\"\n",
    "    lines = [\n",
    "        \"# Paste your API keys below.\",\n",
    "        \"# OpenAI example: OPENAI_API_KEY=sk-...\",\n",
    "        \"# Gemini (Google AI) example: GEMINI_API_KEY=your-google-api-key\",\n",
    "        \"\",\n",
    "    ]\n",
    "\n",
    "    lines.append(f\"OPENAI_API_KEY={openai_api_key}\" if openai_api_key else \"# OPENAI_API_KEY=\")\n",
    "    lines.append(f\"GEMINI_API_KEY={gemini_api_key}\" if gemini_api_key else \"# GEMINI_API_KEY=\")\n",
    "\n",
    "    try:\n",
    "        with open(env_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "        print(f\"✅ `.env` file created at: {env_path}\")\n",
    "        print(\"👉 In Colab, click the folder icon on the left, go to the appropriate folder, right-click `.env`, and choose 'Edit' to enter your keys.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create `.env`: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_openai_connection(model: str = \"gpt-4o\") -> None:\n",
    "    \"\"\"\n",
    "    Loads the OpenAI API key from a .env file and sends a test message\n",
    "    to verify the connection to the OpenAI Chat Completion API.\n",
    "\n",
    "    Parameters:\n",
    "        model (str): The model name to test (default: \"gpt-4o\")\n",
    "\n",
    "    Expected Output:\n",
    "        - ✅ Confirmation that the API call succeeded\n",
    "        - 🤖 The assistant's response to \"Hello, what is 2 + 2?\"\n",
    "    \"\"\"\n",
    "\n",
    "    # 🔄 Load environment variables from the .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # 🔑 Fetch the OpenAI API key\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"❌ OPENAI_API_KEY not found. Please make sure it is set in your .env file.\")\n",
    "\n",
    "    # ✅ Initialize the OpenAI client\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    try:\n",
    "        # 📤 Send a test prompt to the model\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Hello, what is 2 + 2?\"}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 🟢 Print the result if successful\n",
    "        print(\"✅ API call successful!\")\n",
    "        print(\"🤖 Response:\", response.choices[0].message.content)\n",
    "\n",
    "    except AuthenticationError:\n",
    "        print(\"❌ Authentication failed. Please check your OPENAI_API_KEY.\")\n",
    "    except RateLimitError:\n",
    "        print(\"⚠️ Rate limit exceeded. Wait and try again later.\")\n",
    "    except APIConnectionError:\n",
    "        print(\"📡 Network issue or OpenAI server unavailable. Check your connection.\")\n",
    "    # except InvalidRequestError as e:\n",
    "    #     print(f\"🚫 Invalid request to the API: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❗ An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_llm_filtering_pipeline_colab(\n",
    "        model_name: str,\n",
    "        cfg: dict,\n",
    "        placeholders: dict\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run the LLM-based abstract filtering pipeline in Google Colab.\n",
    "\n",
    "    Parameters:\n",
    "        model_name (str): The LLM to use (e.g., \"gpt-4o\", \"gpt-4o-mini\")\n",
    "        cfg (dict): Configuration dictionary with keys:\n",
    "            - project_review\n",
    "            - config_file\n",
    "            - project_root\n",
    "            - yaml_path\n",
    "            - main_folder\n",
    "        placeholders (dict): Dictionary with placeholder text for the LLM (e.g., topic and context)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Initializing LLM filtering pipeline...\")\n",
    "\n",
    "        # Ensure project_root is in sys.path\n",
    "        if cfg['project_root'] not in sys.path:\n",
    "            sys.path.append(cfg['project_root'])\n",
    "\n",
    "        # Get project paths\n",
    "        path_dic = project_folder(\n",
    "            project_review=cfg['project_review'],\n",
    "            config_file=cfg['config_file']\n",
    "        )\n",
    "        main_folder = path_dic['main_folder']\n",
    "        csv_path = path_dic['database_path']\n",
    "\n",
    "        # Agent configuration\n",
    "        agentic_setting = {\n",
    "            \"agent_name\": \"abstract_filter\",\n",
    "            \"column_name\": \"abstract_filter\",\n",
    "            \"yaml_path\": cfg['yaml_path'],\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "\n",
    "        # Output folders\n",
    "        methodology_json_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'json_output')\n",
    "        multiple_runs_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'multiple_runs_folder')\n",
    "        final_cross_check_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'final_cross_check_folder')\n",
    "\n",
    "        # Processing configuration\n",
    "        process_setup = {\n",
    "            'batch_process': False,\n",
    "            'manual_paste_llm': False,\n",
    "            'iterative_confirmation': False,\n",
    "            'overwrite_csv': True,\n",
    "            'cross_check_enabled': False,\n",
    "            'cross_check_runs': 3,\n",
    "            'cross_check_agent_name': 'agent_cross_check',\n",
    "            'cleanup_json': False\n",
    "        }\n",
    "\n",
    "        # Run the LLM-based filtering\n",
    "        run_pipeline(\n",
    "            agentic_setting,\n",
    "            process_setup,\n",
    "            placeholders=placeholders,\n",
    "            csv_path=csv_path,\n",
    "            main_folder=main_folder,\n",
    "            methodology_json_folder=methodology_json_folder,\n",
    "            multiple_runs_folder=multiple_runs_folder,\n",
    "            final_cross_check_folder=final_cross_check_folder,\n",
    "        )\n",
    "\n",
    "        logging.info(\"✅ LLM filtering pipeline completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ An error occurred while running the LLM filtering pipeline: {e}\")\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# def combine_methodology_json_colab(cfg: dict, model_name_method_extractor: str) -> None:\n",
    "#     \"\"\"\n",
    "#     Combine individual JSON files from the methodology gap extractor into a single combined JSON file.\n",
    "#\n",
    "#     Parameters:\n",
    "#         cfg (dict): Configuration dictionary with keys:\n",
    "#             - project_review\n",
    "#             - config_file\n",
    "#             - main_folder\n",
    "#             - methodology_gap_extractor_path\n",
    "#         model_name_method_extractor (str): Name of the model used in the methodology gap extractor step.\n",
    "#                                            This determines the subdirectory to read JSONs from.\n",
    "#\n",
    "#     Raises:\n",
    "#         FileNotFoundError: If the input directory does not exist.\n",
    "#\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#\n",
    "#     # Load paths from registered project\n",
    "#     path_dict = project_folder(\n",
    "#         project_review=cfg['project_review'],\n",
    "#         config_file=cfg['config_file']\n",
    "#     )\n",
    "#\n",
    "#     # Define input and output paths\n",
    "#     input_dir = Path(cfg['methodology_gap_extractor_path']) / model_name_method_extractor\n",
    "#     output_file = os.path.join(path_dict['main_folder'], 'combined_output.json')\n",
    "#\n",
    "#     # Validate input directory\n",
    "#     if not input_dir.exists():\n",
    "#         raise FileNotFoundError(f\"Input directory not found: {input_dir}\")\n",
    "#\n",
    "#     # Combine JSON files into one\n",
    "#     combine_json_files(\n",
    "#         input_directory=input_dir,\n",
    "#         output_file=output_file\n",
    "#     )\n",
    "#\n",
    "#     print(f\"✅ Combined JSON saved to: {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "def run_abstract_filtering_colab(model_name: str, placeholders: dict, cfg: dict) -> None:\n",
    "    \"\"\"\n",
    "    Run the abstract filtering pipeline using a specified LLM in a Colab environment.\n",
    "\n",
    "    Parameters:\n",
    "        model_name (str): The LLM to use (e.g., \"gpt-4o\", \"gpt-4o-mini\")\n",
    "        placeholders (dict): Dictionary with keys like \"topic\" and \"topic_context\"\n",
    "        cfg (dict): Configuration dictionary containing:\n",
    "            - project_review\n",
    "            - config_file\n",
    "            - project_root\n",
    "            - yaml_path\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"🚀 Starting abstract filtering pipeline...\")\n",
    "\n",
    "        # Ensure project root is available\n",
    "        if cfg['project_root'] not in sys.path:\n",
    "            sys.path.append(cfg['project_root'])\n",
    "\n",
    "        # Load project folder structure\n",
    "        path_dic = project_folder(\n",
    "            project_review=cfg['project_review'],\n",
    "            config_file=cfg['config_file']\n",
    "        )\n",
    "        main_folder = path_dic['main_folder']\n",
    "        csv_path = path_dic['database_path']\n",
    "\n",
    "        # Agent configuration\n",
    "        agentic_setting = {\n",
    "            \"agent_name\": \"abstract_filter\",\n",
    "            \"column_name\": \"abstract_filter\",\n",
    "            \"yaml_path\": cfg['yaml_path'],\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "\n",
    "        # Define output folders\n",
    "        methodology_json_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'json_output')\n",
    "        multiple_runs_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'multiple_runs_folder')\n",
    "        final_cross_check_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'final_cross_check_folder')\n",
    "\n",
    "        # Runtime options\n",
    "        process_setup = {\n",
    "            'batch_process': False,\n",
    "            'manual_paste_llm': False,\n",
    "            'iterative_confirmation': False,\n",
    "            'overwrite_csv': True,\n",
    "            'cross_check_enabled': False,\n",
    "            'cross_check_runs': 3,\n",
    "            'cross_check_agent_name': 'agent_cross_check',\n",
    "            'cleanup_json': False\n",
    "        }\n",
    "\n",
    "        # Run the pipeline\n",
    "        run_pipeline(\n",
    "            agentic_setting,\n",
    "            process_setup,\n",
    "            placeholders=placeholders,\n",
    "            csv_path=csv_path,\n",
    "            main_folder=main_folder,\n",
    "            methodology_json_folder=methodology_json_folder,\n",
    "            multiple_runs_folder=multiple_runs_folder,\n",
    "            final_cross_check_folder=final_cross_check_folder,\n",
    "        )\n",
    "\n",
    "        logging.info(\"✅ Abstract filtering completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ An error occurred during the abstract filtering process: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def run_methodology_extraction_colab(\n",
    "        model_name_method_extractor: str,\n",
    "        agent_name: str,\n",
    "        placeholders: dict,\n",
    "        cfg: dict\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run the methodology gap extraction pipeline using a specified model and agent in Colab.\n",
    "\n",
    "    Parameters:\n",
    "        model_name_method_extractor (str): LLM model to use (e.g., \"gpt-4o-mini\")\n",
    "        agent_name (str): Name of the agent configuration (e.g., \"methodology_gap_extractor\")\n",
    "        placeholders (dict): Dictionary with keys like \"topic\" and \"topic_context\"\n",
    "        cfg (dict): Configuration dictionary with keys:\n",
    "            - project_review\n",
    "            - config_file\n",
    "            - yaml_path\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"🚀 Starting methodology gap extraction pipeline...\")\n",
    "\n",
    "        # Load project folder paths\n",
    "        path_dic = project_folder(\n",
    "            project_review=cfg['project_review'],\n",
    "            config_file=cfg['config_file']\n",
    "        )\n",
    "        main_folder = path_dic['main_folder']\n",
    "        csv_path = path_dic['database_path']\n",
    "\n",
    "        # Set up the agent\n",
    "        agentic_setting = {\n",
    "            \"agent_name\": agent_name,\n",
    "            \"column_name\": \"methodology_gap\",\n",
    "            \"yaml_path\": cfg['yaml_path'],\n",
    "            \"model_name\": model_name_method_extractor\n",
    "        }\n",
    "\n",
    "        # Define output folders\n",
    "        methodology_json_folder = os.path.join(\n",
    "            main_folder, agent_name, 'json_output', model_name_method_extractor\n",
    "        )\n",
    "        multiple_runs_folder = os.path.join(main_folder, agent_name, 'multiple_runs_folder')\n",
    "        final_cross_check_folder = os.path.join(main_folder, agent_name, 'final_cross_check_folder')\n",
    "\n",
    "        # Runtime options\n",
    "        process_setup = {\n",
    "            'batch_process': False,\n",
    "            'manual_paste_llm': False,\n",
    "            'iterative_confirmation': False,\n",
    "            'overwrite_csv': True,\n",
    "            'cross_check_enabled': False,\n",
    "            'cross_check_runs': 3,\n",
    "            'cross_check_agent_name': 'agent_cross_check',\n",
    "            'cleanup_json': False,\n",
    "            'used_abstract': True  # Always enable abstract fallback\n",
    "        }\n",
    "\n",
    "        # Run the pipeline\n",
    "        run_pipeline(\n",
    "            agentic_setting,\n",
    "            process_setup,\n",
    "            placeholders=placeholders,\n",
    "            csv_path=csv_path,\n",
    "            main_folder=main_folder,\n",
    "            methodology_json_folder=methodology_json_folder,\n",
    "            multiple_runs_folder=multiple_runs_folder,\n",
    "            final_cross_check_folder=final_cross_check_folder,\n",
    "        )\n",
    "\n",
    "        logging.info(\"✅ Methodology extraction completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ An error occurred during the methodology extraction: {e}\")\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from setting.project_path import project_folder\n",
    "from helper.combine_json import combine_json_files\n",
    "import logging\n",
    "\n",
    "def combine_methodology_output_colab(model_name_method_extractor: str, cfg: dict) -> None:\n",
    "    \"\"\"\n",
    "    Combine individual JSON files from the methodology gap extractor into a single combined JSON.\n",
    "\n",
    "    Parameters:\n",
    "        model_name_method_extractor (str): Name of the model used in the methodology gap extractor step\n",
    "        cfg (dict): Configuration dictionary containing:\n",
    "            - project_review\n",
    "            - config_file\n",
    "            - methodology_gap_extractor_path\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"🔄 Starting to combine methodology gap JSON files...\")\n",
    "\n",
    "        # Load project paths\n",
    "        path_dict = project_folder(\n",
    "            project_review=cfg['project_review'],\n",
    "            config_file=cfg['config_file']\n",
    "        )\n",
    "\n",
    "        # Define input and output paths\n",
    "        input_dir = Path(cfg['methodology_gap_extractor_path']) / model_name_method_extractor\n",
    "        output_file = os.path.join(path_dict['main_folder'], 'combined_output.json')\n",
    "\n",
    "        # Validate input directory\n",
    "        if not input_dir.exists():\n",
    "            raise FileNotFoundError(f\"❌ Input directory not found: {input_dir}\")\n",
    "\n",
    "        # Combine JSONs into one file\n",
    "        combine_json_files(\n",
    "            input_directory=input_dir,\n",
    "            output_file=output_file\n",
    "        )\n",
    "\n",
    "        logging.info(f\"✅ Combined JSON saved to: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Failed to combine JSON files: {e}\")\n"
   ],
   "id": "f0d6f70cd657b263"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🔑 **Step 3: Obtain Your API Keys**\n",
    "\n",
    "Before running any LLM-based tools, you'll need to obtain API keys for **OpenAI** and optionally **Google Gemini**, if you plan to use both.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Why You Need This\n",
    "\n",
    "API keys are required to authenticate your access to models like `gpt-4o` or `gemini-pro`. Without them, the system won’t be able to connect to the services.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔐 Where to Get Your Keys\n",
    "\n",
    "| Provider   | Key Name         | Get It From                                                          |\n",
    "| ---------- | ---------------- | -------------------------------------------------------------------- |\n",
    "| **OpenAI** | `OPENAI_API_KEY` | [platform.openai.com/api-keys](https://platform.openai.com/api-keys) |\n",
    "| **Gemini** | `GEMINI_API_KEY` | [aistudio.google.com](https://aistudio.google.com/apikey)            |\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 What To Do With Them Now\n",
    "\n",
    "Just **copy and save your API keys in a safe place** (e.g., a password manager or text file on your machine).\n",
    "\n",
    "> 📝 You do **not** need to put them in a `.env` file yet.\n",
    "\n",
    "In the **next step**, you’ll insert them into a configuration file automatically using a helper function.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Example (Save for Later)\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "GEMINI_API_KEY=AIzaSy...\n",
    "```\n",
    "\n",
    "> ⚠️ Do not share these keys with anyone or post them online.\n"
   ],
   "id": "340c081a2edc466"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🔧 Step 4: Create and Register Your Project Folder\n",
    "\n",
    "In this step, you'll register the **main project folder**, so the system knows where to store and retrieve all files related to your project.\n",
    "\n",
    "This setup automatically creates a structured folder system under your specified `main_folder`, and stores the configuration in a central JSON file. This ensures your projects remain organized, especially when working with multiple studies or datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Project Folder Structure\n",
    "\n",
    "On the first run, the following folder structure will be automatically created under your specified `main_folder`:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│   └── <project_name>_database.xlsx  ← auto-generated path (not file creation)\n",
    "├── pdf/\n",
    "└── xml/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Example\n",
    "\n",
    "Suppose your project name is `corona_discharge`, and you want to store all project files under:\n",
    "\n",
    "```\n",
    "G:\\My Drive\\research_related\\ear_eog\n",
    "```\n",
    "\n",
    "You can register this setup by running:\n",
    "\n",
    "```python\n",
    "project_folder(\n",
    "    project_review='corona_discharge',\n",
    "    main_folder=r'G:\\My Drive\\research_related\\ear_eog'\n",
    ")\n",
    "```\n",
    "\n",
    "✅ This will:\n",
    "\n",
    "* Save the project path in `setting/project_folders.json`\n",
    "* Create the full folder structure: `database/scopus`, `pdf`, and `xml`\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Loading the Project Later\n",
    "\n",
    "Once registered, you can access the project folders in future sessions by providing just the `project_review` name:\n",
    "\n",
    "```python\n",
    "paths = project_folder(project_review='corona_discharge')\n",
    "\n",
    "print(paths['main_folder'])  # Main project folder\n",
    "print(paths['csv_path'])     # Path to the Excel database file\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ What Happens in the Background?\n",
    "\n",
    "* A file named `project_folders.json` is stored in the `setting/` directory within your project.\n",
    "* It maps each `project_review` to its corresponding `main_folder`.\n",
    "* Folder structure is created automatically on the first run.\n",
    "* On subsequent runs, the system reads the JSON to locate your project — no need to re-enter paths.\n"
   ],
   "id": "f948aeff3b509926"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from academic_paper_maker.setting.project_path import project_folder\n",
    "\n",
    "cfg={\n",
    "    \"project_review\": \"wafer\",\n",
    "    \"env_path\": \"/content/.env\",\n",
    "    \"main_folder\": \"/content/my_project\",  # Use a Unix-style path for Colab\n",
    "    \"project_root\": \"/content/my_project/database\",\n",
    "    'yaml_path': '/content/academic_paper_maker/research_filter/agent/abstract_check.yaml',\n",
    "    'config_file':'/content/setting/project_folders.json',\n",
    "    'methodology_gap_extractor_path': '/content/my_project/methodology_gap_extractor/json_output',\n",
    "}\n",
    "\n",
    "\n",
    "project_folder(project_review=cfg['project_review'], main_folder=cfg['main_folder'])\n",
    "\n",
    "print('Success')"
   ],
   "id": "2392925d0cc32d92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 🔐 **Step 5: Create a `.env` File to Store API Keys**\n",
    "\n",
    "Now that you’ve obtained your API keys, the next step is to securely store them in a `.env` file inside your project directory. This ensures your credentials are protected and accessible by your code—without hardcoding them.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ What This Step Does\n",
    "\n",
    "This step will:\n",
    "\n",
    "* ✅ Create a `.env` file at the path defined by `cfg[\"env_path\"]`\n",
    "* ✅ Add helpful comments inside the file so you know where to paste your keys\n",
    "* ✅ Support both `OPENAI_API_KEY` and `GEMINI_API_KEY` (also known as `GOOGLE_API_KEY`)\n",
    "* ✅ Ensure compatibility with tools like `python-dotenv` for easy runtime access\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ How to Manually Edit the File\n",
    "\n",
    "1. Click the 📁 **folder icon** on the left sidebar\n",
    "2. Find `.env` in your project folder (e.g., `/content/my_project`)\n",
    "3. Right-click it → **Edit**\n",
    "4. Paste your keys in this format:\n",
    "\n",
    "```env\n",
    "# 🔑 OpenAI API key\n",
    "OPENAI_API_KEY=sk-...\n",
    "\n",
    "# 🔑 Google Gemini API key\n",
    "GEMINI_API_KEY=AIzaSy...\n",
    "```\n",
    "\n",
    "> 💡 `GEMINI_API_KEY` may also be labeled as `GOOGLE_API_KEY` in some SDKs—they’re interchangeable.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔒 Best Practices\n",
    "\n",
    "* ❌ Never share or commit your `.env` file\n",
    "* ✅ Add `.env` to `.gitignore` if using Git\n",
    "* 🔄 Use `load_dotenv()` in your scripts to access the keys safely\n",
    "\n",
    "> This step sets up a secure foundation for accessing LLMs in your project.\n",
    "\n",
    "---"
   ],
   "id": "15ba22d74294c815"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Minimal (user will edit manually later)\n",
    "create_env_file(cfg[\"env_path\"])\n",
    "\n",
    "# Or with keys already known\n",
    "create_env_file(cfg[\"env_path\"],\n",
    "                openai_api_key=\"sk-abc123\",\n",
    "                # gemini_api_key=\"AIzaSy...\"\n",
    "                )"
   ],
   "id": "1b7c6064e3d64195"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# ⚙️ **Step 5a: Load API Key and Test OpenAI Connection**\n",
    "\n",
    "After setting up your `.env` file, it's important to **verify that your OpenAI API key is working correctly**. This step uses a built-in helper function to test the connection.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 What This Step Does\n",
    "\n",
    "* Loads your API key from the `.env` file\n",
    "* Initializes an OpenAI client with that key\n",
    "* Sends a simple test message to the GPT model (e.g., `gpt-4o`)\n",
    "* Confirms success or provides a clear error message if something goes wrong\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ How to Run It\n",
    "\n",
    "Just call the helper function:\n",
    "\n",
    "```python\n",
    "from academic_paper_maker.helper.google_colab import test_openai_connection\n",
    "\n",
    "test_openai_connection()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Expected Outcome\n",
    "\n",
    "If everything is set up correctly, you'll see output like:\n",
    "\n",
    "```\n",
    "✅ API call successful!\n",
    "🤖 Response: 2 + 2 is 4.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ If Something Goes Wrong\n",
    "\n",
    "Common errors and their meanings:\n",
    "\n",
    "| Error Type             | What It Means                                 |\n",
    "| ---------------------- | --------------------------------------------- |\n",
    "| ❌ AuthenticationError  | API key is missing or incorrect               |\n",
    "| ⚠️ RateLimitError      | You’re sending too many requests too quickly  |\n",
    "| 📡 APIConnectionError  | Network issue or OpenAI server is unreachable |\n",
    "| 🚫 InvalidRequestError | Incorrect model name or bad request structure |\n",
    "\n",
    "Make sure your `.env` file includes a valid key in this format:\n",
    "\n",
    "```env\n",
    "OPENAI_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "> 🔁 Re-run the test after fixing the `.env` or internet connection if needed.\n",
    "\n",
    "---\n"
   ],
   "id": "e86a255673fda1f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"🔍 Testing OpenAI connection... Please wait.\\n\")\n",
    "\n",
    "# Run the test\n",
    "test_openai_connection()\n",
    "\n",
    "print(\"\\n✅ If the connection is successful, you should see a response from the assistant above.and the answer should be something like:  Response: Hello! The sum of 2 + 2 is 4.\")\n",
    "print(\"⚠️ If not, please check your `.env` file and verify that your `OPENAI_API_KEY` is correct.\")"
   ],
   "id": "63f6c4f98891cf7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📥 Step 6: Download the Scopus BibTeX File\n",
    "\n",
    "In this step, you'll use the **Scopus database** to find and download relevant papers for your project. Scopus is a comprehensive and widely-used repository of peer-reviewed academic literature.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Using Scopus Advanced Search\n",
    "\n",
    "To retrieve high-quality and relevant papers, we recommend using **Scopus' Advanced Search** feature. This powerful tool lets you refine your search based on:\n",
    "\n",
    "* Keywords\n",
    "* Authors\n",
    "* Publication dates\n",
    "* Document types\n",
    "* And more...\n",
    "\n",
    "This ensures that your literature collection is both targeted and comprehensive.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Get Keyword Ideas with a Prompt\n",
    "\n",
    "To help you formulate effective search queries, you can use the following **prompt-based suggestion tool**:\n",
    "\n",
    "👉 [Keyword Search Prompt](https://gist.github.com/balandongiv/886437963d38252e61634ddc00b9d983)\n",
    "\n",
    "You may need to modify the prompt to better suit your research domain. Here are some example domains:\n",
    "\n",
    "* `\"corona discharge\"`\n",
    "* `\"fatigue driving EEG\"`\n",
    "* `\"wafer classification\"`\n",
    "\n",
    "Feel free to add, remove, or tweak keywords as needed to refine your search results.\n",
    "\n",
    "---\n",
    "\n",
    "## 💾 Save and Organize Your Results\n",
    "\n",
    "Once you've finalized your search:\n",
    "\n",
    "1. **Select all available attributes** when exporting results from Scopus.\n",
    "2. # Access Scopus\n",
    "![Scopus CSV Export](image/scopus_csv_export.png)\n",
    "\n",
    "\n",
    "2. Choose the **BibTeX** format when saving the export file.\n",
    "3. Save the file inside the `database/scopus/` folder of your project.\n",
    "\n",
    "The resulting folder structure might look like this:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── scopus(1).bib\n",
    "│       ├── scopus(2).bib\n",
    "│       ├── scopus(3).bib\n",
    "```\n",
    "\n",
    "Make sure the BibTeX files are correctly named and stored to ensure smooth integration in later steps."
   ],
   "id": "791752803ba495d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📊 Step 7: Combine Scopus BibTeX Files into Excel\n",
    "\n",
    "Once you've downloaded multiple `.bib` files from Scopus, the next step is to **combine and convert** them into a structured Excel file. This makes it easier to filter, sort, and review the metadata of all collected papers.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 What This Step Does\n",
    "\n",
    "* Loads all `.bib` files from your project's `database/scopus/` folder\n",
    "* Parses the relevant metadata (e.g., title, authors, year, source, DOI)\n",
    "* Combines the results into a single Excel spreadsheet\n",
    "* Saves the spreadsheet in the `database/` folder as `combined_filtered.xlsx`\n",
    "\n",
    "\n",
    "\n",
    "## 📁 Folder Structure Example\n",
    "\n",
    "After running the script, your folder might look like this:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   ├── scopus/\n",
    "│   │   ├── scopus(1).bib\n",
    "│   │   ├── scopus(2).bib\n",
    "│   │   ├── scopus(3).bib\n",
    "│   └── combined_filtered.xlsx\n",
    "```\n",
    "\n",
    "This Excel file will serve as your primary reference for filtering papers before downloading PDFs.\n"
   ],
   "id": "72c1fb6183a242ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from academic_paper_maker.download_pdf.database_preparation import combine_scopus_bib_to_excel\n",
    "from academic_paper_maker.setting.project_path import project_folder\n",
    "\n",
    "# project_review='corona_discharge'\n",
    "path_dic=project_folder(project_review=cfg['project_review'])\n",
    "folder_path=path_dic['scopus_path']\n",
    "output_excel =  path_dic['database_path']\n",
    "combine_scopus_bib_to_excel(folder_path, output_excel)"
   ],
   "id": "b5539ae56b1485ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 8: Automatically Filter Relevant References Using LLM**\n",
    "\n",
    "After retrieving thousands of BibTeX references from Scopus (**Step 3**) and combining them into an Excel file (`combined_filtered.xlsx`) in **Step 4**, you'll likely find many entries irrelevant to your specific research focus.\n",
    "\n",
    "In this step, we use a **Large Language Model (LLM)** to classify abstracts based on a defined research topic and context, eliminating the need for tedious manual filtering.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ **What This Step Does**\n",
    "\n",
    "* Loads abstracts from `combined_filtered.xlsx`\n",
    "* Applies a **custom LLM prompt** to assess relevance\n",
    "* Adds a new column to the Excel file with `True`/`False` labels:\n",
    "\n",
    "  * ✅ `True` → Relevant\n",
    "  * ❌ `False` → Not relevant\n",
    "\n",
    "This process is based on a topic-specific prompt defined by you.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 **LLM Prompt Customization**\n",
    "\n",
    "For the automated relevance filtering to work effectively, you **must provide two key inputs** to guide the LLM:\n",
    "\n",
    "* `topic`: A concise statement of your **specific research goal or area of interest**.\n",
    "  *Example:* `\"wafer defect classification\"`\n",
    "\n",
    "* `topic_context`: A description of the **broader scientific or industrial context** where your topic belongs.\n",
    "  *Example:* `\"semiconductor manufacturing and inspection\"`\n",
    "\n",
    "These inputs help the LLM understand what kinds of abstracts are considered relevant and which ones should be filtered out.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 *Not sure how to define your `topic` and `topic_context`?*\n",
    "\n",
    "You can get help from an LLM to generate these values. Here’s how:\n",
    "\n",
    "1. **Open the filtering prompt** template defined in the YAML file:\n",
    "\n",
    "   ```\n",
    "   research_filter/agent/abstract_check.yaml\n",
    "   ```\n",
    "\n",
    "2. **Copy the prompt structure** (including the placeholders for `topic` and `topic_context`).\n",
    "\n",
    "3. **Ask any LLM to assist**, by providing it the YAML prompt and a description of your research.\n",
    "   For example, you could say:\n",
    "\n",
    "   > 🧠 *\"Given the following prompt structure, and knowing that my research is about identifying AI-generated academic papers, can you help me fill in the `topic` and `topic_context` placeholders?\"*\n",
    "\n",
    "4. The LLM might then suggest:\n",
    "\n",
    "   ```yaml\n",
    "   topic: \"AI-generated academic paper detection\"\n",
    "   topic_context: \"scientific publishing and machine learning ethics\"\n",
    "   ```\n",
    "\n",
    "This approach ensures your filtering prompt is both precise and contextually grounded, improving the accuracy of the classification.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ **Configuration via `agentic_setting`**\n",
    "\n",
    "The entire filtering behavior is controlled by a configuration dictionary called `agentic_setting`:\n",
    "\n",
    "```python\n",
    "agentic_setting = {\n",
    "    \"agent_name\": \"abstract_filter\",     # The identifier for the agent logic (must match YAML)\n",
    "    \"column_name\": \"abstract_filter\",    # Name of the column added to the Excel file\n",
    "    \"yaml_path\": yaml_path,              # Path to the YAML file defining agent behavior\n",
    "    \"model_name\": model_name             # Name of the LLM model to use\n",
    "}\n",
    "```\n",
    "\n",
    "### 🔍 Parameter Breakdown:\n",
    "\n",
    "| Key           | Description                                                             |\n",
    "| ------------- | ----------------------------------------------------------------------- |\n",
    "| `agent_name`  | Matches the name of the agent defined in the YAML configuration file.   |\n",
    "| `column_name` | The name of the new column in the Excel file where results are saved.   |\n",
    "| `yaml_path`   | Path to the YAML file containing the agent's logic and prompt template. |\n",
    "| `model_name`  | The specific LLM model used (e.g., `\"gpt-4\"` or `\"claude-3-opus\"`).     |\n",
    "\n",
    "---\n",
    "\n",
    "## 📂 **File and Folder Structure**\n",
    "\n",
    "To avoid redundant LLM calls (which can be costly), results are cached as JSON files:\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       └── combined_filtered.xlsx           ← Updated with filtering results\n",
    "├── abstract_filter/\n",
    "│   └── json_output/\n",
    "│       ├── kim_2019.json\n",
    "│       ├── smith_2020.json\n",
    "│       └── ...\n",
    "```\n",
    "\n",
    "* Each abstract’s filtering result is saved individually.\n",
    "* If the run encounters an error, it can resume without reprocessing previous abstracts.\n",
    "* These files are later reused for **cross-checking** and **final review**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📤 **Excel Output Behavior**\n",
    "\n",
    "Depending on the `overwrite_csv` setting:\n",
    "\n",
    "* `True` → Updates the original `combined_filtered.xlsx`\n",
    "* `False` → Creates a new file (e.g., `combined_filtered_updated.xlsx`)\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ **Caution: Manual Review is Still Required**\n",
    "\n",
    "> ❗ **LLMs are powerful but not perfect**. They may misclassify edge cases or ambiguous abstracts.\n",
    ">\n",
    "> 🔍 Always **manually inspect** the final results before using them for publication or decision-making."
   ],
   "id": "cbe3a3d9948e9dfa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "placeholders = {\n",
    "    \"topic\": \"wafer defect classification\",\n",
    "    \"topic_context\": \"semiconductor manufacturing and inspection\"\n",
    "}\n",
    "\n",
    "run_abstract_filtering_colab(model_name=\"gpt-4o-mini\", placeholders=placeholders, cfg=cfg)"
   ],
   "id": "9187b834ff7002b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 7: Extract Methodology Details Using LLM**\n",
    "\n",
    "At this stage, your reference list is filtered and the corresponding PDFs (or abstracts) are available. Now, the focus shifts to **extracting key methodological insights** from each paper, such as:\n",
    "\n",
    "* 🧠 Classification algorithms\n",
    "* 🛠️ Feature engineering approaches\n",
    "* 📏 Evaluation metrics\n",
    "\n",
    "This is achieved using a specialized **LLM agent** with a targeted prompt for methodology extraction.\n",
    "\n",
    "> 📌 This step works with **full manuscripts (PDF)** when available, and **falls back to abstracts** if no PDF exists. This flexibility ensures comprehensive analysis even with incomplete data.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Loads your filtered Excel or CSV file from Step 5 or 6.\n",
    "* For each relevant paper:\n",
    "\n",
    "  * If the PDF is available: extracts methodology from the full text.\n",
    "  * If the PDF is **not** available: extracts from the **abstract** instead.\n",
    "* Uses a domain-specific LLM prompt to analyze methodological content.\n",
    "* Appends the results to the existing Excel or CSV file.\n",
    "* Saves per-paper structured JSON files for advanced or customized usage.\n",
    "* Handles backups automatically if overwriting the metadata file.\n",
    "\n",
    "> ⛑️ **Safety Note**: If `'overwrite_csv': True`, a **timestamped backup** of the original `.csv` or `.xlsx` file is automatically created **in the same folder** before any updates are made. This prevents accidental corruption and allows for recovery or version tracking.\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Prompt Purpose\n",
    "\n",
    "This step uses a **domain-aware analytical agent** designed to:\n",
    "\n",
    "> “Extract methodological details (e.g., classification algorithms, feature engineering, evaluation metrics) from filtered papers relevant to a specific machine learning task.”\n",
    "\n",
    "The prompt is defined in a YAML config file (`agent_ml.yaml`) and is tailored by the agent name you provide.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Folder and Output Structure\n",
    "\n",
    "Your project directory may look like this after completing this step:\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── combined_filtered.xlsx                  ← Updated metadata with extraction results\n",
    "│       ├── combined_filtered_backup_20250508_1530.xlsx  ← Auto-generated backup (if overwrite_csv=True)\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf                              ← Saved using bibtex_key\n",
    "│   ├── kim_2019.pdf\n",
    "│   └── ...\n",
    "├── methodology_gap_extractor/\n",
    "│   ├── json_output/\n",
    "│   │   └── smith_2020.json\n",
    "│   ├── multiple_runs_folder/\n",
    "│   └── final_cross_check_folder/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Supported Models\n",
    "\n",
    "Choose from the following supported LLMs:\n",
    "\n",
    "* `\"gpt-4o\"`\n",
    "* `\"gpt-4o-mini\"`\n",
    "* `\"gpt-o3-mini\"`\n",
    "* `'gemini-1.5-pro'`\n",
    "* `'gemini-exp-1206'`\n",
    "* `'gemini-2.0-flash-thinking-exp-01-21'`\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Important Reminders\n",
    "\n",
    "* ✅ Set `used_abstract = True` if some papers lack full PDFs.\n",
    "* 🛑 **Always verify** extracted methodologies manually before using them in analysis, models, or publication. LLMs can hallucinate or misinterpret technical details.\n",
    "\n",
    "---"
   ],
   "id": "f9a902dfa2cb91b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Shortcut of below is\n",
    "run_llm_filtering_pipeline_colab"
   ],
   "id": "4d851a0cd5fb43c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "run_methodology_extraction_colab(\n",
    "    model_name_method_extractor=\"gpt-4o-mini\",\n",
    "    agent_name=\"methodology_gap_extractor\",\n",
    "    placeholders=placeholders,\n",
    "    cfg=cfg\n",
    ")\n"
   ],
   "id": "e8437991a3a3968c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 8: Draft Literature Review (Chapter 2) Using Combined JSON**\n",
    "\n",
    "Once you've extracted methodological insights in Step 9, the next logical move is to **start organizing and drafting your literature review** (typically Chapter 2 of a thesis or paper). This step outlines how to **combine extracted results** into a single, structured file — and how to use that file to produce high-quality summaries, tables, and narrative drafts using an LLM.\n",
    "\n",
    "> 📌 This is a branching step — not everyone will follow this exact path — but it's a powerful way to move from **structured data → written draft** efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Collects individual methodology JSON files.\n",
    "* Merges them into a single, unified JSON (`combined_output.json`).\n",
    "* Prepares this JSON as input for:\n",
    "\n",
    "  * Google AI Studio (e.g., Gemini Pro)\n",
    "  * GPT-based agents\n",
    "  * Jupyter notebooks or drafting pipelines\n",
    "* Enables:\n",
    "\n",
    "  * Narrative generation for Chapter 2\n",
    "  * Thematic clustering of methods\n",
    "  * Structured tables summarizing key findings\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Output Structure Example\n",
    "\n",
    "```\n",
    "corona_discharge/\n",
    "├── combined_output.json      ← ✅ Master file for summarization and drafting\n",
    "├── methodology_gap_extractor_partial_discharge/\n",
    "│   └── json_output/\n",
    "│       └── *.json            ← Individual extracted documents\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Using LLM to Generate Summary Tables\n",
    "\n",
    "With the `combined_output.json`, you can prompt an LLM to create structured tables summarizing key findings. For example:\n",
    "\n",
    "> “Using the combined JSON, generate a table with the following columns:\n",
    "> **Author**, **Machine Learning Algorithm**, **Feature Types**, **Dataset Used**, and **Performance Metrics**.”\n",
    "\n",
    "This gives you a **quick-glance overview** of the landscape, helpful both for understanding trends and citing clearly in your literature review.\n",
    "\n",
    "### ✅ Sample Columns for Summary Table:\n",
    "\n",
    "* **Author / Year**\n",
    "* **ML Algorithm(s) Used**\n",
    "* **Features**\n",
    "* **Dataset / Source**\n",
    "* **Performance (Accuracy, F1, etc.)**\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ Drafting Strategy Tips\n",
    "\n",
    "You can also use LLM prompts like:\n",
    "\n",
    "> “Based on the combined JSON, write a summary paragraph comparing the top 3 machine learning techniques used for partial discharge classification.”\n",
    "\n",
    "Or:\n",
    "\n",
    "> “Generate an introduction section discussing the evolution of feature engineering techniques in this domain.”\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Important Reminders\n",
    "\n",
    "* ✅ Make sure all JSON structures are consistent before combining.\n",
    "* 📉 Segment or cluster the JSON by subdomain if the file becomes too large.\n",
    "* 🔍 Review all generated tables and text — LLMs are **tools**, not final authorities.\n",
    "\n",
    "---"
   ],
   "id": "7ea8ae2257b2a4d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "combine_methodology_output_colab(model_name_method_extractor=\"gpt-4o-mini\", cfg=cfg)",
   "id": "4ac8c1c50a679818"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 9: Export Filtered Excel to BibTeX**\n",
    "\n",
    "After reviewing and filtering your `combined_filtered.xlsx` file, you can convert the refined list of papers back into a **BibTeX file**. This can be helpful for citation management or integration with tools like LaTeX or reference managers.\n",
    "\n",
    "---\n",
    "\n",
    "# ✨ What This Step Does\n",
    "\n",
    "* Loads the filtered Excel file containing your selected papers\n",
    "* Converts the data back into BibTeX format\n",
    "* Saves the result to a `.bib` file for easy reuse or citation\n",
    "\n",
    "\n",
    "# 📁 File Structure Example\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── combined_filtered.xlsx                  ← Filtered Excel file with selected papers\n",
    "│       ├── combined_filtered_backup_20250508_1530.xlsx  ← Auto backup if overwrite is enabled\n",
    "│       └── filtered_output.bib                     ← Newly generated BibTeX file\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf                              ← Full-text PDFs saved by bibtex_key\n",
    "│   ├── kim_2019.pdf\n",
    "│   └── ...\n",
    "├── methodology_gap_extractor_wafer_defect/\n",
    "│   ├── json_output/\n",
    "│   │   └── smith_2020.json                         ← Extracted methodology as structured JSON\n",
    "│   ├── multiple_runs_folder/\n",
    "│   └── final_cross_check_folder/\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Notes\n",
    "\n",
    "* Ensure the Excel file has standard bibliographic columns like: `title`, `author`, `year`, `journal`, `doi`, etc.\n",
    "* The function `generate_bibtex()` maps these fields into valid BibTeX entries.\n",
    "* You can open the `.bib` file in any text editor or reference manager to confirm the results."
   ],
   "id": "69a46695749d99c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from post_code_saviour.excel_to_bib import generate_bibtex\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=cfg['project_review'])\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Define input and output paths\n",
    "input_excel = os.path.join(main_folder, 'database', 'combined_filtered.xlsx')\n",
    "output_bib = os.path.join(main_folder, 'database', 'filtered_output.bib')\n",
    "\n",
    "# Load the filtered Excel file\n",
    "df = pd.read_excel(input_excel)\n",
    "\n",
    "# Generate BibTeX file\n",
    "generate_bibtex(df, output_file=output_bib)"
   ],
   "id": "c3026c463e76dec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 📄 BibTeX Generator: Use This Shortcut\n",
    "\n",
    "You can simplify the import with an alias. Here's the **shortcut version**:\n",
    "\n",
    "```python\n",
    "from academic_paper_maker.helper.google_colab import generate_bibtex_from_excel_colab as gen_bib\n",
    "```\n",
    "\n",
    "Now you can call the function like this:\n",
    "\n",
    "```python\n",
    "gen_bib(cfg=cfg)\n",
    "```\n",
    "\n",
    "✅ This will read the filtered Excel file and generate a `.bib` file for your bibliography—keeping everything clean and automatic."
   ],
   "id": "1091db873c2ccdd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from academic_paper_maker.helper.google_colab import generate_bibtex_from_excel_colab as gen_bib\n",
    "gen_bib(cfg=cfg)"
   ],
   "id": "7ec7a91033242791"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
