{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📦 **Step 1: Install Required Packages**\n",
    "\n",
    "Before running the notebook, make sure to **clone the repository** and **install all required dependencies** directly in your Colab environment.\n",
    "\n",
    "---\n",
    "\n",
    "## 📥 Clone the Repository\n",
    "\n",
    "```python\n",
    "!git clone https://github.com/balandongiv/academic_paper_maker.git\n",
    "%cd academic_paper_maker\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Install Dependencies\n",
    "\n",
    "Install all the necessary Python packages listed in `requirements.txt`:\n",
    "\n",
    "```python\n",
    "!pip install -r requirements.txt\n",
    "!pip install openai\n",
    "```\n",
    "\n",
    "> ✅ **Note:** `openai` is installed separately in case it’s not included in the `requirements.txt`.\n",
    "\n",
    "---\n"
   ],
   "id": "be071df856ca8197"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!git clone https://github.com/balandongiv/academic_paper_maker.git\n",
    "%cd academic_paper_maker\n",
    "!pip install -r requirements.txt\n",
    "# !pip install python-dotenv\n",
    "!pip install openai"
   ],
   "id": "5a7bd8e31138e4e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🔧 Step 2: Create and Register Your Project Folder\n",
    "\n",
    "In this step, you'll register the **main project folder**, so the system knows where to store and retrieve all files related to your project.\n",
    "\n",
    "This setup automatically creates a structured folder system under your specified `main_folder`, and stores the configuration in a central JSON file. This ensures your projects remain organized, especially when working with multiple studies or datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Project Folder Structure\n",
    "\n",
    "On the first run, the following folder structure will be automatically created under your specified `main_folder`:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│   └── <project_name>_database.xlsx  ← auto-generated path (not file creation)\n",
    "├── pdf/\n",
    "└── xml/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Example\n",
    "\n",
    "Suppose your project name is `corona_discharge`, and you want to store all project files under:\n",
    "\n",
    "```\n",
    "G:\\My Drive\\research_related\\ear_eog\n",
    "```\n",
    "\n",
    "You can register this setup by running:\n",
    "\n",
    "```python\n",
    "project_folder(\n",
    "    project_review='corona_discharge',\n",
    "    main_folder=r'G:\\My Drive\\research_related\\ear_eog'\n",
    ")\n",
    "```\n",
    "\n",
    "✅ This will:\n",
    "\n",
    "* Save the project path in `setting/project_folders.json`\n",
    "* Create the full folder structure: `database/scopus`, `pdf`, and `xml`\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Loading the Project Later\n",
    "\n",
    "Once registered, you can access the project folders in future sessions by providing just the `project_review` name:\n",
    "\n",
    "```python\n",
    "paths = project_folder(project_review='corona_discharge')\n",
    "\n",
    "print(paths['main_folder'])  # Main project folder\n",
    "print(paths['csv_path'])     # Path to the Excel database file\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ What Happens in the Background?\n",
    "\n",
    "* A file named `project_folders.json` is stored in the `setting/` directory within your project.\n",
    "* It maps each `project_review` to its corresponding `main_folder`.\n",
    "* Folder structure is created automatically on the first run.\n",
    "* On subsequent runs, the system reads the JSON to locate your project — no need to re-enter paths.\n"
   ],
   "id": "f948aeff3b509926"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from academic_paper_maker.setting.project_path import project_folder\n",
    "\n",
    "cfg={\n",
    "    \"project_review\": \"corona_discharge\",\n",
    "    \"env_path\": \"/content/my_project/.env\",\n",
    "    \"main_folder\": \"/content/my_project\",  # Use a Unix-style path for Colab\n",
    "    \"project_root\": \"/content/my_project/database\",\n",
    "    'yaml_path': '/content/academic_paper_maker/research_filter/agent/abstract_check.yaml',\n",
    "    'config_file':'/content/academic_paper_maker/setting/project_folders.json',\n",
    "    'methodology_gap_extractor_path': '/content/my_project/methodology_gap_extractor/json_output',\n",
    "}\n",
    "\n",
    "\n",
    "project_folder(project_review=cfg['project_review'], main_folder=cfg['main_folder'])"
   ],
   "id": "2392925d0cc32d92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# 🔐 **Step 3: Create a `.env` File for API Keys**\n",
    "\n",
    "To securely manage API access, you'll create a `.env` file inside your project directory. This file stores your credentials for services like **OpenAI** and **Gemini (Google)** — without exposing them directly in your code.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧾 What This Step Does\n",
    "\n",
    "* Automatically creates a `.env` file at the location defined by `cfg[\"env_path\"]`\n",
    "* Supports **both** `OPENAI_API_KEY` and `GEMINI_API_KEY` (a.k.a. `GOOGLE_API_KEY`)\n",
    "* Adds clear comments so users understand the purpose of each key\n",
    "* Compatible with libraries like `python-dotenv` for seamless key loading\n",
    "\n",
    "You can:\n",
    "\n",
    "* Create a **blank template** for manual editing\n",
    "* OR **prefill** with known keys during function call\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ How to Edit or Prefill Keys\n",
    "\n",
    "After running the code cell:\n",
    "\n",
    "1. Click the 📁 **folder icon** on the left sidebar in Colab\n",
    "2. Navigate to your project folder (e.g., `/content/my_project`)\n",
    "3. Locate the `.env` file and **right-click → Edit**\n",
    "4. Enter your API keys like this:\n",
    "\n",
    "```env\n",
    "# OpenAI GPT-4 / GPT-4o\n",
    "OPENAI_API_KEY=sk-...\n",
    "\n",
    "# Google Gemini (AI Studio)\n",
    "GEMINI_API_KEY=AIzaSy...\n",
    "```\n",
    "\n",
    "> 💡 `GEMINI_API_KEY` is sometimes referred to as `GOOGLE_API_KEY` in third-party SDKs — they are functionally equivalent.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛡️ Best Practices\n",
    "\n",
    "* 🔒 **Never share or commit** your `.env` file\n",
    "* 📂 Make sure `.env` is included in `.gitignore`\n",
    "* ✅ Use `load_dotenv()` to load these variables at runtime without hardcoding\n",
    "\n",
    "> This step ensures your API keys are securely stored, portable across environments, and easy to manage.\n",
    "\n",
    "---"
   ],
   "id": "15ba22d74294c815"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from academic_paper_maker.helper.google_colab import create_env_file\n",
    "# Minimal (user will edit manually later)\n",
    "create_env_file(cfg[\"env_path\"])\n",
    "\n",
    "# Or with keys already known\n",
    "create_env_file(cfg[\"env_path\"],\n",
    "                openai_api_key=\"sk-abc123\",\n",
    "                gemini_api_key=\"AIzaSy...\")\n",
    "\n"
   ],
   "id": "1b7c6064e3d64195"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# ⚙️ **Step 3a: Load API Key and Test OpenAI Connection**\n",
    "\n",
    "After setting up your `.env` file, it's important to **verify that your OpenAI API key is working correctly**. This step uses a built-in helper function to test the connection.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 What This Step Does\n",
    "\n",
    "* Loads your API key from the `.env` file\n",
    "* Initializes an OpenAI client with that key\n",
    "* Sends a simple test message to the GPT model (e.g., `gpt-4o`)\n",
    "* Confirms success or provides a clear error message if something goes wrong\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ How to Run It\n",
    "\n",
    "Just call the helper function:\n",
    "\n",
    "```python\n",
    "from academic_paper_maker.helper.google_colab import test_openai_connection\n",
    "\n",
    "test_openai_connection()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Expected Outcome\n",
    "\n",
    "If everything is set up correctly, you'll see output like:\n",
    "\n",
    "```\n",
    "✅ API call successful!\n",
    "🤖 Response: 2 + 2 is 4.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ If Something Goes Wrong\n",
    "\n",
    "Common errors and their meanings:\n",
    "\n",
    "| Error Type             | What It Means                                 |\n",
    "| ---------------------- | --------------------------------------------- |\n",
    "| ❌ AuthenticationError  | API key is missing or incorrect               |\n",
    "| ⚠️ RateLimitError      | You’re sending too many requests too quickly  |\n",
    "| 📡 APIConnectionError  | Network issue or OpenAI server is unreachable |\n",
    "| 🚫 InvalidRequestError | Incorrect model name or bad request structure |\n",
    "\n",
    "Make sure your `.env` file includes a valid key in this format:\n",
    "\n",
    "```env\n",
    "OPENAI_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "> 🔁 Re-run the test after fixing the `.env` or internet connection if needed.\n",
    "\n",
    "---\n"
   ],
   "id": "e86a255673fda1f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from academic_paper_maker.helper.google_colab import test_openai_connection\n",
    "\n",
    "print(\"🔍 Testing OpenAI connection... Please wait.\\n\")\n",
    "\n",
    "# Run the test\n",
    "test_openai_connection()\n",
    "\n",
    "print(\"\\n✅ If the connection is successful, you should see a response from the assistant above.\")\n",
    "print(\"⚠️ If not, please check your `.env` file and verify that your `OPENAI_API_KEY` is correct.\")\n"
   ],
   "id": "63f6c4f98891cf7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from academic_paper_maker.setting.project_path import project_folder\n",
    "\n",
    "project_name = 'corona_discharge'\n",
    "main_folder = '/content/my_project'  # Use a Unix-style path for Colab\n",
    "\n",
    "project_folder(project_review=project_name, main_folder=main_folder)"
   ],
   "id": "95782a760f85b5b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📥 Third Step: Download the Scopus BibTeX File\n",
    "\n",
    "In this step, you'll use the **Scopus database** to find and download relevant papers for your project. Scopus is a comprehensive and widely-used repository of peer-reviewed academic literature.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Using Scopus Advanced Search\n",
    "\n",
    "To retrieve high-quality and relevant papers, we recommend using **Scopus' Advanced Search** feature. This powerful tool lets you refine your search based on:\n",
    "\n",
    "* Keywords\n",
    "* Authors\n",
    "* Publication dates\n",
    "* Document types\n",
    "* And more...\n",
    "\n",
    "This ensures that your literature collection is both targeted and comprehensive.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Get Keyword Ideas with a Prompt\n",
    "\n",
    "To help you formulate effective search queries, you can use the following **prompt-based suggestion tool**:\n",
    "\n",
    "👉 [Keyword Search Prompt](https://gist.github.com/balandongiv/886437963d38252e61634ddc00b9d983)\n",
    "\n",
    "You may need to modify the prompt to better suit your research domain. Here are some example domains:\n",
    "\n",
    "* `\"corona discharge\"`\n",
    "* `\"fatigue driving EEG\"`\n",
    "* `\"wafer classification\"`\n",
    "\n",
    "Feel free to add, remove, or tweak keywords as needed to refine your search results.\n",
    "\n",
    "---\n",
    "\n",
    "## 💾 Save and Organize Your Results\n",
    "\n",
    "Once you've finalized your search:\n",
    "\n",
    "1. **Select all available attributes** when exporting results from Scopus.\n",
    "2. # Access Scopus\n",
    "![Scopus CSV Export](image/scopus_csv_export.png)\n",
    "\n",
    "\n",
    "2. Choose the **BibTeX** format when saving the export file.\n",
    "3. Save the file inside the `database/scopus/` folder of your project.\n",
    "\n",
    "The resulting folder structure might look like this:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── scopus(1).bib\n",
    "│       ├── scopus(2).bib\n",
    "│       ├── scopus(3).bib\n",
    "```\n",
    "\n",
    "Make sure the BibTeX files are correctly named and stored to ensure smooth integration in later steps."
   ],
   "id": "72c1fb6183a242ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from academic_paper_maker.download_pdf.database_preparation import combine_scopus_bib_to_excel\n",
    "from academic_paper_maker.setting.project_path import project_folder\n",
    "\n",
    "# project_review='corona_discharge'\n",
    "path_dic=project_folder(project_review=cfg['project_review'])\n",
    "folder_path=path_dic['scopus_path']\n",
    "output_excel =  path_dic['database_path']\n",
    "combine_scopus_bib_to_excel(folder_path, output_excel)"
   ],
   "id": "b5539ae56b1485ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from academic_paper_maker.download_pdf.database_preparation import combine_scopus_bib_to_excel\n",
    "from academic_paper_maker.setting.project_path import project_folder\n",
    "\n",
    "project_review='corona_discharge'\n",
    "path_dic=project_folder(project_review=project_review)\n",
    "folder_path=path_dic['scopus_path']\n",
    "output_excel =  path_dic['database_path']\n",
    "combine_scopus_bib_to_excel(folder_path, output_excel)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from academic_paper_maker.setting.project_path import project_folder\n",
    "import os\n",
    "import sys\n",
    "from research_filter.auto_llm import run_pipeline\n",
    "# Path to the directory where \"research_filter\" lives\n",
    "# project_root = \"/content/academic_paper_maker\"\n",
    "\n",
    "# Add it to Python's module search path\n",
    "if cfg['project_root'] not in sys.path:\n",
    "    sys.path.append(cfg['project_root'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path_dic = project_folder(project_review=cfg['project_review'],config_file=cfg['config_file'])\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Choose your LLM. or you can experiment with other models that is cheap or expensive and check the performance\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "placeholders = {\n",
    "    \"topic\": \"wafer defect classification\",\n",
    "    \"topic_context\": \"semiconductor manufacturing and inspection\"\n",
    "}\n",
    "base_dir = os.getcwd()\n",
    "# yaml_path ='/content/academic_paper_maker/research_filter/agent/abstract_check.yaml'\n",
    "# Agent configuration\n",
    "agentic_setting = {\n",
    "    \"agent_name\": \"abstract_filter\",# This is agent name as what available in yaml\n",
    "    \"column_name\": \"abstract_filter\", # This is the column name in the excel, we will use this name to save the result and the result is either True or False. While you have the flexibility to choose the column name, it is recommended to use the same name as written here as there some task that use this naming convention to clean up the llm output that being stored under the df in the column column_name. This is specifically true only for this step.\n",
    "    \"yaml_path\": cfg['yaml_path'], # This is the path to the yaml file or the agent\n",
    "    \"model_name\": model_name # This is the model name, you can choose from the available models\n",
    "}\n",
    "\n",
    "# Paths and folders\n",
    "csv_path = path_dic['database_path']\n",
    "methodology_json_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'json_output')\n",
    "multiple_runs_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'multiple_runs_folder')\n",
    "final_cross_check_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'final_cross_check_folder')\n",
    "# LLM runtime settings. If unsure, use the default settings\n",
    "process_setup = {\n",
    "    'batch_process': False,\n",
    "    'manual_paste_llm': False,\n",
    "    'iterative_confirmation': False,\n",
    "    'overwrite_csv': True,\n",
    "    'cross_check_enabled': False,\n",
    "    'cross_check_runs': 3,\n",
    "    'cross_check_agent_name': 'agent_cross_check',\n",
    "    'cleanup_json': False\n",
    "}\n",
    "\n",
    "# Run the LLM-based filtering\n",
    "run_pipeline(\n",
    "    agentic_setting,\n",
    "    process_setup,\n",
    "    placeholders=placeholders,\n",
    "    csv_path=csv_path,\n",
    "    main_folder=main_folder,\n",
    "    methodology_json_folder=methodology_json_folder,\n",
    "    multiple_runs_folder=multiple_runs_folder,\n",
    "    final_cross_check_folder=final_cross_check_folder,\n",
    ")\n"
   ],
   "id": "712bb934a2db9134",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Path to the directory where \"research_filter\" lives\n",
    "project_root = \"/content/academic_paper_maker\"\n",
    "\n",
    "# Add it to Python's module search path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from research_filter.auto_llm import run_pipeline"
   ],
   "id": "6e63882d3ca6145e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Shortcut of below is\n",
    "run_llm_filtering_pipeline_colab"
   ],
   "id": "4d851a0cd5fb43c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from research_filter.auto_llm import run_pipeline\n",
    "from setting.project_path import project_folder\n",
    "import os\n",
    "\n",
    "# Define the project\n",
    "# project_review = 'corona_discharge'\n",
    "# Select your LLM model. For this step, it is recommended to use a more powerful model like `gpt-4o` or `gemini-2.0-flash-thinking-exp-01-21` for better performance. Use model that have reasoning capability for better performance.\n",
    "\n",
    "\n",
    "model_name_method_extractor = 'gpt-4o-mini'\n",
    "agent_name = \"methodology_gap_extractor\"\n",
    "\n",
    "# config_file='/content/academic_paper_maker/setting/project_folders.json'\n",
    "\n",
    "\n",
    "path_dic = project_folder(project_review=cfg['project_review'],config_file=cfg['config_file'])\n",
    "\n",
    "\n",
    "# path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "# Construct the relative path\n",
    "# yaml_path = '/content/academic_paper_maker/research_filter/agent/agent_ml.yaml'\n",
    "\n",
    "# Agent and config setup\n",
    "agentic_setting = {\n",
    "    \"agent_name\": agent_name,\n",
    "    \"column_name\": \"methodology_gap\",\n",
    "    \"yaml_path\": cfg['yaml_path'],\n",
    "    \"model_name\": model_name_method_extractor\n",
    "}\n",
    "\n",
    "# Output directories\n",
    "methodology_json_folder = os.path.join(main_folder, agent_name, 'json_output',model_name)\n",
    "multiple_runs_folder = os.path.join(main_folder, agent_name, 'multiple_runs_folder')\n",
    "final_cross_check_folder = os.path.join(main_folder, agent_name, 'final_cross_check_folder')\n",
    "\n",
    "# Excel input\n",
    "csv_path = path_dic['database_path']\n",
    "\n",
    "# Topic placeholders (adjust per project)\n",
    "placeholders = {\n",
    "    \"topic\": \"wafer defect classification\",\n",
    "    \"topic_context\": \"semiconductor manufacturing and inspection\"\n",
    "}\n",
    "\n",
    "# LLM runtime configuration\n",
    "process_setup = {\n",
    "    'batch_process': False,\n",
    "    'manual_paste_llm': False,\n",
    "    'iterative_confirmation': False,\n",
    "    'overwrite_csv': True,\n",
    "    'cross_check_enabled': False,\n",
    "    'cross_check_runs': 3,\n",
    "    'cross_check_agent_name': 'agent_cross_check',\n",
    "    'cleanup_json': False,\n",
    "    'used_abstract': True  # Always True to enable fallback to abstract if PDF is missing\n",
    "}\n",
    "\n",
    "# Run the pipeline\n",
    "run_pipeline(\n",
    "    agentic_setting,\n",
    "    process_setup,\n",
    "    placeholders=placeholders,\n",
    "    csv_path=csv_path,\n",
    "    main_folder=main_folder,\n",
    "    methodology_json_folder=methodology_json_folder,\n",
    "    multiple_runs_folder=multiple_runs_folder,\n",
    "    final_cross_check_folder=final_cross_check_folder,\n",
    ")"
   ],
   "id": "586f35e09784fa92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can simplify that import statement using an alias. Here's the **shortcut version**:\n",
    "\n",
    "```python\n",
    "from academic_paper_maker.helper.google_colab import run_llm_filtering_pipeline_colab as run_llm\n",
    "```\n",
    "\n",
    "Now you can call the function like this:\n",
    "\n",
    "```python\n",
    "run_llm(model_name=\"gpt-4o-mini\", cfg=cfg, placeholders=placeholders)\n",
    "```\n",
    "\n",
    "✅ This keeps your code cleaner while still referencing the full functionality.\n"
   ],
   "id": "ee2879a69067f4cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from academic_paper_maker.helper.google_colab import run_llm_filtering_pipeline_colab as run_llm\n",
    "\n",
    "placeholders = {\n",
    "    \"topic\": \"wafer defect classification\",\n",
    "    \"topic_context\": \"semiconductor manufacturing and inspection\"\n",
    "}\n",
    "\n",
    "run_llm(model_name=\"gpt-4o-mini\", cfg=cfg, placeholders=placeholders)\n"
   ],
   "id": "e8437991a3a3968c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 8: Draft Literature Review (Chapter 2) Using Combined JSON**\n",
    "\n",
    "Once you've extracted methodological insights in Step 9, the next logical move is to **start organizing and drafting your literature review** (typically Chapter 2 of a thesis or paper). This step outlines how to **combine extracted results** into a single, structured file — and how to use that file to produce high-quality summaries, tables, and narrative drafts using an LLM.\n",
    "\n",
    "> 📌 This is a branching step — not everyone will follow this exact path — but it's a powerful way to move from **structured data → written draft** efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Collects individual methodology JSON files.\n",
    "* Merges them into a single, unified JSON (`combined_output.json`).\n",
    "* Prepares this JSON as input for:\n",
    "\n",
    "  * Google AI Studio (e.g., Gemini Pro)\n",
    "  * GPT-based agents\n",
    "  * Jupyter notebooks or drafting pipelines\n",
    "* Enables:\n",
    "\n",
    "  * Narrative generation for Chapter 2\n",
    "  * Thematic clustering of methods\n",
    "  * Structured tables summarizing key findings\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Output Structure Example\n",
    "\n",
    "```\n",
    "corona_discharge/\n",
    "├── combined_output.json      ← ✅ Master file for summarization and drafting\n",
    "├── methodology_gap_extractor_partial_discharge/\n",
    "│   └── json_output/\n",
    "│       └── *.json            ← Individual extracted documents\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Using LLM to Generate Summary Tables\n",
    "\n",
    "With the `combined_output.json`, you can prompt an LLM to create structured tables summarizing key findings. For example:\n",
    "\n",
    "> “Using the combined JSON, generate a table with the following columns:\n",
    "> **Author**, **Machine Learning Algorithm**, **Feature Types**, **Dataset Used**, and **Performance Metrics**.”\n",
    "\n",
    "This gives you a **quick-glance overview** of the landscape, helpful both for understanding trends and citing clearly in your literature review.\n",
    "\n",
    "### ✅ Sample Columns for Summary Table:\n",
    "\n",
    "* **Author / Year**\n",
    "* **ML Algorithm(s) Used**\n",
    "* **Features**\n",
    "* **Dataset / Source**\n",
    "* **Performance (Accuracy, F1, etc.)**\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ Drafting Strategy Tips\n",
    "\n",
    "You can also use LLM prompts like:\n",
    "\n",
    "> “Based on the combined JSON, write a summary paragraph comparing the top 3 machine learning techniques used for partial discharge classification.”\n",
    "\n",
    "Or:\n",
    "\n",
    "> “Generate an introduction section discussing the evolution of feature engineering techniques in this domain.”\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Important Reminders\n",
    "\n",
    "* ✅ Make sure all JSON structures are consistent before combining.\n",
    "* 📉 Segment or cluster the JSON by subdomain if the file becomes too large.\n",
    "* 🔍 Review all generated tables and text — LLMs are **tools**, not final authorities.\n",
    "\n",
    "---"
   ],
   "id": "7ea8ae2257b2a4d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from setting.project_path import project_folder\n",
    "from helper.combine_json import combine_json_files\n",
    "from pathlib import Path\n",
    "# project_review = 'corona_discharge'\n",
    "# config_file='/content/academic_paper_maker/setting/project_folders.json'\n",
    "\n",
    "\n",
    "path_dict = project_folder(project_review=cfg['project_review'],config_file=cfg['config_file'])\n",
    "\n",
    "# The model_name_method_extractor is the model name used in the methodology gap extractor step, it is used to find the correct folder to combine the json files.\n",
    "\n",
    "\n",
    "input_dir = Path(cfg['methodology_gap_extractor_path']) / model_name_method_extractor # this path might be different based on the model being used\n",
    "\n",
    "# input_dir = cfg['methodology_gap_extractor_path'],'gpt-4o-mini'\n",
    "output_file = os.path.join(path_dict['main_folder'], 'combined_output.json')\n",
    "\n",
    "if not os.path.exists(input_dir):\n",
    "    raise FileNotFoundError(f\"Input directory not found: {input_dir}\")\n",
    "\n",
    "combine_json_files(\n",
    "    input_directory=input_dir,\n",
    "    output_file=output_file\n",
    ")\n",
    "\n",
    "print(f\"Combined JSON saved to: {output_file}\")\n"
   ],
   "id": "99f29be5d0a2412b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here's the **amended version** using your alias template for `combine_methodology_json_colab`:\n",
    "\n",
    "---\n",
    "\n",
    "You can simplify that import statement using an alias. Here's the **shortcut version**:\n",
    "\n",
    "```python\n",
    "from academic_paper_maker.helper.google_colab import combine_methodology_json_colab as combine_json\n",
    "```\n",
    "\n",
    "Now you can call the function like this:\n",
    "\n",
    "```python\n",
    "combine_json(cfg=cfg, model_name_method_extractor=\"gpt-4o-mini\")\n",
    "```\n",
    "\n",
    "✅ This keeps your code cleaner while still referencing the full functionality.\n"
   ],
   "id": "898b8d73292a5ced"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from academic_paper_maker.helper.google_colab import combine_methodology_json_colab as combine_json\n",
    "combine_json(cfg=cfg, model_name_method_extractor=\"gpt-4o-mini\")\n"
   ],
   "id": "4ac8c1c50a679818"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from download_pdf.download_pdf import run_pipeline, process_scihub_downloads, process_fallback_ieee\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "# Define your project\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "file_path = path_dic['database_path']\n",
    "output_folder = os.path.join(main_folder, 'pdf')\n",
    "\n",
    "# Run the main pipeline to load and categorize the data\n",
    "categories, data_filtered = run_pipeline(file_path)\n",
    "\n",
    "# First step, we will always use Sci-Hub to attempt PDF downloads\n",
    "process_scihub_downloads(categories, output_folder, data_filtered)\n",
    "\n",
    "# Fallback options for entries not available via Sci-Hub:\n",
    "# Uncomment the following lines one by one if you want to try downloading from specific sources\n",
    "\n",
    "# Uncomment to attempt fallback download from IEEE\n",
    "# process_fallback_ieee(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download using IEEE Search\n",
    "# process_fallback_ieee_search(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download from MDPI\n",
    "# process_fallback_mdpi(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download from ScienceDirect\n",
    "# Note: ScienceDirect URLs can be extracted but PDFs may not be downloadable due to security restrictions\n",
    "# process_fallback_sciencedirect(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to save the updated data to Excel after processing\n",
    "# save_data(data_filtered, file_path)"
   ],
   "id": "6a37458687d840b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from post_code_saviour.excel_to_bib import generate_bibtex\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=cfg['project_review'])\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Define input and output paths\n",
    "input_excel = os.path.join(main_folder, 'database', 'combined_filtered.xlsx')\n",
    "output_bib = os.path.join(main_folder, 'database', 'filtered_output.bib')\n",
    "\n",
    "# Load the filtered Excel file\n",
    "df = pd.read_excel(input_excel)\n",
    "\n",
    "# Generate BibTeX file\n",
    "generate_bibtex(df, output_file=output_bib)"
   ],
   "id": "c3026c463e76dec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 📄 BibTeX Generator: Use This Shortcut\n",
    "\n",
    "You can simplify the import with an alias. Here's the **shortcut version**:\n",
    "\n",
    "```python\n",
    "from academic_paper_maker.helper.google_colab import generate_bibtex_from_excel_colab as gen_bib\n",
    "```\n",
    "\n",
    "Now you can call the function like this:\n",
    "\n",
    "```python\n",
    "gen_bib(cfg=cfg)\n",
    "```\n",
    "\n",
    "✅ This will read the filtered Excel file and generate a `.bib` file for your bibliography—keeping everything clean and automatic."
   ],
   "id": "1091db873c2ccdd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from academic_paper_maker.helper.google_colab import generate_bibtex_from_excel_colab as gen_bib\n",
    "gen_bib(cfg=cfg)\n"
   ],
   "id": "7ec7a91033242791"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Eighth Step (Optional): Convert XML to JSON**\n",
    "\n",
    "This step converts GROBID-generated TEI XML files into structured JSON format. While optional, it can be helpful for reviewing document content, integrating into other tools, or preparing data to feed into an LLM.\n",
    "\n",
    "> 📝 **Note:** This step is **optional** — the main pipeline (`run_llm`) reads directly from XML. Use this conversion if you want to inspect or process JSON files instead.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Reads all `*.xml` files from the `xml/` directory.\n",
    "* Converts each into a corresponding `*.json` file (preserving the **BibTeX key as filename** for consistency).\n",
    "* Stores all JSON outputs in `xml/json/`.\n",
    "\n",
    "In addition, it handles and organizes special cases:\n",
    "\n",
    "* 📁 **`xml/json/no_intro_conclusion/`**: XML files where GROBID could not detect an *introduction* or *conclusion* section.\n",
    "* 📁 **`xml/json/untitled_section/`**: XML files where GROBID could not detect any section titles at all — these require manual checking.\n",
    "* 📄 Other successfully processed files are stored directly in `xml/json/`.\n",
    "\n",
    "---\n",
    "\n",
    "## ▶️ Example Code\n",
    "\n",
    "```python\n",
    "from setting.project_path import project_folder\n",
    "from grobid_tei_xml.xml_json import run_pipeline\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "\n",
    "# Convert all XML files in the specified folder to JSON\n",
    "run_pipeline(path_dic['xml_path'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Example Folder Structure After Conversion\n",
    "\n",
    "```\n",
    "corona_discharge/\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf\n",
    "│   └── kim_2019.pdf\n",
    "├── xml/\n",
    "│   ├── smith_2020.xml\n",
    "│   ├── kim_2019.xml\n",
    "│   └── json/\n",
    "│       ├── smith_2020.json\n",
    "│       ├── kim_2019.json\n",
    "│       ├── no_intro_conclusion/\n",
    "│       │   └── failed_paper1.json\n",
    "│       └── untitled_section/\n",
    "│           └── failed_paper2.json\n",
    "```"
   ],
   "id": "24dba46998c443c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from setting.project_path import project_folder\n",
    "from grobid_tei_xml.xml_json import run_pipeline\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "run_pipeline(path_dic['xml_path'])"
   ],
   "id": "17f7f677505cff0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Ninth Step: Extract Methodology Details Using LLM**\n",
    "\n",
    "At this stage, your reference list is filtered and the corresponding PDFs (or abstracts) are available. Now, the focus shifts to **extracting key methodological insights** from each paper, such as:\n",
    "\n",
    "* 🧠 Classification algorithms\n",
    "* 🛠️ Feature engineering approaches\n",
    "* 📏 Evaluation metrics\n",
    "\n",
    "This is achieved using a specialized **LLM agent** with a targeted prompt for methodology extraction.\n",
    "\n",
    "> 📌 This step works with **full manuscripts (PDF)** when available, and **falls back to abstracts** if no PDF exists. This flexibility ensures comprehensive analysis even with incomplete data.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Loads your filtered Excel or CSV file from Step 5 or 6.\n",
    "* For each relevant paper:\n",
    "\n",
    "  * If the PDF is available: extracts methodology from the full text.\n",
    "  * If the PDF is **not** available: extracts from the **abstract** instead.\n",
    "* Uses a domain-specific LLM prompt to analyze methodological content.\n",
    "* Appends the results to the existing Excel or CSV file.\n",
    "* Saves per-paper structured JSON files for advanced or customized usage.\n",
    "* Handles backups automatically if overwriting the metadata file.\n",
    "\n",
    "> ⛑️ **Safety Note**: If `'overwrite_csv': True`, a **timestamped backup** of the original `.csv` or `.xlsx` file is automatically created **in the same folder** before any updates are made. This prevents accidental corruption and allows for recovery or version tracking.\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Prompt Purpose\n",
    "\n",
    "This step uses a **domain-aware analytical agent** designed to:\n",
    "\n",
    "> “Extract methodological details (e.g., classification algorithms, feature engineering, evaluation metrics) from filtered papers relevant to a specific machine learning task.”\n",
    "\n",
    "The prompt is defined in a YAML config file (`agent_ml.yaml`) and is tailored by the agent name you provide.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Folder and Output Structure\n",
    "\n",
    "Your project directory may look like this after completing this step:\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── combined_filtered.xlsx                  ← Updated metadata with extraction results\n",
    "│       ├── combined_filtered_backup_20250508_1530.xlsx  ← Auto-generated backup (if overwrite_csv=True)\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf                              ← Saved using bibtex_key\n",
    "│   ├── kim_2019.pdf\n",
    "│   └── ...\n",
    "├── methodology_gap_extractor/\n",
    "│   ├── json_output/\n",
    "│   │   └── smith_2020.json\n",
    "│   ├── multiple_runs_folder/\n",
    "│   └── final_cross_check_folder/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Supported Models\n",
    "\n",
    "Choose from the following supported LLMs:\n",
    "\n",
    "* `\"gpt-4o\"`\n",
    "* `\"gpt-4o-mini\"`\n",
    "* `\"gpt-o3-mini\"`\n",
    "* `'gemini-1.5-pro'`\n",
    "* `'gemini-exp-1206'`\n",
    "* `'gemini-2.0-flash-thinking-exp-01-21'`\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Important Reminders\n",
    "\n",
    "* ✅ Set `used_abstract = True` if some papers lack full PDFs.\n",
    "* 🛑 **Always verify** extracted methodologies manually before using them in analysis, models, or publication. LLMs can hallucinate or misinterpret technical details.\n",
    "\n",
    "---"
   ],
   "id": "9f09bf489150aabb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from research_filter.auto_llm import run_pipeline\n",
    "from setting.project_path import project_folder\n",
    "import os\n",
    "\n",
    "# Define the project\n",
    "project_review = 'corona_discharge'\n",
    "# Select your LLM model. For this step, it is recommended to use a more powerful model like `gpt-4o` or `gemini-2.0-flash-thinking-exp-01-21` for better performance. Use model that have reasoning capability for better performance.\n",
    "\n",
    "\n",
    "model_name = 'gpt-4o-mini'\n",
    "agent_name = \"methodology_gap_extractor\"\n",
    "\n",
    "config_file='/content/academic_paper_maker/setting/project_folders.json'\n",
    "\n",
    "\n",
    "path_dic = project_folder(project_review=project_review,config_file=config_file)\n",
    "\n",
    "\n",
    "# path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "# Construct the relative path\n",
    "yaml_path = '/content/academic_paper_maker/research_filter/agent/agent_ml.yaml'\n",
    "\n",
    "# Agent and config setup\n",
    "agentic_setting = {\n",
    "    \"agent_name\": agent_name,\n",
    "    \"column_name\": \"methodology_gap\",\n",
    "    \"yaml_path\": yaml_path,\n",
    "    \"model_name\": model_name\n",
    "}\n",
    "\n",
    "# Output directories\n",
    "methodology_json_folder = os.path.join(main_folder, agent_name, 'json_output',model_name)\n",
    "multiple_runs_folder = os.path.join(main_folder, agent_name, 'multiple_runs_folder')\n",
    "final_cross_check_folder = os.path.join(main_folder, agent_name, 'final_cross_check_folder')\n",
    "\n",
    "# Excel input\n",
    "csv_path = path_dic['database_path']\n",
    "\n",
    "# Topic placeholders (adjust per project)\n",
    "placeholders = {\n",
    "    \"topic\": \"wafer defect classification\",\n",
    "    \"topic_context\": \"semiconductor manufacturing and inspection\"\n",
    "}\n",
    "\n",
    "# LLM runtime configuration\n",
    "process_setup = {\n",
    "    'batch_process': False,\n",
    "    'manual_paste_llm': False,\n",
    "    'iterative_confirmation': False,\n",
    "    'overwrite_csv': True,\n",
    "    'cross_check_enabled': False,\n",
    "    'cross_check_runs': 3,\n",
    "    'cross_check_agent_name': 'agent_cross_check',\n",
    "    'cleanup_json': False,\n",
    "    'used_abstract': True  # Always True to enable fallback to abstract if PDF is missing\n",
    "}\n",
    "\n",
    "# Run the pipeline\n",
    "run_pipeline(\n",
    "    agentic_setting,\n",
    "    process_setup,\n",
    "    placeholders=placeholders,\n",
    "    csv_path=csv_path,\n",
    "    main_folder=main_folder,\n",
    "    methodology_json_folder=methodology_json_folder,\n",
    "    multiple_runs_folder=multiple_runs_folder,\n",
    "    final_cross_check_folder=final_cross_check_folder,\n",
    ")"
   ],
   "id": "d4cc866c39eb8c2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Tenth Step: Draft Literature Review (Chapter 2) Using Combined JSON**\n",
    "\n",
    "Once you've extracted methodological insights in Step 9, the next logical move is to **start organizing and drafting your literature review** (typically Chapter 2 of a thesis or paper). This step outlines how to **combine extracted results** into a single, structured file — and how to use that file to produce high-quality summaries, tables, and narrative drafts using an LLM.\n",
    "\n",
    "> 📌 This is a branching step — not everyone will follow this exact path — but it's a powerful way to move from **structured data → written draft** efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Collects individual methodology JSON files.\n",
    "* Merges them into a single, unified JSON (`combined_output.json`).\n",
    "* Prepares this JSON as input for:\n",
    "\n",
    "  * Google AI Studio (e.g., Gemini Pro)\n",
    "  * GPT-based agents\n",
    "  * Jupyter notebooks or drafting pipelines\n",
    "* Enables:\n",
    "\n",
    "  * Narrative generation for Chapter 2\n",
    "  * Thematic clustering of methods\n",
    "  * Structured tables summarizing key findings\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Code Example: Combine JSONs\n",
    "\n",
    "```python\n",
    "import os\n",
    "from setting.project_path import project_folder\n",
    "from helper.combine_json import combine_json_files\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "path_dict = project_folder(project_review=project_review)\n",
    "\n",
    "input_dir = os.path.join(\n",
    "    path_dict['main_folder'],\n",
    "    r\"methodology_gap_extractor_partial_discharge\\json_output\\gemini-2.0-flash-thinking-exp-01-21_updated\"\n",
    ")\n",
    "output_file = os.path.join(path_dict['main_folder'], 'combined_output.json')\n",
    "\n",
    "if not os.path.exists(input_dir):\n",
    "    raise FileNotFoundError(f\"Input directory not found: {input_dir}\")\n",
    "\n",
    "combine_json_files(\n",
    "    input_directory=input_dir,\n",
    "    output_file=output_file\n",
    ")\n",
    "\n",
    "print(f\"Combined JSON saved to: {output_file}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Output Structure Example\n",
    "\n",
    "```\n",
    "corona_discharge/\n",
    "├── combined_output.json      ← ✅ Master file for summarization and drafting\n",
    "├── methodology_gap_extractor_partial_discharge/\n",
    "│   └── json_output/\n",
    "│       └── *.json            ← Individual extracted documents\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Using LLM to Generate Summary Tables\n",
    "\n",
    "With the `combined_output.json`, you can prompt an LLM to create structured tables summarizing key findings. For example:\n",
    "\n",
    "> “Using the combined JSON, generate a table with the following columns:\n",
    "> **Author**, **Machine Learning Algorithm**, **Feature Types**, **Dataset Used**, and **Performance Metrics**.”\n",
    "\n",
    "This gives you a **quick-glance overview** of the landscape, helpful both for understanding trends and citing clearly in your literature review.\n",
    "\n",
    "### ✅ Sample Columns for Summary Table:\n",
    "\n",
    "* **Author / Year**\n",
    "* **ML Algorithm(s) Used**\n",
    "* **Features**\n",
    "* **Dataset / Source**\n",
    "* **Performance (Accuracy, F1, etc.)**\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ Drafting Strategy Tips\n",
    "\n",
    "You can also use LLM prompts like:\n",
    "\n",
    "> “Based on the combined JSON, write a summary paragraph comparing the top 3 machine learning techniques used for partial discharge classification.”\n",
    "\n",
    "Or:\n",
    "\n",
    "> “Generate an introduction section discussing the evolution of feature engineering techniques in this domain.”\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Important Reminders\n",
    "\n",
    "* ✅ Make sure all JSON structures are consistent before combining.\n",
    "* 📉 Segment or cluster the JSON by subdomain if the file becomes too large.\n",
    "* 🔍 Review all generated tables and text — LLMs are **tools**, not final authorities.\n",
    "\n",
    "---"
   ],
   "id": "411c0567bd535174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from setting.project_path import project_folder\n",
    "from helper.combine_json import combine_json_files\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "config_file='/content/academic_paper_maker/setting/project_folders.json'\n",
    "\n",
    "\n",
    "path_dict = project_folder(project_review=project_review,config_file=config_file)\n",
    "\n",
    "input_dir = '/content/my_project/methodology_gap_extractor/json_output/gpt-4o-mini'\n",
    "output_file = os.path.join(path_dict['main_folder'], 'combined_output.json')\n",
    "\n",
    "if not os.path.exists(input_dir):\n",
    "    raise FileNotFoundError(f\"Input directory not found: {input_dir}\")\n",
    "\n",
    "combine_json_files(\n",
    "    input_directory=input_dir,\n",
    "    output_file=output_file\n",
    ")\n",
    "\n",
    "print(f\"Combined JSON saved to: {output_file}\")\n"
   ],
   "id": "527d5be14321c3c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Eleventh Step: Export Filtered Excel to BibTeX**\n",
    "\n",
    "After reviewing and filtering your `combined_filtered.xlsx` file, you can convert the refined list of papers back into a **BibTeX file**. This can be helpful for citation management or integration with tools like LaTeX or reference managers.\n",
    "\n",
    "---\n",
    "\n",
    "# ✨ What This Step Does\n",
    "\n",
    "* Loads the filtered Excel file containing your selected papers\n",
    "* Converts the data back into BibTeX format\n",
    "* Saves the result to a `.bib` file for easy reuse or citation\n",
    "\n",
    "\n",
    "# 📁 File Structure Example\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── combined_filtered.xlsx                  ← Filtered Excel file with selected papers\n",
    "│       ├── combined_filtered_backup_20250508_1530.xlsx  ← Auto backup if overwrite is enabled\n",
    "│       └── filtered_output.bib                     ← Newly generated BibTeX file\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf                              ← Full-text PDFs saved by bibtex_key\n",
    "│   ├── kim_2019.pdf\n",
    "│   └── ...\n",
    "├── methodology_gap_extractor_wafer_defect/\n",
    "│   ├── json_output/\n",
    "│   │   └── smith_2020.json                         ← Extracted methodology as structured JSON\n",
    "│   ├── multiple_runs_folder/\n",
    "│   └── final_cross_check_folder/\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Notes\n",
    "\n",
    "* Ensure the Excel file has standard bibliographic columns like: `title`, `author`, `year`, `journal`, `doi`, etc.\n",
    "* The function `generate_bibtex()` maps these fields into valid BibTeX entries.\n",
    "* You can open the `.bib` file in any text editor or reference manager to confirm the results."
   ],
   "id": "3c8648d56cd6763f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from post_code_saviour.excel_to_bib import generate_bibtex\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "# Define your project\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Define input and output paths\n",
    "input_excel = os.path.join(main_folder, 'database', 'combined_filtered.xlsx')\n",
    "output_bib = os.path.join(main_folder, 'database', 'filtered_output.bib')\n",
    "\n",
    "# Load the filtered Excel file\n",
    "df = pd.read_excel(input_excel)\n",
    "\n",
    "# Generate BibTeX file\n",
    "generate_bibtex(df, output_file=output_bib)"
   ],
   "id": "35d9935f47aebf25",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
