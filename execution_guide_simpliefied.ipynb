{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notebook Summary: Academic Paper Maker Workflow\n",
    "\n",
    "This notebook guides you through a step-by-step process for setting up your environment, collecting academic papers, filtering them using Large Language Models (LLMs), extracting key information, and preparing files for literature review drafting and citation.\n",
    "\n",
    "---\n",
    "\n",
    "## How to Run This Notebook\n",
    "\n",
    "To run this notebook, follow these instructions:\n",
    "\n",
    "1.  **Read each step** below first to understand the process.\n",
    "2.  **Run the code cells** one by one, in the order they appear.\n",
    "3.  Click on a code cell and press `Shift + Enter`, or click the play button (`▶`) on the left side of the cell.\n",
    "4.  A `[*]` next to a cell indicates it's currently running. It will change to a number (e.g., `[1]`, `[2]`) when execution is complete.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Steps\n",
    "\n",
    "This notebook consists of **12 distinct steps** presented sequentially through Markdown headers, guiding you through the automated academic paper review process.\n",
    "\n",
    "Here is a summary of each step and its purpose:\n",
    "\n",
    "1.  **Step 1: Install Required Packages**\n",
    "    *   **Purpose:** To set up the necessary Python environment by cloning the `academic_paper_maker` repository and installing all required dependencies from `requirements.txt`.\n",
    "\n",
    "2.  **Step 2: Utility Functions for Colab Execution**\n",
    "    *   **Purpose:** To define helper functions specifically tailored for running the various steps within a Google Colab environment, simplifying common operations like API testing, filtering, JSON merging, and BibTeX generation. (Code cell implements these functions).\n",
    "\n",
    "3.  **Step 3: Obtain Your API Keys**\n",
    "    *   **Purpose:** To instruct the user on how to acquire necessary API keys (e.g., OpenAI, Gemini) from their respective platforms, which are required for using LLM-based features later in the workflow. Keys are saved externally at this stage.\n",
    "\n",
    "4.  **Step 4: Create and Register Your Project Folder**\n",
    "    *   **Purpose:** To define and initialize the main directory for your project, setting up a standard folder structure (`database/scopus`, `pdf`, `xml`) and registering the project's location for easy access in future sessions.\n",
    "\n",
    "5.  **Step 5: Create a `.env` File to Store API Keys**\n",
    "    *   **Purpose:** To create a `.env` file in the project directory. This file serves as a secure location to store your obtained API keys (manually added by the user after creation), preventing credentials from being hardcoded.\n",
    "\n",
    "6.  **Step 5a: Load API Key and Test OpenAI Connection**\n",
    "    *   **Purpose:** To verify that the OpenAI API key stored in the `.env` file is correctly loaded and can successfully connect and communicate with the OpenAI API by sending a simple test query.\n",
    "\n",
    "7.  **Step 6: Download the Scopus BibTeX File**\n",
    "    *   **Purpose:** To guide the user on how to use the Scopus database (or a similar source) to perform advanced searches for relevant academic papers and download the results in BibTeX format (`.bib`) into the designated `database/scopus` folder.\n",
    "\n",
    "8.  **Step 7: Combine Scopus BibTeX Files into Excel**\n",
    "    *   **Purpose:** To process the downloaded BibTeX (`.bib`) files from the `database/scopus` folder, extract key metadata from all files, and merge them into a single, structured Excel file (`combined_filtered.xlsx`) for subsequent filtering and review.\n",
    "\n",
    "9.  **Step 8: Automatically Filter Relevant References Using LLM**\n",
    "    *   **Purpose:** To employ a Large Language Model (LLM) to automatically read the abstracts from the `combined_filtered.xlsx` file and classify their relevance based on a user-defined `topic` and `topic_context`, adding a `True`/`False` filter column to the Excel.\n",
    "\n",
    "10. **Step 7: Extract Methodology Details Using LLM** (Note: Header numbers are inconsistent, but this is a distinct step logically)\n",
    "    *   **Purpose:** To use an LLM agent to extract detailed methodological information (e.g., algorithms, datasets, metrics) from the filtered papers, utilizing either the full PDF text (if available) or the abstract as a fallback, and saving the results in structured JSON files and updating the Excel.\n",
    "\n",
    "11. **Step 8: Draft Literature Review (Chapter 2) Using Combined JSON** (Note: Header numbers are inconsistent, but this is a distinct step logically)\n",
    "    *   **Purpose:** To combine the individual JSON files containing extracted methodology details into a single `combined_output.json`. This file then serves as structured input for LLMs or scripts to assist in drafting literature review sections, generating summary tables, or performing thematic analysis.\n",
    "\n",
    "12. **Step 9: Export Filtered Excel to BibTeX**\n",
    "    *   **Purpose:** To convert the final, manually reviewed and filtered Excel file (`combined_filtered.xlsx`) back into a BibTeX (`.bib`) file, facilitating easy import into citation managers or integration with LaTeX documents for generating bibliographies.\n",
    "\n",
    "---\n",
    "\n",
    "This sequence of steps automates significant portions of the literature review process, from collecting initial references to extracting detailed insights and preparing final outputs for writing."
   ],
   "id": "f56132de33410437"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📦 **Step 1: Install Required Packages + Configure API Keys**\n",
    "\n",
    "Before you start using this notebook, ensure all required Python packages are installed **and** that your API keys are properly set up in a `.env` file. This prepares your environment for accessing services like OpenAI or Google Gemini.\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 How to Install Required Packages\n",
    "\n",
    "Open your terminal or command prompt, navigate to the project directory (where `requirements.txt` is located), and run:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "> ✅ **Tip:** Use a virtual environment (e.g., `venv`, `conda`) to keep your dependencies clean and isolated.\n",
    "\n",
    "If you're running this from a Jupyter notebook:\n",
    "\n",
    "```python\n",
    "!pip install -r requirements.txt\n",
    "```"
   ],
   "id": "ff6e1aa0d8728b23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🧠 **LLM Prompt Customization**\n",
    "\n",
    "For the automated relevance filtering to work effectively, you **must provide two key inputs** to guide the LLM:\n",
    "\n",
    "* `topic`: A concise statement of your **specific research goal or area of interest**.\n",
    "  *Example:* `\"wafer defect classification\"`\n",
    "\n",
    "* `topic_context`: A description of the **broader scientific or industrial context** where your topic belongs.\n",
    "  *Example:* `\"semiconductor manufacturing and inspection\"`\n",
    "\n",
    "These inputs help the LLM understand what kinds of abstracts are considered relevant and which ones should be filtered out.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 *Not sure how to define your `topic` and `topic_context`?*\n",
    "\n",
    "You can get help from an LLM to generate these values. Here’s how:\n",
    "\n",
    "1. **Open the filtering prompt** template defined in the YAML file:\n",
    "\n",
    "   ```\n",
    "   research_filter/agent/abstract_check.yaml\n",
    "   ```\n",
    "\n",
    "2. **Copy the prompt structure** (including the placeholders for `topic` and `topic_context`).\n",
    "\n",
    "3. **Ask any LLM to assist**, by providing it the YAML prompt and a description of your research.\n",
    "   For example, you could say:\n",
    "\n",
    "   > 🧠 *\"Given the following prompt structure, and knowing that my research is about identifying AI-generated academic papers, can you help me fill in the `topic` and `topic_context` placeholders?\"*\n",
    "\n",
    "4. The LLM might then suggest:\n",
    "\n",
    "   ```yaml\n",
    "   topic: \"AI-generated academic paper detection\"\n",
    "   topic_context: \"scientific publishing and machine learning ethics\"\n",
    "   ```\n",
    "\n",
    "This approach ensures your filtering prompt is both precise and contextually grounded, improving the accuracy of the classification.\n"
   ],
   "id": "27602cd3f4b83333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T00:51:42.536981Z",
     "start_time": "2025-06-10T00:51:42.531994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Click me to execute\n",
    "# You can define your topic and context here, or use the LLM to help you fill in these placeholders.\n",
    "\n",
    "placeholders = {\n",
    "    \"topic\": \"wafer defect classification\",\n",
    "    \"topic_context\": \"semiconductor manufacturing and inspection\"\n",
    "}\n",
    "\n",
    "project_theme=\"wafer\""
   ],
   "id": "42ea437e2d41aebc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🔧 Step 2: Create and Register Your Project Folder\n",
    "\n",
    "In this step, you'll register the **main project folder**, so the system knows where to store and retrieve all files related to your project.\n",
    "\n",
    "This setup automatically creates a structured folder system under your specified `main_folder`, and stores the configuration in a central JSON file. This ensures your projects remain organized, especially when working with multiple studies or datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Project Folder Structure\n",
    "\n",
    "On the first run, the following folder structure will be automatically created under your specified `main_folder`:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│   └── <project_name>_database.xlsx  ← auto-generated path (not file creation)\n",
    "├── pdf/\n",
    "└── xml/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Example\n",
    "\n",
    "Suppose your project name is `corona_discharge`, and you want to store all project files under:\n",
    "\n",
    "```\n",
    "G:\\My Drive\\research_related\\ear_eog\n",
    "```\n",
    "\n",
    "You can register this setup by running:\n",
    "\n",
    "```python\n",
    "project_folder(\n",
    "    project_review='corona_discharge',\n",
    "    main_folder=r'G:\\My Drive\\research_related\\ear_eog'\n",
    ")\n",
    "```\n",
    "\n",
    "✅ This will:\n",
    "\n",
    "* Save the project path in `setting/project_folders.json`\n",
    "* Create the full folder structure: `database/scopus`, `pdf`, and `xml`\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Loading the Project Later\n",
    "\n",
    "Once registered, you can access the project folders in future sessions by providing just the `project_review` name:\n",
    "\n",
    "```python\n",
    "paths = project_folder(project_review='corona_discharge')\n",
    "\n",
    "print(paths['main_folder'])  # Main project folder\n",
    "print(paths['csv_path'])     # Path to the Excel database file\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ What Happens in the Background?\n",
    "\n",
    "* A file named `project_folders.json` is stored in the `setting/` directory within your project.\n",
    "* It maps each `project_review` to its corresponding `main_folder`.\n",
    "* Folder structure is created automatically on the first run.\n",
    "* On subsequent runs, the system reads the JSON to locate your project — no need to re-enter paths.\n"
   ],
   "id": "ae2406adcdb5ed5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T00:52:50.845846Z",
     "start_time": "2025-06-10T00:52:50.842337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === User-Defined Base Directory ===\n",
    "# Recommended: Set the base directory where all project files will live\n",
    "base_dir = r\"D:\\test_me\"  # Change this to your preferred location\n"
   ],
   "id": "423ead1032deba06",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ⚙️ Step 2 **Utility Functions for Colab Execution**\n",
    "\n",
    "To streamline your workflow, we've wrapped common operations into **modular utility functions**. These abstract away repetitive code and let you focus on your analysis—not setup."
   ],
   "id": "d22d4f51a2fb286d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T00:51:46.733992Z",
     "start_time": "2025-06-10T00:51:45.784055Z"
    }
   },
   "cell_type": "code",
   "source": "from notebook.helper import generate_bibtex_from_excel_colab,generate_bibtex, create_env_file, test_openai_connection, run_llm_filtering_pipeline_colab, run_abstract_filtering_colab, run_methodology_extraction_colab, combine_methodology_output_colab",
   "id": "fd917524744f0f84",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T00:51:01.021149Z",
     "start_time": "2025-06-10T00:51:01.012153Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "# 🔑 **Step 3: Obtain Your API Keys**\n",
    "\n",
    "Before running any LLM-based tools, you'll need to obtain API keys for **OpenAI** and optionally **Google Gemini**, if you plan to use both.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Why You Need This\n",
    "\n",
    "API keys are required to authenticate your access to models like `gpt-4o` or `gemini-pro`. Without them, the system won’t be able to connect to the services.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔐 Where to Get Your Keys\n",
    "\n",
    "| Provider   | Key Name         | Get It From                                                          |\n",
    "| ---------- | ---------------- | -------------------------------------------------------------------- |\n",
    "| **OpenAI** | `OPENAI_API_KEY` | [platform.openai.com/api-keys](https://platform.openai.com/api-keys) |\n",
    "| **Gemini** | `GEMINI_API_KEY` | [aistudio.google.com](https://aistudio.google.com/apikey)            |\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 What To Do With Them Now\n",
    "\n",
    "Just **copy and save your API keys in a safe place** (e.g., a password manager or text file on your machine).\n",
    "\n",
    "> 📝 You do **not** need to put them in a `.env` file yet.\n",
    "\n",
    "In the **next step**, you’ll insert them into a configuration file automatically using a helper function.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Example (Save for Later)\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "GEMINI_API_KEY=AIzaSy...\n",
    "```\n",
    "\n",
    "> ⚠️ Do not share these keys with anyone or post them online.\n"
   ],
   "id": "d7f532f8125d4e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ⚙️ **Step 4: Create and Register Your Project Folder**\n",
    "\n",
    "Just execute the code below to set up your project folder. This will create a structured directory for storing all your project files, including databases, PDFs, and XML files."
   ],
   "id": "8564a02f0ad37883"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T00:54:07.384103Z",
     "start_time": "2025-06-10T00:54:07.375190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from setting.project_path import project_folder\n",
    "from notebook.helper import build_project_config\n",
    "\n",
    "\n",
    "# === Optional: Define Custom YAML Configuration Paths ===\n",
    "# These paths point to YAML files used for abstract and methodology filtering\n",
    "notebook_dir = os.getcwd()\n",
    "yaml_dir = join(notebook_dir, \"research_filter\", \"agent\")\n",
    "custom_abstract = join(yaml_dir, \"abstract_check.yaml\")\n",
    "custom_methodology = join(yaml_dir, \"agent_ml.yaml\")\n",
    "\n",
    "# === Build Project Configuration ===\n",
    "cfg = build_project_config(\n",
    "    project_name=\"wafer\",\n",
    "    base_dir=base_dir,\n",
    "    yaml_path_abstract=custom_abstract,\n",
    "    yaml_path_methodology=custom_methodology\n",
    ")\n",
    "\n",
    "# === Initialize Folder Structure ===\n",
    "# Registers and prepares the folder structure for this project\n",
    "project_folder(\n",
    "    project_review=cfg[\"project_review\"],\n",
    "    main_folder=cfg[\"main_folder\"],\n",
    "    config_file=cfg[\"config_file\"]\n",
    ")\n",
    "\n",
    "print(\"✅ Configuration loaded and project folder initialized successfully.\")\n"
   ],
   "id": "834635c800492e25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded and project folder initialized successfully.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T00:54:13.216072Z",
     "start_time": "2025-06-10T00:54:13.212262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Minimal (user will edit manually later)\n",
    "create_env_file(cfg[\"env_path\"])\n",
    "\n",
    "# Or with keys already known\n",
    "create_env_file(cfg[\"env_path\"],\n",
    "                openai_api_key=\"go_and_replace with_your_openai_key\",\n",
    "                # gemini_api_key=\"AIzaSy...\"\n",
    "                )"
   ],
   "id": "95782a760f85b5b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ `.env` file created at: D:\\test_me\\project_files\\.env\n",
      "👉 In Colab, click the folder icon on the left, go to the appropriate folder, right-click `.env`, and choose 'Edit' to enter your keys.\n",
      "✅ `.env` file created at: D:\\test_me\\project_files\\.env\n",
      "👉 In Colab, click the folder icon on the left, go to the appropriate folder, right-click `.env`, and choose 'Edit' to enter your keys.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# ⚙️ **Step 5a: Load API Key and Test OpenAI Connection**\n",
    "\n",
    "After setting up your `.env` file, it's important to **verify that your OpenAI API key is working correctly**. This step uses a built-in helper function to test the connection.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 What This Step Does\n",
    "\n",
    "* Loads your API key from the `.env` file\n",
    "* Initializes an OpenAI client with that key\n",
    "* Sends a simple test message to the GPT model (e.g., `gpt-4o`)\n",
    "* Confirms success or provides a clear error message if something goes wrong\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ How to Run It\n",
    "\n",
    "Just call the helper function:\n",
    "\n",
    "```python\n",
    "from academic_paper_maker.helper.google_colab import test_openai_connection\n",
    "\n",
    "test_openai_connection()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Expected Outcome\n",
    "\n",
    "If everything is set up correctly, you'll see output like:\n",
    "\n",
    "```\n",
    "✅ API call successful!\n",
    "🤖 Response: 2 + 2 is 4.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ If Something Goes Wrong\n",
    "\n",
    "Common errors and their meanings:\n",
    "\n",
    "| Error Type             | What It Means                                 |\n",
    "| ---------------------- | --------------------------------------------- |\n",
    "| ❌ AuthenticationError  | API key is missing or incorrect               |\n",
    "| ⚠️ RateLimitError      | You’re sending too many requests too quickly  |\n",
    "| 📡 APIConnectionError  | Network issue or OpenAI server is unreachable |\n",
    "| 🚫 InvalidRequestError | Incorrect model name or bad request structure |\n",
    "\n",
    "Make sure your `.env` file includes a valid key in this format:\n",
    "\n",
    "```env\n",
    "OPENAI_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "> 🔁 Re-run the test after fixing the `.env` or internet connection if needed.\n",
    "\n",
    "---\n"
   ],
   "id": "7d4cd5260d0ccddb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T00:54:42.932329Z",
     "start_time": "2025-06-10T00:54:40.682169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"🔍 Testing OpenAI connection... Please wait.\\n\")\n",
    "\n",
    "# Run the test\n",
    "test_openai_connection()\n",
    "\n",
    "print(\"\\n✅ If the connection is successful, you should see a response from the assistant above.and the answer should be something like:  Response: Hello! The sum of 2 + 2 is 4.\")\n",
    "print(\"⚠️ If not, please check your `.env` file and verify that your `OPENAI_API_KEY` is correct.\")"
   ],
   "id": "2416afbf2740e9aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing OpenAI connection... Please wait.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 08:54:42,912 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API call successful!\n",
      "🤖 Response: Hello! The sum of 2 + 2 is 4.\n",
      "\n",
      "✅ If the connection is successful, you should see a response from the assistant above.and the answer should be something like:  Response: Hello! The sum of 2 + 2 is 4.\n",
      "⚠️ If not, please check your `.env` file and verify that your `OPENAI_API_KEY` is correct.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📥 Step 6: Download the Scopus BibTeX File\n",
    "\n",
    "In this step, you'll use the **Scopus database** to find and download relevant papers for your project. Scopus is a comprehensive and widely-used repository of peer-reviewed academic literature.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Using Scopus Advanced Search\n",
    "\n",
    "To retrieve high-quality and relevant papers, we recommend using **Scopus' Advanced Search** feature. This powerful tool lets you refine your search based on:\n",
    "\n",
    "* Keywords\n",
    "* Authors\n",
    "* Publication dates\n",
    "* Document types\n",
    "* And more...\n",
    "\n",
    "This ensures that your literature collection is both targeted and comprehensive.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Get Keyword Ideas with a Prompt\n",
    "\n",
    "To help you formulate effective search queries, you can use the following **prompt-based suggestion tool**:\n",
    "\n",
    "👉 [Keyword Search Prompt](https://gist.github.com/balandongiv/886437963d38252e61634ddc00b9d983)\n",
    "\n",
    "You may need to modify the prompt to better suit your research domain. Here are some example domains:\n",
    "\n",
    "* `\"corona discharge\"`\n",
    "* `\"fatigue driving EEG\"`\n",
    "* `\"wafer classification\"`\n",
    "\n",
    "Feel free to add, remove, or tweak keywords as needed to refine your search results.\n",
    "\n",
    "---\n",
    "\n",
    "## 💾 Save and Organize Your Results\n",
    "\n",
    "Once you've finalized your search:\n",
    "\n",
    "1. **Select all available attributes** when exporting results from Scopus.\n",
    "2. # Access Scopus\n",
    "![Scopus CSV Export](image/scopus_csv_export.png)\n",
    "\n",
    "\n",
    "2. Choose the **BibTeX** format when saving the export file.\n",
    "3. Save the file inside the `database/scopus/` folder of your project.\n",
    "\n",
    "The resulting folder structure might look like this:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── scopus(1).bib\n",
    "│       ├── scopus(2).bib\n",
    "│       ├── scopus(3).bib\n",
    "```\n",
    "\n",
    "Make sure the BibTeX files are correctly named and stored to ensure smooth integration in later steps."
   ],
   "id": "72c1fb6183a242ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📊 Step 7: Combine Scopus BibTeX Files into Excel\n",
    "\n",
    "Once you've downloaded multiple `.bib` files from Scopus, the next step is to **combine and convert** them into a structured Excel file. This makes it easier to filter, sort, and review the metadata of all collected papers.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 What This Step Does\n",
    "\n",
    "* Loads all `.bib` files from your project's `database/scopus/` folder\n",
    "* Parses the relevant metadata (e.g., title, authors, year, source, DOI)\n",
    "* Combines the results into a single Excel spreadsheet\n",
    "* Saves the spreadsheet in the `database/` folder as `combined_filtered.xlsx`\n",
    "\n",
    "\n",
    "\n",
    "## 📁 Folder Structure Example\n",
    "\n",
    "After running the script, your folder might look like this:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   ├── scopus/\n",
    "│   │   ├── scopus(1).bib\n",
    "│   │   ├── scopus(2).bib\n",
    "│   │   ├── scopus(3).bib\n",
    "│   └── combined_filtered.xlsx\n",
    "```\n",
    "\n",
    "This Excel file will serve as your primary reference for filtering papers before downloading PDFs.\n"
   ],
   "id": "b5539ae56b1485ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T01:10:03.230444Z",
     "start_time": "2025-06-10T01:08:54.463625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from download_pdf.database_preparation import combine_scopus_bib_to_excel\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "path_dic=project_folder(project_review=cfg['project_review'],config_file=cfg[\"config_file\"])\n",
    "combine_scopus_bib_to_excel(path_dic['scopus_path'], path_dic['database_path'])"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 .bib files in D:\\test_me\\project_files\\database\\scopus\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msetting\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mproject_path\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m project_folder\n\u001B[32m      4\u001B[39m path_dic=project_folder(project_review=cfg[\u001B[33m'\u001B[39m\u001B[33mproject_review\u001B[39m\u001B[33m'\u001B[39m],config_file=cfg[\u001B[33m\"\u001B[39m\u001B[33mconfig_file\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[43mcombine_scopus_bib_to_excel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_dic\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mscopus_path\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath_dic\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdatabase_path\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\IdeaProjects\\academic_paper_maker\\download_pdf\\database_preparation.py:407\u001B[39m, in \u001B[36mcombine_scopus_bib_to_excel\u001B[39m\u001B[34m(folder_path, output_excel_path)\u001B[39m\n\u001B[32m    402\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    403\u001B[39m \u001B[33;03mMain function that combines multiple Scopus bib files in a folder,\u001B[39;00m\n\u001B[32m    404\u001B[39m \u001B[33;03mapplies cleaning/formatting, removes duplicates, and saves to Excel.\u001B[39;00m\n\u001B[32m    405\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    406\u001B[39m \u001B[38;5;66;03m# 1. Gather all bibtex files\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m407\u001B[39m df=\u001B[43mparse_scopus_bib_files\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfolder_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    409\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m df.empty:\n\u001B[32m    410\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mNo valid data to process. Process aborted.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\IdeaProjects\\academic_paper_maker\\download_pdf\\database_preparation.py:37\u001B[39m, in \u001B[36mparse_scopus_bib_files\u001B[39m\u001B[34m(folder_path, output_csv)\u001B[39m\n\u001B[32m     35\u001B[39m records = []\n\u001B[32m     36\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m bib_file \u001B[38;5;129;01min\u001B[39;00m bib_files:\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbib_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mutf-8\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m bf:\n\u001B[32m     38\u001B[39m         bib = bibtexparser.load(bf)\n\u001B[32m     39\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m bib.entries:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen codecs>:309\u001B[39m, in \u001B[36m__init__\u001B[39m\u001B[34m(self, errors)\u001B[39m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 8: Automatically Filter Relevant References Using LLM**\n",
    "\n",
    "After retrieving thousands of BibTeX references from Scopus (**Step 3**) and combining them into an Excel file (`combined_filtered.xlsx`) in **Step 4**, you'll likely find many entries irrelevant to your specific research focus.\n",
    "\n",
    "In this step, we use a **Large Language Model (LLM)** to classify abstracts based on a defined research topic and context, eliminating the need for tedious manual filtering.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ **What This Step Does**\n",
    "\n",
    "* Loads abstracts from `combined_filtered.xlsx`\n",
    "* Applies a **custom LLM prompt** to assess relevance\n",
    "* Adds a new column to the Excel file with `True`/`False` labels:\n",
    "\n",
    "  * ✅ `True` → Relevant\n",
    "  * ❌ `False` → Not relevant\n",
    "\n",
    "This process is based on a topic-specific prompt defined by you.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 **LLM Prompt Customization**\n",
    "\n",
    "For the automated relevance filtering to work effectively, you **must provide two key inputs** to guide the LLM:\n",
    "\n",
    "* `topic`: A concise statement of your **specific research goal or area of interest**.\n",
    "  *Example:* `\"wafer defect classification\"`\n",
    "\n",
    "* `topic_context`: A description of the **broader scientific or industrial context** where your topic belongs.\n",
    "  *Example:* `\"semiconductor manufacturing and inspection\"`\n",
    "\n",
    "These inputs help the LLM understand what kinds of abstracts are considered relevant and which ones should be filtered out.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 *Not sure how to define your `topic` and `topic_context`?*\n",
    "\n",
    "You can get help from an LLM to generate these values. Here’s how:\n",
    "\n",
    "1. **Open the filtering prompt** template defined in the YAML file:\n",
    "\n",
    "   ```\n",
    "   research_filter/agent/abstract_check.yaml\n",
    "   ```\n",
    "\n",
    "2. **Copy the prompt structure** (including the placeholders for `topic` and `topic_context`).\n",
    "\n",
    "3. **Ask any LLM to assist**, by providing it the YAML prompt and a description of your research.\n",
    "   For example, you could say:\n",
    "\n",
    "   > 🧠 *\"Given the following prompt structure, and knowing that my research is about identifying AI-generated academic papers, can you help me fill in the `topic` and `topic_context` placeholders?\"*\n",
    "\n",
    "4. The LLM might then suggest:\n",
    "\n",
    "   ```yaml\n",
    "   topic: \"AI-generated academic paper detection\"\n",
    "   topic_context: \"scientific publishing and machine learning ethics\"\n",
    "   ```\n",
    "\n",
    "This approach ensures your filtering prompt is both precise and contextually grounded, improving the accuracy of the classification.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ **Configuration via `agentic_setting`**\n",
    "\n",
    "The entire filtering behavior is controlled by a configuration dictionary called `agentic_setting`:\n",
    "\n",
    "```python\n",
    "agentic_setting = {\n",
    "    \"agent_name\": \"abstract_filter\",     # The identifier for the agent logic (must match YAML)\n",
    "    \"column_name\": \"abstract_filter\",    # Name of the column added to the Excel file\n",
    "    \"yaml_path\": yaml_path,              # Path to the YAML file defining agent behavior\n",
    "    \"model_name\": model_name             # Name of the LLM model to use\n",
    "}\n",
    "```\n",
    "\n",
    "### 🔍 Parameter Breakdown:\n",
    "\n",
    "| Key           | Description                                                             |\n",
    "| ------------- | ----------------------------------------------------------------------- |\n",
    "| `agent_name`  | Matches the name of the agent defined in the YAML configuration file.   |\n",
    "| `column_name` | The name of the new column in the Excel file where results are saved.   |\n",
    "| `yaml_path`   | Path to the YAML file containing the agent's logic and prompt template. |\n",
    "| `model_name`  | The specific LLM model used (e.g., `\"gpt-4\"` or `\"claude-3-opus\"`).     |\n",
    "\n",
    "---\n",
    "\n",
    "## 📂 **File and Folder Structure**\n",
    "\n",
    "To avoid redundant LLM calls (which can be costly), results are cached as JSON files:\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       └── combined_filtered.xlsx           ← Updated with filtering results\n",
    "├── abstract_filter/\n",
    "│   └── json_output/\n",
    "│       ├── kim_2019.json\n",
    "│       ├── smith_2020.json\n",
    "│       └── ...\n",
    "```\n",
    "\n",
    "* Each abstract’s filtering result is saved individually.\n",
    "* If the run encounters an error, it can resume without reprocessing previous abstracts.\n",
    "* These files are later reused for **cross-checking** and **final review**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📤 **Excel Output Behavior**\n",
    "\n",
    "Depending on the `overwrite_csv` setting:\n",
    "\n",
    "* `True` → Updates the original `combined_filtered.xlsx`\n",
    "* `False` → Creates a new file (e.g., `combined_filtered_updated.xlsx`)\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ **Caution: Manual Review is Still Required**\n",
    "\n",
    "> ❗ **LLMs are powerful but not perfect**. They may misclassify edge cases or ambiguous abstracts.\n",
    ">\n",
    "> 🔍 Always **manually inspect** the final results before using them for publication or decision-making."
   ],
   "id": "712bb934a2db9134"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 8: Automatically Filter Relevant References Using LLM**\n",
    "\n",
    "After retrieving thousands of BibTeX references from Scopus (**Step 3**) and combining them into an Excel file (`combined_filtered.xlsx`) in **Step 4**, you'll likely find many entries irrelevant to your specific research focus.\n",
    "\n",
    "In this step, we use a **Large Language Model (LLM)** to classify abstracts based on a defined research topic and context, eliminating the need for tedious manual filtering.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ **What This Step Does**\n",
    "\n",
    "* Loads abstracts from `combined_filtered.xlsx`\n",
    "* Applies a **custom LLM prompt** to assess relevance\n",
    "* Adds a new column to the Excel file with `True`/`False` labels:\n",
    "\n",
    "  * ✅ `True` → Relevant\n",
    "  * ❌ `False` → Not relevant\n",
    "\n",
    "This process is based on a topic-specific prompt defined by you.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ **Configuration via `agentic_setting`**\n",
    "\n",
    "The entire filtering behavior is controlled by a configuration dictionary called `agentic_setting`:\n",
    "\n",
    "```python\n",
    "agentic_setting = {\n",
    "    \"agent_name\": \"abstract_filter\",     # The identifier for the agent logic (must match YAML)\n",
    "    \"column_name\": \"abstract_filter\",    # Name of the column added to the Excel file\n",
    "    \"yaml_path\": yaml_path,              # Path to the YAML file defining agent behavior\n",
    "    \"model_name\": model_name             # Name of the LLM model to use\n",
    "}\n",
    "```\n",
    "\n",
    "### 🔍 Parameter Breakdown:\n",
    "\n",
    "| Key           | Description                                                             |\n",
    "| ------------- | ----------------------------------------------------------------------- |\n",
    "| `agent_name`  | Matches the name of the agent defined in the YAML configuration file.   |\n",
    "| `column_name` | The name of the new column in the Excel file where results are saved.   |\n",
    "| `yaml_path`   | Path to the YAML file containing the agent's logic and prompt template. |\n",
    "| `model_name`  | The specific LLM model used (e.g., `\"gpt-4\"` or `\"claude-3-opus\"`).     |\n",
    "\n",
    "---\n",
    "\n",
    "## 📂 **File and Folder Structure**\n",
    "\n",
    "To avoid redundant LLM calls (which can be costly), results are cached as JSON files:\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       └── combined_filtered.xlsx           ← Updated with filtering results\n",
    "├── abstract_filter/\n",
    "│   └── json_output/\n",
    "│       ├── kim_2019.json\n",
    "│       ├── smith_2020.json\n",
    "│       └── ...\n",
    "```\n",
    "\n",
    "* Each abstract’s filtering result is saved individually.\n",
    "* If the run encounters an error, it can resume without reprocessing previous abstracts.\n",
    "* These files are later reused for **cross-checking** and **final review**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📤 **Excel Output Behavior**\n",
    "\n",
    "Depending on the `overwrite_csv` setting:\n",
    "\n",
    "* `True` → Updates the original `combined_filtered.xlsx`\n",
    "* `False` → Creates a new file (e.g., `combined_filtered_updated.xlsx`)\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ **Caution: Manual Review is Still Required**\n",
    "\n",
    "> ❗ **LLMs are powerful but not perfect**. They may misclassify edge cases or ambiguous abstracts.\n",
    ">\n",
    "> 🔍 Always **manually inspect** the final results before using them for publication or decision-making."
   ],
   "id": "cec0a80c590dcbad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T00:57:45.792249Z",
     "start_time": "2025-06-10T00:57:40.012167Z"
    }
   },
   "cell_type": "code",
   "source": "run_abstract_filtering_colab(model_name=\"gpt-4o-mini\", placeholders=placeholders, cfg=cfg)",
   "id": "7ea8ae2257b2a4d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\U0001f680' in position 33: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\balan\\AppData\\Local\\Temp\\ipykernel_45840\\3505874759.py\", line 1, in <module>\n",
      "    run_abstract_filtering_colab(model_name=\"gpt-4o-mini\", placeholders=placeholders, cfg=cfg)\n",
      "  File \"C:\\Users\\balan\\IdeaProjects\\academic_paper_maker\\notebook\\helper.py\", line 319, in run_abstract_filtering_colab\n",
      "    logging.info(\"🚀 Starting abstract filtering pipeline...\")\n",
      "Message: '🚀 Starting abstract filtering pipeline...'\n",
      "Arguments: ()\n",
      "2025-06-10 08:57:40,012 - INFO - 🚀 Starting abstract filtering pipeline...\n",
      "2025-06-10 08:57:40,022 - INFO - Pipeline execution started.\n",
      "2025-06-10 08:57:40,022 - INFO - Loading YAML configuration from C:\\Users\\balan\\IdeaProjects\\academic_paper_maker\\research_filter\\agent\\abstract_check.yaml.\n",
      "2025-06-10 08:57:40,022 - INFO - Preparing role instructions for abstract_filter.\n",
      "2025-06-10 08:57:40,022 - INFO - The agent instruction is: role: An expert evaluator specializing in the relevance of research abstracts  related to wafer defect classification.  You possess advanced knowledge of machine learning applications  specifically tailored to semiconductor manufacturing and inspection.\n",
      "\n",
      "goal: Determine whether the provided abstract is directly relevant to the  research topic: \"wafer defect classification\".  The evaluation should focus on identifying: - The use of machine learning techniques. - A specific application to wafer defect classification.\n",
      "\n",
      "backstory: You are a seasoned researcher with extensive expertise in the intersection  of machine learning and semiconductor manufacturing and inspection, particularly in identifying  and classifying wafer defect classification. Your task is to filter abstracts,  ensuring that only those that contribute significantly to the specified  research topic are considered relevant.\n",
      "\n",
      "evaluation_criteria: The abstract explicitly or implicitly focuses on wafer defect classification.\n",
      "Algorithmic methods, machine learning techniques, or related approaches are mentioned or clearly implied.the wafer defect classification process.\n",
      "The abstract contributes directly to the stated research focus without excessive divergence.\n",
      "\n",
      "expected_output: Respond with only \"True\" if all criteria are met. Respond with only \"False\" if any criterion is not met. Provide no explanation or commentary., and this will be process for the document D:\\test_me\\project_files\\database\\wafer_database.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent instruction is: role: An expert evaluator specializing in the relevance of research abstracts  related to wafer defect classification.  You possess advanced knowledge of machine learning applications  specifically tailored to semiconductor manufacturing and inspection.\n",
      "\n",
      "goal: Determine whether the provided abstract is directly relevant to the  research topic: \"wafer defect classification\".  The evaluation should focus on identifying: - The use of machine learning techniques. - A specific application to wafer defect classification.\n",
      "\n",
      "backstory: You are a seasoned researcher with extensive expertise in the intersection  of machine learning and semiconductor manufacturing and inspection, particularly in identifying  and classifying wafer defect classification. Your task is to filter abstracts,  ensuring that only those that contribute significantly to the specified  research topic are considered relevant.\n",
      "\n",
      "evaluation_criteria: The abstract explicitly or implicitly focuses on wafer defect classification.\n",
      "Algorithmic methods, machine learning techniques, or related approaches are mentioned or clearly implied.the wafer defect classification process.\n",
      "The abstract contributes directly to the stated research focus without excessive divergence.\n",
      "\n",
      "expected_output: Respond with only \"True\" if all criteria are met. Respond with only \"False\" if any criterion is not met. Provide no explanation or commentary., and this will be process for the document D:\\test_me\\project_files\\database\\wafer_database.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 08:57:40,822 - INFO - Loading DataFrame from D:\\test_me\\project_files\\database\\wafer_database.xlsx\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]2025-06-10 08:57:40,842 - INFO - Using abstract text for Tsai_2025.\n",
      "2025-06-10 08:57:40,842 - INFO - Processing Tsai_2025 with AI agent gpt-4o-mini.\n",
      "2025-06-10 08:57:41,657 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 08:57:41,664 - INFO - Successfully processed Tsai_2025, saving at D:\\test_me\\project_files\\abstract_filter\\json_output\\Tsai_2025.json\n",
      " 12%|█▎        | 1/8 [00:00<00:05,  1.22it/s]2025-06-10 08:57:41,664 - INFO - Using abstract text for Ingle_2025.\n",
      "2025-06-10 08:57:41,664 - INFO - Processing Ingle_2025 with AI agent gpt-4o-mini.\n",
      "2025-06-10 08:57:42,442 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 08:57:42,452 - INFO - Successfully processed Ingle_2025, saving at D:\\test_me\\project_files\\abstract_filter\\json_output\\Ingle_2025.json\n",
      " 25%|██▌       | 2/8 [00:01<00:04,  1.25it/s]2025-06-10 08:57:42,452 - INFO - Using abstract text for Zhang_2025.\n",
      "2025-06-10 08:57:42,452 - INFO - Processing Zhang_2025 with AI agent gpt-4o-mini.\n",
      "2025-06-10 08:57:43,172 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 08:57:43,182 - INFO - Successfully processed Zhang_2025, saving at D:\\test_me\\project_files\\abstract_filter\\json_output\\Zhang_2025.json\n",
      " 38%|███▊      | 3/8 [00:02<00:03,  1.30it/s]2025-06-10 08:57:43,186 - INFO - Using abstract text for Joo_2025.\n",
      "2025-06-10 08:57:43,186 - INFO - Processing Joo_2025 with AI agent gpt-4o-mini.\n",
      "2025-06-10 08:57:43,692 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 08:57:43,702 - INFO - Successfully processed Joo_2025, saving at D:\\test_me\\project_files\\abstract_filter\\json_output\\Joo_2025.json\n",
      " 50%|█████     | 4/8 [00:02<00:02,  1.48it/s]2025-06-10 08:57:43,712 - INFO - Using abstract text for Feng_2025.\n",
      "2025-06-10 08:57:43,712 - INFO - Processing Feng_2025 with AI agent gpt-4o-mini.\n",
      "2025-06-10 08:57:44,262 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 08:57:44,267 - INFO - Successfully processed Feng_2025, saving at D:\\test_me\\project_files\\abstract_filter\\json_output\\Feng_2025.json\n",
      " 62%|██████▎   | 5/8 [00:03<00:01,  1.58it/s]2025-06-10 08:57:44,267 - INFO - Using abstract text for Chanpornpakdi_2025.\n",
      "2025-06-10 08:57:44,267 - INFO - Processing Chanpornpakdi_2025 with AI agent gpt-4o-mini.\n",
      "2025-06-10 08:57:44,792 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 08:57:44,792 - INFO - Successfully processed Chanpornpakdi_2025, saving at D:\\test_me\\project_files\\abstract_filter\\json_output\\Chanpornpakdi_2025.json\n",
      " 75%|███████▌  | 6/8 [00:03<00:01,  1.68it/s]2025-06-10 08:57:44,792 - INFO - Using abstract text for Cao_2025.\n",
      "2025-06-10 08:57:44,792 - INFO - Processing Cao_2025 with AI agent gpt-4o-mini.\n",
      "2025-06-10 08:57:45,282 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 08:57:45,287 - INFO - Successfully processed Cao_2025, saving at D:\\test_me\\project_files\\abstract_filter\\json_output\\Cao_2025.json\n",
      " 88%|████████▊ | 7/8 [00:04<00:00,  1.78it/s]2025-06-10 08:57:45,287 - INFO - Using abstract text for Yang_2025.\n",
      "2025-06-10 08:57:45,287 - INFO - Processing Yang_2025 with AI agent gpt-4o-mini.\n",
      "2025-06-10 08:57:45,752 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 08:57:45,752 - INFO - Successfully processed Yang_2025, saving at D:\\test_me\\project_files\\abstract_filter\\json_output\\Yang_2025.json\n",
      "2025-06-10 08:57:45,762 - INFO - Now we going to update the DataFrame from the JSON files\n",
      "2025-06-10 08:57:45,765 - WARNING - Unable to re-load D:\\test_me\\project_files\\abstract_filter\\json_output\\Tsai_2025.json: Incompatible indexer with Series\n",
      "2025-06-10 08:57:45,772 - INFO - Backup saved to D:\\test_me\\project_files\\database\\wafer_database_backup_2025-06-10_08-57-45.xlsx\n",
      "2025-06-10 08:57:45,772 - INFO - New CSV has been created, but a backup has been made.\n",
      "2025-06-10 08:57:45,772 - INFO - Saving updated DataFrame to D:\\test_me\\project_files\\database\\wafer_database.xlsx\n",
      "2025-06-10 08:57:45,787 - INFO - Pipeline execution complete in 5.77 s.\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u2705' in position 33: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\balan\\AppData\\Local\\Temp\\ipykernel_45840\\3505874759.py\", line 1, in <module>\n",
      "    run_abstract_filtering_colab(model_name=\"gpt-4o-mini\", placeholders=placeholders, cfg=cfg)\n",
      "  File \"C:\\Users\\balan\\IdeaProjects\\academic_paper_maker\\notebook\\helper.py\", line 370, in run_abstract_filtering_colab\n",
      "    logging.info(\"✅ Abstract filtering completed successfully.\")\n",
      "Message: '✅ Abstract filtering completed successfully.'\n",
      "Arguments: ()\n",
      "2025-06-10 08:57:45,787 - INFO - ✅ Abstract filtering completed successfully.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 8b: Review The Excel**\n",
    "\n",
    "As mentioned in the previous step, the LLM-based filtering will update your Excel file with a new column (under the column name abstract_filter) indicating whether each abstract is relevant (True or False) to your research topic. You may want to review this file before proceeding to the next step.It is recommended to manually delete any entries that are not relevant to your research topic, even if the LLM marked them as relevant.\n",
    "\n",
    "By dropping these entries, you ensure that the next step (downloading PDFs) only includes papers that are truly pertinent to your study."
   ],
   "id": "e13f293c057ccd45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 9: Download PDFs (you may skip this)**\n",
    "\n",
    "Now that your reference list has been filtered to include only **relevant papers** (based on the abstract analysis in **Step 5**), you're ready to automatically download their corresponding PDFs.\n",
    "\n",
    "This step uses the filtered Excel file (updated in Step 5) to retrieve and save PDFs for each BibTeX entry. The script is powered by Selenium and supports fallback strategies for sources like IEEE, MDPI, and ScienceDirect.\n",
    "\n",
    "> 🛑 **Note:** This step launches a full browser window during execution. Some publishers may block headless downloads — using a visible browser avoids this issue.\n",
    "\n",
    "> 📝 **Important:** By default, only **Sci-Hub** is enabled. To use fallback sources like IEEE, MDPI, or ScienceDirect, you must **manually uncomment the relevant function calls** in the script. This allows you to selectively control which sources to attempt.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Loads metadata from the filtered Excel file (`combined_filtered.xlsx` or the updated version from Step 5).\n",
    "* Attempts to download each paper from **Sci-Hub** first.\n",
    "* If Sci-Hub fails for a paper, you can optionally enable **fallback downloads** from:\n",
    "\n",
    "  * IEEE\n",
    "  * IEEE Search\n",
    "  * MDPI\n",
    "  * ScienceDirect (note: may fail due to access restrictions)\n",
    "* Saves each PDF as `{bibtex_key}.pdf` in the `pdf/` directory for easy tracking and consistent file naming.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Code Snippet\n",
    "\n",
    "```python\n",
    "from download_pdf.download_pdf import (\n",
    "    run_pipeline,\n",
    "    process_scihub_downloads,\n",
    "    process_fallback_ieee,\n",
    "    # process_fallback_ieee_search,\n",
    "    # process_fallback_mdpi,\n",
    "    # process_fallback_sciencedirect,\n",
    ")\n",
    "from setting.project_path import project_folder\n",
    "import os\n",
    "\n",
    "# Project setup\n",
    "project_review = 'wafer_defect'\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Use the filtered Excel from Step 5\n",
    "file_path = path_dic['csv_path']\n",
    "output_folder = os.path.join(main_folder, 'pdf')\n",
    "\n",
    "# Load and categorize data\n",
    "categories, data_filtered = run_pipeline(file_path)\n",
    "\n",
    "# Step 1: Attempt to download from Sci-Hub\n",
    "process_scihub_downloads(categories, output_folder, data_filtered)\n",
    "\n",
    "# Optional fallback sources — uncomment as needed:\n",
    "# Step 2: Try IEEE fallback if Sci-Hub fails\n",
    "# process_fallback_ieee(categories, data_filtered, output_folder)\n",
    "\n",
    "# Step 3: Use IEEE Search-based fallback\n",
    "# process_fallback_ieee_search(categories, data_filtered, output_folder)\n",
    "\n",
    "# Step 4: Try MDPI fallback\n",
    "# process_fallback_mdpi(categories, data_filtered, output_folder)\n",
    "\n",
    "# Step 5: Try ScienceDirect fallback (limited due to restrictions)\n",
    "# process_fallback_sciencedirect(categories, data_filtered, output_folder)\n",
    "\n",
    "# Optional: Save updated Excel with download statuses\n",
    "# save_data(data_filtered, file_path)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Example Folder Structure\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       └── combined_filtered.xlsx     ← Filtered metadata with BibTeX keys (updated in Step 5)\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf                 ← Saved using bibtex_key\n",
    "│   ├── kim_2019.pdf\n",
    "│   └── ...\n",
    "```\n",
    "\n",
    "---"
   ],
   "id": "99f29be5d0a2412b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from download_pdf.download_pdf import run_pipeline, process_scihub_downloads, process_fallback_ieee\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "# Define your project\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "file_path = path_dic['database_path']\n",
    "output_folder = os.path.join(main_folder, 'pdf')\n",
    "\n",
    "# Run the main pipeline to load and categorize the data\n",
    "categories, data_filtered = run_pipeline(file_path)\n",
    "\n",
    "# First step, we will always use Sci-Hub to attempt PDF downloads\n",
    "process_scihub_downloads(categories, output_folder, data_filtered)\n",
    "\n",
    "# Fallback options for entries not available via Sci-Hub:\n",
    "# Uncomment the following lines one by one if you want to try downloading from specific sources\n",
    "\n",
    "# Uncomment to attempt fallback download from IEEE\n",
    "# process_fallback_ieee(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download using IEEE Search\n",
    "# process_fallback_ieee_search(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download from MDPI\n",
    "# process_fallback_mdpi(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download from ScienceDirect\n",
    "# Note: ScienceDirect URLs can be extracted but PDFs may not be downloadable due to security restrictions\n",
    "# process_fallback_sciencedirect(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to save the updated data to Excel after processing\n",
    "# save_data(data_filtered, file_path)"
   ],
   "id": "6a37458687d840b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 10: Convert PDFs to XML using GROBID (you may skip this)**\n",
    "\n",
    "After downloading the PDFs, the next step is to convert them into structured **TEI XML** format using [**GROBID**](https://grobid.readthedocs.io). This step enables downstream tasks like metadata extraction, reference parsing, and full-text analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 What This Step Does\n",
    "\n",
    "* Processes all PDF files in your `pdf/` directory.\n",
    "* Uses GROBID's **batch processing API**.\n",
    "* Saves the resulting XML files into the `xml/` folder (one `.xml` per `.pdf`).\n",
    "* Leverages Docker for fast, isolated execution.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Setup Requirements\n",
    "\n",
    "> 🛠️ **GROBID requires WSL + Docker on Windows**\n",
    "\n",
    "* You must have **WSL** installed (tested on **WSL2 with Ubuntu 22.04**).\n",
    "* You must have **Docker** installed and **running** before launching GROBID.\n",
    "\n",
    "---\n",
    "\n",
    "## 🐳 How to Install & Run GROBID\n",
    "\n",
    "1. **Pull the Docker image from Docker Hub**\n",
    "   Check for the [latest version](https://hub.docker.com/r/grobid/grobid/tags), or use the stable one:\n",
    "\n",
    "   ```bash\n",
    "   docker pull grobid/grobid:0.8.1\n",
    "   ```\n",
    "\n",
    "2. **Start the GROBID container in Ubuntu (WSL)**\n",
    "\n",
    "   Open your Ubuntu terminal and run:\n",
    "\n",
    "   ```bash\n",
    "   docker run --rm --gpus all --init --ulimit core=0 -p 8070:8070 grobid/grobid:0.8.1\n",
    "   ```\n",
    "\n",
    "   > ✅ This exposes GROBID's REST API on `http://localhost:8070/`\n",
    "\n",
    "3. **Test it in your browser**\n",
    "\n",
    "   Open a browser (e.g., Firefox or Chrome) and navigate to:\n",
    "\n",
    "   ```\n",
    "   http://localhost:8070/\n",
    "   ```\n",
    "\n",
    "   You should see the GROBID interface.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Batch Conversion Command\n",
    "\n",
    "Once the GROBID service is running, you can convert all PDFs in your `pdf/` folder to XML using:\n",
    "\n",
    "```bash\n",
    "# From your project root (in WSL/Ubuntu)\n",
    "cd path/to/your/project\n",
    "\n",
    "# Create output folder if not exists\n",
    "mkdir -p xml\n",
    "\n",
    "# Run batch processing using curl\n",
    "curl -v --form \"input=@pdf/\" localhost:8070/api/processFulltextDocument -o xml/\n",
    "```\n",
    "\n",
    "Or, use a Python wrapper or script to iterate over PDFs and call GROBID’s REST API for more control.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Folder Structure After Conversion\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf\n",
    "│   └── kim_2019.pdf\n",
    "├── xml/\n",
    "│   ├── smith_2020.xml\n",
    "│   └── kim_2019.xml\n",
    "```\n"
   ],
   "id": "c3026c463e76dec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 11: Convert XML to JSON (you may skip this)**\n",
    "\n",
    "This step converts GROBID-generated TEI XML files into structured JSON format. While optional, it can be helpful for reviewing document content, integrating into other tools, or preparing data to feed into an LLM.\n",
    "\n",
    "> 📝 **Note:** This step is **optional** — the main pipeline (`run_llm`) reads directly from XML. Use this conversion if you want to inspect or process JSON files instead.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Reads all `*.xml` files from the `xml/` directory.\n",
    "* Converts each into a corresponding `*.json` file (preserving the **BibTeX key as filename** for consistency).\n",
    "* Stores all JSON outputs in `xml/json/`.\n",
    "\n",
    "In addition, it handles and organizes special cases:\n",
    "\n",
    "* 📁 **`xml/json/no_intro_conclusion/`**: XML files where GROBID could not detect an *introduction* or *conclusion* section.\n",
    "* 📁 **`xml/json/untitled_section/`**: XML files where GROBID could not detect any section titles at all — these require manual checking.\n",
    "* 📄 Other successfully processed files are stored directly in `xml/json/`.\n",
    "\n",
    "---\n",
    "\n",
    "## ▶️ Example Code\n",
    "\n",
    "```python\n",
    "from setting.project_path import project_folder\n",
    "from grobid_tei_xml.xml_json import run_pipeline\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "\n",
    "# Convert all XML files in the specified folder to JSON\n",
    "run_pipeline(path_dic['xml_path'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Example Folder Structure After Conversion\n",
    "\n",
    "```\n",
    "corona_discharge/\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf\n",
    "│   └── kim_2019.pdf\n",
    "├── xml/\n",
    "│   ├── smith_2020.xml\n",
    "│   ├── kim_2019.xml\n",
    "│   └── json/\n",
    "│       ├── smith_2020.json\n",
    "│       ├── kim_2019.json\n",
    "│       ├── no_intro_conclusion/\n",
    "│       │   └── failed_paper1.json\n",
    "│       └── untitled_section/\n",
    "│           └── failed_paper2.json\n",
    "```"
   ],
   "id": "24dba46998c443c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from setting.project_path import project_folder\n",
    "from grobid_tei_xml.xml_json import run_pipeline\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "run_pipeline(path_dic['xml_path'])"
   ],
   "id": "17f7f677505cff0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 12: Extract Methodology Details Using LLM**\n",
    "\n",
    "At this stage, your reference list is filtered and the corresponding PDFs (or abstracts) are available. Now, the focus shifts to **extracting key methodological insights** from each paper, such as:\n",
    "\n",
    "* 🧠 Classification algorithms\n",
    "* 🛠️ Feature engineering approaches\n",
    "* 📏 Evaluation metrics\n",
    "\n",
    "This is achieved using a specialized **LLM agent** with a targeted prompt for methodology extraction.\n",
    "\n",
    "> 📌 This step works with **full manuscripts (PDF)** when available, and **falls back to abstracts** if no PDF exists. This flexibility ensures comprehensive analysis even with incomplete data.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Loads your filtered Excel or CSV file from Step 5 or 6.\n",
    "* For each relevant paper:\n",
    "\n",
    "  * If the PDF is available: extracts methodology from the full text.\n",
    "  * If the PDF is **not** available: extracts from the **abstract** instead.\n",
    "* Uses a domain-specific LLM prompt to analyze methodological content.\n",
    "* Appends the results to the existing Excel or CSV file.\n",
    "* Saves per-paper structured JSON files for advanced or customized usage.\n",
    "* Handles backups automatically if overwriting the metadata file.\n",
    "\n",
    "> ⛑️ **Safety Note**: If `'overwrite_csv': True`, a **timestamped backup** of the original `.csv` or `.xlsx` file is automatically created **in the same folder** before any updates are made. This prevents accidental corruption and allows for recovery or version tracking.\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Prompt Purpose\n",
    "\n",
    "This step uses a **domain-aware analytical agent** designed to:\n",
    "\n",
    "> “Extract methodological details (e.g., classification algorithms, feature engineering, evaluation metrics) from filtered papers relevant to a specific machine learning task.”\n",
    "\n",
    "The prompt is defined in a YAML config file (`agent_ml.yaml`) and is tailored by the agent name you provide.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Folder and Output Structure\n",
    "\n",
    "Your project directory may look like this after completing this step:\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── combined_filtered.xlsx                  ← Updated metadata with extraction results\n",
    "│       ├── combined_filtered_backup_20250508_1530.xlsx  ← Auto-generated backup (if overwrite_csv=True)\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf                              ← Saved using bibtex_key\n",
    "│   ├── kim_2019.pdf\n",
    "│   └── ...\n",
    "├── methodology_gap_extractor/\n",
    "│   ├── json_output/\n",
    "│   │   └── smith_2020.json\n",
    "│   ├── multiple_runs_folder/\n",
    "│   └── final_cross_check_folder/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Supported Models\n",
    "\n",
    "Choose from the following supported LLMs:\n",
    "\n",
    "* `\"gpt-4o\"`\n",
    "* `\"gpt-4o-mini\"`\n",
    "* `\"gpt-o3-mini\"`\n",
    "* `'gemini-1.5-pro'`\n",
    "* `'gemini-exp-1206'`\n",
    "* `'gemini-2.0-flash-thinking-exp-01-21'`\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Important Reminders\n",
    "\n",
    "* ✅ Set `used_abstract = True` if some papers lack full PDFs.\n",
    "* 🛑 **Always verify** extracted methodologies manually before using them in analysis, models, or publication. LLMs can hallucinate or misinterpret technical details.\n",
    "\n",
    "---"
   ],
   "id": "9f09bf489150aabb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T00:59:12.246087Z",
     "start_time": "2025-06-10T00:58:22.092551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_for_method_extractor = \"gpt-4o\"\n",
    "\n",
    "run_methodology_extraction_colab(\n",
    "    model_name_method_extractor=\"gpt-4o\",\n",
    "    agent_name=\"methodology_gap_extractor\",\n",
    "    placeholders=placeholders,\n",
    "    cfg=cfg\n",
    ")\n"
   ],
   "id": "d4cc866c39eb8c2c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\U0001f680' in position 33: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\balan\\AppData\\Local\\Temp\\ipykernel_45840\\3388360310.py\", line 1, in <module>\n",
      "    run_methodology_extraction_colab(\n",
      "  File \"C:\\Users\\balan\\IdeaProjects\\academic_paper_maker\\notebook\\helper.py\", line 399, in run_methodology_extraction_colab\n",
      "    logging.info(\"🚀 Starting methodology gap extraction pipeline...\")\n",
      "Message: '🚀 Starting methodology gap extraction pipeline...'\n",
      "Arguments: ()\n",
      "2025-06-10 08:58:22,092 - INFO - 🚀 Starting methodology gap extraction pipeline...\n",
      "2025-06-10 08:58:22,092 - INFO - Pipeline execution started.\n",
      "2025-06-10 08:58:22,092 - INFO - Loading YAML configuration from C:\\Users\\balan\\IdeaProjects\\academic_paper_maker\\research_filter\\agent\\agent_ml.yaml.\n",
      "2025-06-10 08:58:22,105 - INFO - Preparing role instructions for methodology_gap_extractor.\n",
      "2025-06-10 08:58:22,105 - INFO - The agent instruction is: role: An analytical agent specialized in extracting methodological details  (e.g., classification algorithms, feature engineering approaches, evaluation metrics)  from the filtered set of papers deemed relevant to partial discharge classification using machine learning.\n",
      "\n",
      "goal: From each selected paper, identify the core methods and rationales: - Which machine learning techniques or algorithms are used (if applicable). - Why those techniques were chosen. - Performance metrics employed.\n",
      "\n",
      "backstory: As a meticulous methodology researcher, you have deep knowledge of various  analysis techniques and their typical application domains. You leverage NLP-based parsing  to summarize each paper's approach and methodological framework efficiently.\n",
      "\n",
      "evaluation_criteria: Accurately identifies and lists the machine learning techniques employed (e.g., SVM, LSTM, Random Forest).\n",
      "Extracts the exact text or rationale stated in the paper for selecting the methods used.\n",
      "Notes important performance metrics (accuracy, precision, recall, etc.).\n",
      "\n",
      "expected_output: Each paper should be represented as an object in a structured JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"introduction\": {\n",
      "    \"problem_statement\": \"string\",\n",
      "    \"proposed_solution\": \"explanation about the proposed solution\",\n",
      "    \"gap_in_previous_study\": \"long discussion about the technical gap identified in previous studies\"\n",
      "  },\n",
      "  \"methodology\": {\n",
      "    \"methods_used\": [\n",
      "      \"<Method 1>\",\n",
      "      \"<Method 2>\",\n",
      "      \"<list of other methods if available>\"\n",
      "    ],\n",
      "    \"exact_reasons_for_selection\": {\n",
      "      \"<Method 1>\": \"multiple lines of the exact text from the input about the reason or motivation to use Method 1.\",\n",
      "      \"<Method 2>\": \"multiple lines of the exact text from the input about the reason or motivation to use Method 2\",\n",
      "      \"<list of other methods if available>\": \"\"\n",
      "    },\n",
      "    \"reasons_for_selection\": {\n",
      "      \"<Method 1>\": \"Long and detailed explanation about the reason or motivation to use Method 1.\",\n",
      "      \"<Method 2>\": \"Long and detailed explanation about the reason or motivation to use Method 2.\",\n",
      "      \"<list of other methods if available>\": \"\"\n",
      "    },\n",
      "    \"performance_metrics\": {\n",
      "      \"accuracy\": \"<Value or 'N/A'>\",\n",
      "      \"precision\": \"<Value or 'N/A'>\",\n",
      "      \"recall\": \"<Value or 'N/A'>\",\n",
      "      \"f1_score\": \"<Value or 'N/A'>\",\n",
      "      \"other_metrics\": {\n",
      "        \"<Metric name>\": \"<Value>\"\n",
      "      }\n",
      "    },\n",
      "    \"comparison_with_existing_methods\": {\n",
      "      \"key_differences\": \"A long detailed comparison with other machine learning or state-of-the-art (SOTA) techniques, including their names, highlighting improvements, innovations, or weaknesses. If there is value associated, give\"\n",
      "    }\n",
      "  },\n",
      "  \"discussion\": {\n",
      "    \"limitations_and_future_work\": {\n",
      "      \"current_limitations\": [\n",
      "        \"list of limitation of the proposed study\"\n",
      "      ],\n",
      "      \"future_directions\": [\n",
      "        \"list of future direction that you can think\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "additional_notes: The outputs from this agent feed into the comparative_synthesizer_agent,  which will look for broader patterns., and this will be process for the document D:\\test_me\\project_files\\database\\wafer_database.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent instruction is: role: An analytical agent specialized in extracting methodological details  (e.g., classification algorithms, feature engineering approaches, evaluation metrics)  from the filtered set of papers deemed relevant to partial discharge classification using machine learning.\n",
      "\n",
      "goal: From each selected paper, identify the core methods and rationales: - Which machine learning techniques or algorithms are used (if applicable). - Why those techniques were chosen. - Performance metrics employed.\n",
      "\n",
      "backstory: As a meticulous methodology researcher, you have deep knowledge of various  analysis techniques and their typical application domains. You leverage NLP-based parsing  to summarize each paper's approach and methodological framework efficiently.\n",
      "\n",
      "evaluation_criteria: Accurately identifies and lists the machine learning techniques employed (e.g., SVM, LSTM, Random Forest).\n",
      "Extracts the exact text or rationale stated in the paper for selecting the methods used.\n",
      "Notes important performance metrics (accuracy, precision, recall, etc.).\n",
      "\n",
      "expected_output: Each paper should be represented as an object in a structured JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"introduction\": {\n",
      "    \"problem_statement\": \"string\",\n",
      "    \"proposed_solution\": \"explanation about the proposed solution\",\n",
      "    \"gap_in_previous_study\": \"long discussion about the technical gap identified in previous studies\"\n",
      "  },\n",
      "  \"methodology\": {\n",
      "    \"methods_used\": [\n",
      "      \"<Method 1>\",\n",
      "      \"<Method 2>\",\n",
      "      \"<list of other methods if available>\"\n",
      "    ],\n",
      "    \"exact_reasons_for_selection\": {\n",
      "      \"<Method 1>\": \"multiple lines of the exact text from the input about the reason or motivation to use Method 1.\",\n",
      "      \"<Method 2>\": \"multiple lines of the exact text from the input about the reason or motivation to use Method 2\",\n",
      "      \"<list of other methods if available>\": \"\"\n",
      "    },\n",
      "    \"reasons_for_selection\": {\n",
      "      \"<Method 1>\": \"Long and detailed explanation about the reason or motivation to use Method 1.\",\n",
      "      \"<Method 2>\": \"Long and detailed explanation about the reason or motivation to use Method 2.\",\n",
      "      \"<list of other methods if available>\": \"\"\n",
      "    },\n",
      "    \"performance_metrics\": {\n",
      "      \"accuracy\": \"<Value or 'N/A'>\",\n",
      "      \"precision\": \"<Value or 'N/A'>\",\n",
      "      \"recall\": \"<Value or 'N/A'>\",\n",
      "      \"f1_score\": \"<Value or 'N/A'>\",\n",
      "      \"other_metrics\": {\n",
      "        \"<Metric name>\": \"<Value>\"\n",
      "      }\n",
      "    },\n",
      "    \"comparison_with_existing_methods\": {\n",
      "      \"key_differences\": \"A long detailed comparison with other machine learning or state-of-the-art (SOTA) techniques, including their names, highlighting improvements, innovations, or weaknesses. If there is value associated, give\"\n",
      "    }\n",
      "  },\n",
      "  \"discussion\": {\n",
      "    \"limitations_and_future_work\": {\n",
      "      \"current_limitations\": [\n",
      "        \"list of limitation of the proposed study\"\n",
      "      ],\n",
      "      \"future_directions\": [\n",
      "        \"list of future direction that you can think\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "additional_notes: The outputs from this agent feed into the comparative_synthesizer_agent,  which will look for broader patterns., and this will be process for the document D:\\test_me\\project_files\\database\\wafer_database.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 08:58:22,453 - INFO - Loading DataFrame from D:\\test_me\\project_files\\database\\wafer_database.xlsx\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]2025-06-10 08:58:22,463 - INFO - Using abstract text for Tsai_2025.\n",
      "2025-06-10 08:58:22,463 - INFO - Processing Tsai_2025 with AI agent gpt-4o.\n",
      "2025-06-10 08:58:31,142 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 08:58:31,149 - INFO - Successfully processed Tsai_2025, saving at D:\\test_me\\project_files\\methodology_gap_extractor\\json_output\\gpt-4o\\Tsai_2025.json\n",
      " 20%|██        | 1/5 [00:08<00:34,  8.69s/it]2025-06-10 08:58:31,152 - INFO - Using abstract text for Ingle_2025.\n",
      "2025-06-10 08:58:31,152 - INFO - Processing Ingle_2025 with AI agent gpt-4o.\n",
      "2025-06-10 08:58:41,222 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 08:58:41,222 - INFO - Successfully processed Ingle_2025, saving at D:\\test_me\\project_files\\methodology_gap_extractor\\json_output\\gpt-4o\\Ingle_2025.json\n",
      " 40%|████      | 2/5 [00:18<00:28,  9.50s/it]2025-06-10 08:58:41,222 - INFO - Using abstract text for Zhang_2025.\n",
      "2025-06-10 08:58:41,222 - INFO - Processing Zhang_2025 with AI agent gpt-4o.\n",
      "2025-06-10 08:58:56,219 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 08:58:56,222 - INFO - Successfully processed Zhang_2025, saving at D:\\test_me\\project_files\\methodology_gap_extractor\\json_output\\gpt-4o\\Zhang_2025.json\n",
      " 60%|██████    | 3/5 [00:33<00:24, 12.01s/it]2025-06-10 08:58:56,224 - INFO - Using abstract text for Joo_2025.\n",
      "2025-06-10 08:58:56,224 - INFO - Processing Joo_2025 with AI agent gpt-4o.\n",
      "2025-06-10 08:59:05,282 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 08:59:05,282 - INFO - Successfully processed Joo_2025, saving at D:\\test_me\\project_files\\methodology_gap_extractor\\json_output\\gpt-4o\\Joo_2025.json\n",
      " 80%|████████  | 4/5 [00:42<00:10, 10.85s/it]2025-06-10 08:59:05,282 - INFO - Using abstract text for Feng_2025.\n",
      "2025-06-10 08:59:05,282 - INFO - Processing Feng_2025 with AI agent gpt-4o.\n",
      "2025-06-10 08:59:12,195 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 08:59:12,196 - INFO - Successfully processed Feng_2025, saving at D:\\test_me\\project_files\\methodology_gap_extractor\\json_output\\gpt-4o\\Feng_2025.json\n",
      "2025-06-10 08:59:12,202 - INFO - Now we going to update the DataFrame from the JSON files\n",
      "2025-06-10 08:59:12,202 - WARNING - Unable to re-load D:\\test_me\\project_files\\methodology_gap_extractor\\json_output\\gpt-4o\\Tsai_2025.json: Incompatible indexer with Series\n",
      "2025-06-10 08:59:12,222 - INFO - Backup saved to D:\\test_me\\project_files\\database\\wafer_database_backup_2025-06-10_08-59-12.xlsx\n",
      "2025-06-10 08:59:12,222 - INFO - New CSV has been created, but a backup has been made.\n",
      "2025-06-10 08:59:12,228 - INFO - Saving updated DataFrame to D:\\test_me\\project_files\\database\\wafer_database.xlsx\n",
      "2025-06-10 08:59:12,242 - INFO - Pipeline execution complete in 50.15 s.\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u2705' in position 33: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\balan\\AppData\\Local\\Temp\\ipykernel_45840\\3388360310.py\", line 1, in <module>\n",
      "    run_methodology_extraction_colab(\n",
      "  File \"C:\\Users\\balan\\IdeaProjects\\academic_paper_maker\\notebook\\helper.py\", line 450, in run_methodology_extraction_colab\n",
      "    logging.info(\"✅ Methodology extraction completed successfully.\")\n",
      "Message: '✅ Methodology extraction completed successfully.'\n",
      "Arguments: ()\n",
      "2025-06-10 08:59:12,242 - INFO - ✅ Methodology extraction completed successfully.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 12: Draft Literature Review (Chapter 2) Using Combined JSON**\n",
    "\n",
    "Once you've extracted methodological insights in Step 9, the next logical move is to **start organizing and drafting your literature review** (typically Chapter 2 of a thesis or paper). This step outlines how to **combine extracted results** into a single, structured file — and how to use that file to produce high-quality summaries, tables, and narrative drafts using an LLM.\n",
    "\n",
    "> 📌 This is a branching step — not everyone will follow this exact path — but it's a powerful way to move from **structured data → written draft** efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Collects individual methodology JSON files.\n",
    "* Merges them into a single, unified JSON (`combined_output.json`).\n",
    "* Prepares this JSON as input for:\n",
    "\n",
    "  * Google AI Studio (e.g., Gemini Pro)\n",
    "  * GPT-based agents\n",
    "  * Jupyter notebooks or drafting pipelines\n",
    "* Enables:\n",
    "\n",
    "  * Narrative generation for Chapter 2\n",
    "  * Thematic clustering of methods\n",
    "  * Structured tables summarizing key findings\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Output Structure Example\n",
    "\n",
    "```\n",
    "corona_discharge/\n",
    "├── combined_output.json      ← ✅ Master file for summarization and drafting\n",
    "├── methodology_gap_extractor_partial_discharge/\n",
    "│   └── json_output/\n",
    "│       └── *.json            ← Individual extracted documents\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Using LLM to Generate Summary Tables\n",
    "\n",
    "With the `combined_output.json`, you can prompt an LLM to create structured tables summarizing key findings. For example:\n",
    "\n",
    "> “Using the combined JSON, generate a table with the following columns:\n",
    "> **Author**, **Machine Learning Algorithm**, **Feature Types**, **Dataset Used**, and **Performance Metrics**.”\n",
    "\n",
    "This gives you a **quick-glance overview** of the landscape, helpful both for understanding trends and citing clearly in your literature review.\n",
    "\n",
    "### ✅ Sample Columns for Summary Table:\n",
    "\n",
    "* **Author / Year**\n",
    "* **ML Algorithm(s) Used**\n",
    "* **Features**\n",
    "* **Dataset / Source**\n",
    "* **Performance (Accuracy, F1, etc.)**\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ Drafting Strategy Tips\n",
    "\n",
    "You can also use LLM prompts like:\n",
    "\n",
    "> “Based on the combined JSON, write a summary paragraph comparing the top 3 machine learning techniques used for partial discharge classification.”\n",
    "\n",
    "Or:\n",
    "\n",
    "> “Generate an introduction section discussing the evolution of feature engineering techniques in this domain.”\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Important Reminders\n",
    "\n",
    "* ✅ Make sure all JSON structures are consistent before combining.\n",
    "* 📉 Segment or cluster the JSON by subdomain if the file becomes too large.\n",
    "* 🔍 Review all generated tables and text — LLMs are **tools**, not final authorities.\n",
    "\n",
    "---"
   ],
   "id": "411c0567bd535174"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T01:04:39.629705Z",
     "start_time": "2025-06-10T01:04:39.618135Z"
    }
   },
   "cell_type": "code",
   "source": "combine_methodology_output_colab(model_name_method_extractor=model_for_method_extractor, cfg=cfg)",
   "id": "527d5be14321c3c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\U0001f504' in position 33: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\balan\\AppData\\Local\\Temp\\ipykernel_45840\\2896541282.py\", line 1, in <module>\n",
      "    combine_methodology_output_colab(model_name_method_extractor=model_for_method_extractor, cfg=cfg)\n",
      "  File \"C:\\Users\\balan\\IdeaProjects\\academic_paper_maker\\notebook\\helper.py\", line 473, in combine_methodology_output_colab\n",
      "    - project_review\n",
      "Message: '🔄 Starting to combine methodology gap JSON files...'\n",
      "Arguments: ()\n",
      "2025-06-10 09:04:39,618 - INFO - 🔄 Starting to combine methodology gap JSON files...\n",
      "2025-06-10 09:04:39,622 - INFO - Scanning directory: D:\\test_me\\project_files\\methodology_gap_extractor\\json_output\\gpt-4o\n",
      "2025-06-10 09:04:39,622 - INFO - Found 5 JSON files. Starting to process...\n",
      "Combining JSON files: 100%|██████████| 5/5 [00:00<?, ?it/s]\n",
      "2025-06-10 09:04:39,622 - INFO - Combined JSON saved to: D:\\test_me\\project_files\\combined_output.json\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u2705' in position 33: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\balan\\anaconda3\\envs\\pyblinker\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\balan\\AppData\\Local\\Temp\\ipykernel_45840\\2896541282.py\", line 1, in <module>\n",
      "    combine_methodology_output_colab(model_name_method_extractor=model_for_method_extractor, cfg=cfg)\n",
      "  File \"C:\\Users\\balan\\IdeaProjects\\academic_paper_maker\\notebook\\helper.py\", line 495, in combine_methodology_output_colab\n",
      "    path_dict['main_folder'],\n",
      "Message: '✅ Combined JSON saved to: D:\\\\test_me\\\\project_files\\\\combined_output.json'\n",
      "Arguments: ()\n",
      "2025-06-10 09:04:39,622 - INFO - ✅ Combined JSON saved to: D:\\test_me\\project_files\\combined_output.json\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Step 13: Export Filtered Excel to BibTeX**\n",
    "\n",
    "After reviewing and filtering your `combined_filtered.xlsx` file, you can convert the refined list of papers back into a **BibTeX file**. This can be helpful for citation management or integration with tools like LaTeX or reference managers.\n",
    "\n",
    "---\n",
    "\n",
    "# ✨ What This Step Does\n",
    "\n",
    "* Loads the filtered Excel file containing your selected papers\n",
    "* Converts the data back into BibTeX format\n",
    "* Saves the result to a `.bib` file for easy reuse or citation\n",
    "\n",
    "\n",
    "# 📁 File Structure Example\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── combined_filtered.xlsx                  ← Filtered Excel file with selected papers\n",
    "│       ├── combined_filtered_backup_20250508_1530.xlsx  ← Auto backup if overwrite is enabled\n",
    "│       └── filtered_output.bib                     ← Newly generated BibTeX file\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf                              ← Full-text PDFs saved by bibtex_key\n",
    "│   ├── kim_2019.pdf\n",
    "│   └── ...\n",
    "├── methodology_gap_extractor_wafer_defect/\n",
    "│   ├── json_output/\n",
    "│   │   └── smith_2020.json                         ← Extracted methodology as structured JSON\n",
    "│   ├── multiple_runs_folder/\n",
    "│   └── final_cross_check_folder/\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Notes\n",
    "\n",
    "* Ensure the Excel file has standard bibliographic columns like: `title`, `author`, `year`, `journal`, `doi`, etc.\n",
    "* The function `generate_bibtex()` maps these fields into valid BibTeX entries.\n",
    "* You can open the `.bib` file in any text editor or reference manager to confirm the results."
   ],
   "id": "3c8648d56cd6763f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T01:07:24.195609Z",
     "start_time": "2025-06-10T01:07:24.188367Z"
    }
   },
   "cell_type": "code",
   "source": "print(cfg)",
   "id": "ba57ccbf0093226c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'project_review': 'wafer', 'env_path': 'D:\\\\test_me\\\\project_files\\\\.env', 'main_folder': 'D:\\\\test_me\\\\project_files', 'project_root': 'D:\\\\test_me\\\\project_files\\\\database', 'yaml_path_abstract': 'C:\\\\Users\\\\balan\\\\IdeaProjects\\\\academic_paper_maker\\\\research_filter\\\\agent\\\\abstract_check.yaml', 'yaml_path_methodology': 'C:\\\\Users\\\\balan\\\\IdeaProjects\\\\academic_paper_maker\\\\research_filter\\\\agent\\\\agent_ml.yaml', 'config_file': 'D:\\\\test_me\\\\project_files\\\\project_folders.json', 'methodology_gap_extractor_path': 'D:\\\\test_me\\\\project_files\\\\methodology_gap_extractor\\\\json_output'}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T01:10:44.637034Z",
     "start_time": "2025-06-10T01:10:44.612699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from post_code_saviour.excel_to_bib import generate_bibtex\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=cfg['project_review'],config_file=cfg[\"config_file\"])\n",
    "# main_folder = path_dic['main_folder']\n",
    "\n",
    "\n",
    "output_bib = os.path.join(path_dic['main_folder'], 'database', 'filtered_output.bib')\n",
    "\n",
    "# Load the filtered Excel file\n",
    "df = pd.read_excel(path_dic['database_path'])\n",
    "\n",
    "# Generate BibTeX file\n",
    "generate_bibtex(df, output_file=output_bib)"
   ],
   "id": "35d9935f47aebf25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BibTeX file generated: D:\\test_me\\project_files\\database\\filtered_output.bib\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
